{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talefty123/testing_code/blob/main/Python/Unstructured/H_ISD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSmcwLZuptDU"
      },
      "source": [
        "# Proposed Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG5fZn_ypunP"
      },
      "source": [
        "# **Biblioheque**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--SvzvIspu-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e722e98-f71e-4596-d8a9-da7aa0b63e53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
            "  _C._set_default_tensor_type(t)\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from scipy.stats import rice\n",
        "import pickle\n",
        "# import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import sys\n",
        "import timeit\n",
        "import os\n",
        "\n",
        "torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
        "torch.set_default_dtype(torch.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBk01Vu0pvMm"
      },
      "source": [
        "# class to save results in file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06IymVCkpvYS"
      },
      "outputs": [],
      "source": [
        "class Record:\n",
        "    def __init__(self, TextName):\n",
        "        self.out_file = open(TextName, 'a')\n",
        "        self.old_stdout = sys.stdout\n",
        "        sys.stdout = self\n",
        "\n",
        "    def write(self, text):\n",
        "        self.old_stdout.write(text)\n",
        "        self.out_file.write(text)\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout = self.old_stdout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xPamGjNpv8E"
      },
      "source": [
        "# **slicer the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlQdA0xHpwF7"
      },
      "outputs": [],
      "source": [
        "def slicer(data):\n",
        "    dataI = data[slice(0, len(data), 2)]\n",
        "    dataQ = data[slice(1, len(data), 2)]\n",
        "    return(dataI, dataQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOoiIo_LpwQl"
      },
      "source": [
        "# **Modulation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIftC8fvpwir"
      },
      "outputs": [],
      "source": [
        "def mapper_16QAM(QAM16, data):\n",
        "    map0 = 2*data[slice(0, len(data), 2)] + data[slice(1, len(data), 2)]\n",
        "    map0 = list(map(int, map0))\n",
        "    dataMapped = []\n",
        "    for i in range(len(map0)):\n",
        "        dataMapped.append(QAM16[map0[i]])\n",
        "    return(dataMapped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYhPgAIcqtGT"
      },
      "outputs": [],
      "source": [
        "def calculate_bits(Modulation,NumSubcarriers,NumDataSymb):\n",
        "    if Modulation=='QPSK':\n",
        "        Nbpscs=2\n",
        "    elif Modulation=='16QAM':\n",
        "        Nbpscs=4\n",
        "    return NumDataSymb*NumSubcarriers*Nbpscs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgX6tiqFpwvb"
      },
      "source": [
        "# **generate noise**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LQwZvGaprKZ"
      },
      "outputs": [],
      "source": [
        "def AWGN(IFsig, SNR):\n",
        "    dP = np.zeros(len(IFsig))\n",
        "    P = 0\n",
        "\n",
        "    for i in range(len(IFsig)):\n",
        "        dP[i] = abs(IFsig[i])**2\n",
        "        P = P + dP[i]\n",
        "\n",
        "    P = P/len(IFsig)\n",
        "    gamma = 10**(SNR/10)\n",
        "    N0 = P/gamma\n",
        "    n = ((N0/2)**(0.5))*np.random.standard_normal(len(IFsig))\n",
        "    IF_n = np.zeros((len(IFsig),1))\n",
        "\n",
        "    for i in range(len(IFsig)):\n",
        "        IF_n[i,:] = IFsig[i] + n[i]\n",
        "\n",
        "    return(IF_n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDx6TEAyimk9"
      },
      "source": [
        "# Generate channel model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnoArgs7imk9"
      },
      "outputs": [],
      "source": [
        "def Generate_channel(Nr, Nt, type):\n",
        "    if (type == 'gauss'):\n",
        "        return (np.random.normal(size=(Nr,Nt))+1j*np.random.normal(size=(Nr,Nt)))/np.sqrt(2)\n",
        "    if (type == 'rayleigh'):\n",
        "        return (np.random.rayleigh(scale=(1/np.sqrt(2)), size=(Nr,Nt)) + 1j*np.random.rayleigh(scale=(1/np.sqrt(2)), size=(Nr,Nt)))/np.sqrt(2)\n",
        "    if (type == 'rician'):\n",
        "        b = 1/np.sqrt(2)\n",
        "        return (rice.rvs(b, size=(Nr,Nt)) + 1j*rice.rvs(b, size=(Nr,Nt)))/np.sqrt(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eciNtnFjq2yd"
      },
      "source": [
        "# **Generate Dataset**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AE4Q6jZCq3CY"
      },
      "outputs": [],
      "source": [
        "DataSet_x   = []  # x dataset after modulation\n",
        "DataSet_y   = []  # y dataset\n",
        "DataSet_HH  = []  # H dataset\n",
        "DataSet_b   = []  # binary dataset\n",
        "SNR_min_dB  = 0\n",
        "SNR_max_dB  = 20\n",
        "step_dB     = 5\n",
        "num_dB      = int((SNR_max_dB - SNR_min_dB) / step_dB) + 1\n",
        "\n",
        "SNR         = np.linspace(SNR_min_dB, SNR_max_dB, num=num_dB)\n",
        "\n",
        "\n",
        "Nt = 8             # Tx: 8\n",
        "Nr = 64            # Rx: 128\n",
        "N_samp = 4000\n",
        "\n",
        "\n",
        "def Gen_dataset(mode, snr, imperfect, N_samp):\n",
        "    DataSet_x   = []  # x dataset after modulation\n",
        "    DataSet_y   = []  # y dataset\n",
        "    DataSet_H   = []\n",
        "    DataSet_HH  = []\n",
        "\n",
        "    NumSubcarriers = 1\n",
        "    Modulation = '16QAM'\n",
        "    QAM16 = [-1, -0.333, 0.333, 1]\n",
        "    NumDataSymb = 1\n",
        "    N_type = 'gauss'\n",
        "\n",
        "    if mode == 'train':\n",
        "        for snr in SNR:\n",
        "            for runIdx in range(0, N_samp):      # ! 20000 x Nt: samples\n",
        "                H = Generate_channel(Nt, Nr, N_type)\n",
        "                HH = np.concatenate((np.concatenate((H.real, H.imag), axis=1),\n",
        "                                    np.concatenate((-H.imag, H.real), axis=1)), axis=0)\n",
        "                x = np.zeros((2*Nt, NumSubcarriers))\n",
        "                a = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "                DataRaw = np.zeros((Nt, a))\n",
        "                for t in range(Nt):\n",
        "                    #\"data symbol generate\"\n",
        "                    NumBits = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "                    bit = np.random.randint(1, 3, NumBits)-1\n",
        "                    DataRaw[t, :] = bit\n",
        "                    for j in range(4):\n",
        "                        DataSet_b.append(bit[j])\n",
        "                    I = np.zeros((1, a))\n",
        "                    I[0, :] = DataRaw[t, :]\n",
        "                    (dataI, dataQ) = slicer(I[0])\n",
        "\n",
        "                    # Mapper\n",
        "                    mapI = mapper_16QAM(QAM16, dataI)\n",
        "                    mapQ = mapper_16QAM(QAM16, dataQ)\n",
        "                    x[t] = mapI[0]\n",
        "                    x[t+Nt] = mapQ[0]\n",
        "\n",
        "                # transpose\n",
        "                x = x.transpose()\n",
        "\n",
        "                y_wo_noise = np.matmul(x, HH)\n",
        "\n",
        "                # noise\n",
        "                noise = AWGN(y_wo_noise.transpose(), snr)\n",
        "\n",
        "                y = y_wo_noise + noise.transpose()\n",
        "\n",
        "                DataSet_x.append(x)    # ! I, Q sample distance by Nt.\n",
        "                DataSet_y.append(y)                 # ! output sample\n",
        "\n",
        "                # Imperfect channel: 5%\n",
        "                # coef = (2*np.random.randint(0,2,size=HH.shape) - 1)\n",
        "                # HH = HH + coef * HH * 0.05\n",
        "                DataSet_HH.append(HH)\n",
        "                DataSet_H.append(H)               # ! Generated channel\n",
        "\n",
        "    else:\n",
        "        for runIdx in range(0, N_samp):      # ! 20000 x Nt: samples\n",
        "            H = Generate_channel(Nt, Nr, N_type)\n",
        "            HH = np.concatenate((np.concatenate((H.real, H.imag), axis=1),\n",
        "                                np.concatenate((-H.imag, H.real), axis=1)), axis=0)\n",
        "            x = np.zeros((2*Nt, NumSubcarriers))\n",
        "            a = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "            DataRaw = np.zeros((Nt, a))\n",
        "            for t in range(Nt):\n",
        "                #\"data symbol generate\"\n",
        "                NumBits = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "                bit = np.random.randint(1, 3, NumBits)-1\n",
        "                DataRaw[t, :] = bit\n",
        "                for j in range(4):\n",
        "                    DataSet_b.append(bit[j])\n",
        "                I = np.zeros((1, a))\n",
        "                I[0, :] = DataRaw[t, :]\n",
        "                (dataI, dataQ) = slicer(I[0])\n",
        "\n",
        "                # Mapper\n",
        "                mapI = mapper_16QAM(QAM16, dataI)\n",
        "                mapQ = mapper_16QAM(QAM16, dataQ)\n",
        "                x[t] = mapI[0]\n",
        "                x[t+Nt] = mapQ[0]\n",
        "\n",
        "            # transpose\n",
        "            x = x.transpose()\n",
        "\n",
        "            y_wo_noise = np.matmul(x, HH)\n",
        "\n",
        "            # noise\n",
        "            noise = AWGN(y_wo_noise.transpose(), snr)\n",
        "\n",
        "            y = y_wo_noise + noise.transpose()\n",
        "\n",
        "            DataSet_x.append(x)    # ! I, Q sample distance by Nt.\n",
        "            DataSet_y.append(y)                 # ! output sample\n",
        "\n",
        "            # Imperfect channel: 5%\n",
        "            DataSet_HH.append(HH)\n",
        "            DataSet_H.append(H)               # ! Generated channel\n",
        "\n",
        "\n",
        "    # Shuffle dataset\n",
        "    random.seed(1)\n",
        "    temp = list(zip(DataSet_x, DataSet_y, DataSet_H, DataSet_HH))\n",
        "    random.shuffle(temp)\n",
        "    DataSet_x, DataSet_y, DataSet_H, DataSet_HH = zip(*temp)\n",
        "\n",
        "    return DataSet_x, DataSet_y, DataSet_H, DataSet_HH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nWo2pTrimk-"
      },
      "outputs": [],
      "source": [
        "def reconstruct_channel (H):\n",
        "# H_raw = [R(H) I(H); -I(H) R(H)]\n",
        "# we have four version of H_est\n",
        "    H_est_1 = []\n",
        "    H_est_2 = []\n",
        "    H_est_3 = []\n",
        "    H_est_4 = []\n",
        "\n",
        "    H_est_Re_1 = H[0:Nt, 0:Nr]\n",
        "    H_est_Im_1 = H[0:Nt, Nr:2*Nr]\n",
        "    H_est_Im_2 = - H[Nt:2*Nt, 0:Nr]\n",
        "    H_est_Re_2 = H[Nt:2*Nt, Nr:2*Nr]\n",
        "\n",
        "    H_est_1 = H_est_Re_1 + 1j * H_est_Im_1\n",
        "    H_est_2 = H_est_Re_1 + 1j * H_est_Im_2\n",
        "    H_est_3 = H_est_Re_2 + 1j * H_est_Im_1\n",
        "    H_est_4 = H_est_Re_2 + 1j * H_est_Im_2\n",
        "\n",
        "    return H_est_1, H_est_2, H_est_3, H_est_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQUDE5Q1imk-"
      },
      "outputs": [],
      "source": [
        "# def NMSE(H_est, H_raw):\n",
        "#     H_est_1, H_est_2, H_est_3, H_est_4 = reconstruct_channel(H_est)\n",
        "#     H_est_vec_1 = torch.reshape(H_est_1, [Nt * Nr, 1])\n",
        "#     H_est_vec_2 = torch.reshape(H_est_2, [Nt * Nr, 1])\n",
        "#     H_est_vec_3 = torch.reshape(H_est_3, [Nt * Nr, 1])\n",
        "#     H_est_vec_4 = torch.reshape(H_est_4, [Nt * Nr, 1])\n",
        "\n",
        "#     H_raw_vec = torch.reshape(H_raw, [Nt * Nr, 1])\n",
        "\n",
        "#     mse_1       = (torch.norm(H_raw_vec - H_est_vec_1)**2) / len(H_raw_vec)\n",
        "#     mse_2       = (torch.norm(H_raw_vec - H_est_vec_2)**2) / len(H_raw_vec)\n",
        "#     mse_3       = (torch.norm(H_raw_vec - H_est_vec_3)**2) / len(H_raw_vec)\n",
        "#     mse_4       = (torch.norm(H_raw_vec - H_est_vec_4)**2) / len(H_raw_vec)\n",
        "\n",
        "#     sigEner   = torch.norm(H_raw_vec)**2\n",
        "\n",
        "#     nmse_1      = mse_1 / sigEner\n",
        "#     nmse_2      = mse_2 / sigEner\n",
        "#     nmse_3      = mse_3 / sigEner\n",
        "#     nmse_4      = mse_4 / sigEner\n",
        "\n",
        "#     # Best nmse\n",
        "#     nmse        = min([nmse_1, nmse_2, nmse_3, nmse_4])\n",
        "\n",
        "#     return torch.abs(nmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knU4OnL3imk_"
      },
      "outputs": [],
      "source": [
        "def NMSE(H_est, H_raw):\n",
        "    H_est_1, H_est_2, H_est_3, H_est_4 = reconstruct_channel(H_est)\n",
        "\n",
        "    # Lấy phần thực của các tensor nếu chúng là complex\n",
        "    H_est_vec_1 = torch.reshape(H_est_1, [Nt * Nr, 1]).abs()\n",
        "    H_est_vec_2 = torch.reshape(H_est_2, [Nt * Nr, 1]).abs()\n",
        "    H_est_vec_3 = torch.reshape(H_est_3, [Nt * Nr, 1]).abs()\n",
        "    H_est_vec_4 = torch.reshape(H_est_4, [Nt * Nr, 1]).abs()\n",
        "\n",
        "    H_raw_vec = torch.reshape(H_raw, [Nt * Nr, 1]).abs()\n",
        "\n",
        "    mse_1 = (torch.norm(H_raw_vec - H_est_vec_1)**2) / len(H_raw_vec)\n",
        "    mse_2 = (torch.norm(H_raw_vec - H_est_vec_2)**2) / len(H_raw_vec)\n",
        "    mse_3 = (torch.norm(H_raw_vec - H_est_vec_3)**2) / len(H_raw_vec)\n",
        "    mse_4 = (torch.norm(H_raw_vec - H_est_vec_4)**2) / len(H_raw_vec)\n",
        "\n",
        "    sigEner = torch.norm(H_raw_vec)**2\n",
        "\n",
        "    nmse_1 = mse_1 / sigEner\n",
        "    nmse_2 = mse_2 / sigEner\n",
        "    nmse_3 = mse_3 / sigEner\n",
        "    nmse_4 = mse_4 / sigEner\n",
        "\n",
        "    # Chọn NMSE tốt nhất\n",
        "    nmse = min([nmse_1, nmse_2, nmse_3, nmse_4])\n",
        "\n",
        "    return torch.abs(nmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyjIgxUurU0P"
      },
      "outputs": [],
      "source": [
        "def Input_ISDNN(mode, DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_samp):\n",
        "    H_in = []        # ! H_in    , np.diag(np.diag()) return a diag matrix instead of diag components.\n",
        "    H_true = []   # ! generated s\n",
        "    H_raw = []\n",
        "    e = []        # ! vector errors\n",
        "    xTx = []\n",
        "    xTy = []\n",
        "    Di = []\n",
        "    # steering = [] # ! Steering vector: ZoA and AoA\n",
        "\n",
        "    if mode == 'train':\n",
        "        n_sample = N_samp * len(SNR)\n",
        "    else:\n",
        "        n_sample = N_samp\n",
        "\n",
        "    for i in range (n_sample):\n",
        "        H_true.append(torch.tensor(DataSet_HH[i]))\n",
        "        H_raw.append(torch.tensor(DataSet_H[i]))\n",
        "        Di.append(torch.tensor(np.linalg.pinv(np.diag(np.diag(np.dot(DataSet_x[i].transpose(), DataSet_x[i]))))))\n",
        "        xTy.append(torch.tensor(np.dot(DataSet_x[i].transpose(), DataSet_y[i])))\n",
        "        H_in.append(torch.matmul(Di[i], xTy[i]))\n",
        "        e.append(torch.rand([2*Nt, 2*Nr]))\n",
        "        xTx.append(torch.tensor(np.dot(DataSet_x[i].transpose(), DataSet_x[i])))\n",
        "        # steering.append(torch.tensor(DataSet_Steering[i]))\n",
        "\n",
        "    H_true = torch.stack(H_true, dim=0)\n",
        "    H_raw = torch.stack(H_raw, dim=0)\n",
        "    H_in = torch.stack(H_in, dim=0)\n",
        "    e = torch.stack(e, dim=0)\n",
        "    xTx = torch.stack(xTx, dim=0)\n",
        "    xTy = torch.stack(xTy, dim=0)\n",
        "    Di = torch.stack(Di, dim=0)\n",
        "    # steering = torch.stack(steering, dim=0)\n",
        "\n",
        "    return H_true, H_raw, H_in, e, xTx, xTy, Di"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGhdBsghq3M9"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHceh5kuq3ZD"
      },
      "outputs": [],
      "source": [
        "class xv(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(xv, self).__init__()\n",
        "\n",
        "        self.alpha1 = torch.nn.parameter.Parameter(torch.rand(1))\n",
        "        self.alpha2 = torch.nn.parameter.Parameter(torch.tensor([0.5]))\n",
        "\n",
        "    def forward(self, Di, H, e, xTx, xTy):\n",
        "\n",
        "        xTxH = torch.bmm(xTx, H)\n",
        "\n",
        "        z    = H + torch.bmm(Di, torch.sub(xTy, xTxH)) + self.alpha1 * e\n",
        "\n",
        "        e    = torch.sub(xTy, xTxH)\n",
        "\n",
        "        H    = torch.add((1 - self.alpha2) * z, self.alpha2 * H)\n",
        "\n",
        "        return H, e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woRjD7lJssRq"
      },
      "outputs": [],
      "source": [
        "class model_driven(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(model_driven, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc2 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc3 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc4 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc5 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc6 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc7 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc8 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc9 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc10 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc11 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc12 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "\n",
        "        self.layer1=xv()\n",
        "        self.layer2=xv()\n",
        "        self.layer3=xv()\n",
        "        self.layer4=xv()\n",
        "        self.layer5=xv()\n",
        "        self.layer6=xv()\n",
        "\n",
        "    def forward(self, Di, H_in, e, xTx, xTy):\n",
        "        e = self.fc1(e)\n",
        "        e = self.fc2(e)\n",
        "        H, e = self.layer1(Di, H_in, e, xTx, xTy)\n",
        "        H = torch.tanh(H)\n",
        "\n",
        "        e = self.fc3(e)\n",
        "        e = self.fc4(e)\n",
        "        H, e = self.layer2(Di, H, e, xTx, xTy)\n",
        "        H = torch.tanh(H)\n",
        "\n",
        "        e = self.fc5(e)\n",
        "        e = self.fc6(e)\n",
        "        H, e = self.layer3(Di, H, e, xTx, xTy)\n",
        "        H = torch.tanh(H)\n",
        "\n",
        "        e = self.fc7(e)\n",
        "        e = self.fc8(e)\n",
        "        H, e = self.layer4(Di, H, e, xTx, xTy)\n",
        "        H = torch.tanh(H)\n",
        "\n",
        "        e = self.fc9(e)\n",
        "        e = self.fc10(e)\n",
        "        H, e = self.layer5(Di, H, e, xTx, xTy)\n",
        "        H = torch.tanh(H)\n",
        "\n",
        "        e = self.fc11(e)\n",
        "        e = self.fc12(e)\n",
        "        H, e = self.layer6(Di, H, e, xTx, xTy)\n",
        "\n",
        "        return H, e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZjihTOXq3kG"
      },
      "source": [
        "# Define model, optimizer, and loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp9fRd3gq3tw"
      },
      "outputs": [],
      "source": [
        "def def_model():\n",
        "    model = model_driven()\n",
        "    loss = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    folder_model = './model/'\n",
        "\n",
        "    if not os.path.isdir(folder_model):\n",
        "        os.makedirs(folder_model)\n",
        "\n",
        "    file_model = folder_model + 'H'\n",
        "    # if os.path.isfile(file_model):\n",
        "    #     generator = torch.load(file_model)\n",
        "\n",
        "    record_file = 'H'\n",
        "    return model, loss, optimizer, record_file, file_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYWM7SzItKzS"
      },
      "source": [
        "# Main program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jv7lDwyxtFe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42a4248a-ce61-4c2b-a847-a6a473652977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-3e5f3c45b18a>:6: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  dP[i] = abs(IFsig[i])**2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dữ liệu đã được sinh và lưu lại!\n",
            "Begin training...\n",
            "71.05542434074431\n",
            "70.96783107199315\n",
            "73.87560265002789\n",
            "74.96624600129628\n",
            "73.65998788066815\n",
            "74.00754518523793\n",
            "75.76663384294378\n",
            "75.46079512413549\n",
            "76.60057853696111\n",
            "75.44857325665166\n",
            "74.27539461519612\n",
            "74.36556000491986\n",
            "74.42856790498911\n",
            "71.5766647926624\n",
            "76.66358052392306\n",
            "73.43140593082771\n",
            "74.46705737804153\n",
            "74.21668537540631\n",
            "75.47478374067666\n",
            "75.92017966755762\n",
            "75.83185208943888\n",
            "76.30248557656667\n",
            "75.24654178435962\n",
            "75.32422882631437\n",
            "75.5605750469244\n",
            "73.6309068143351\n",
            "75.95143376466852\n",
            "74.71364690802673\n",
            "75.25826670842194\n",
            "74.49318106180137\n",
            "73.70795139270382\n",
            "75.35484286149983\n",
            "76.39687667549367\n",
            "73.82309996130655\n",
            "74.97316540284973\n",
            "74.54814205628308\n",
            "72.27398839357612\n",
            "77.36781880884656\n",
            "75.57321079169166\n",
            "74.54878632945301\n",
            "74.7104741220142\n",
            "74.32309875542309\n",
            "76.71205166399356\n",
            "75.75302730390682\n",
            "75.60806257141184\n",
            "71.3739527090421\n",
            "72.36604545517304\n",
            "74.61367505995383\n",
            "73.65247520252622\n",
            "76.7139478455716\n",
            "73.63041953053293\n",
            "76.0854999139813\n",
            "74.92382471180491\n",
            "74.70165324943908\n",
            "72.88078977965209\n",
            "72.61297685066245\n",
            "76.22205025494642\n",
            "76.18774383533423\n",
            "74.39397903206381\n",
            "74.7917714594939\n",
            "75.06400506613947\n",
            "73.89393355394839\n",
            "73.13257511586473\n",
            "74.71336122874075\n",
            "74.42522367208997\n",
            "72.81100879195714\n",
            "75.97450498092367\n",
            "71.1779244606848\n",
            "74.11990963044146\n",
            "73.56620154934673\n",
            "71.39184638753036\n",
            "74.23600971180488\n",
            "72.59277953160814\n",
            "71.3101571265283\n",
            "75.26993131882561\n",
            "74.82054354558116\n",
            "73.94648290054306\n",
            "72.97755283979625\n",
            "73.21063527859869\n",
            "75.12700018420314\n",
            "74.55480823627205\n",
            "76.73816376194134\n",
            "74.45052854778639\n",
            "73.66391103901383\n",
            "74.03541989696292\n",
            "74.00548677264682\n",
            "73.67536496511421\n",
            "76.12364689639982\n",
            "75.25255800707598\n",
            "76.42878118241867\n",
            "74.74645637885432\n",
            "75.36453250074865\n",
            "73.76286207140784\n",
            "76.30910403403826\n",
            "73.14011289156032\n",
            "73.92000990813558\n",
            "75.62266204097195\n",
            "74.29677179414716\n",
            "74.03376710254012\n",
            "75.04388772827184\n",
            "73.73040171493555\n",
            "71.87142961899768\n",
            "77.17259512543573\n",
            "73.36159262982315\n",
            "75.2138730679471\n",
            "75.29659439144704\n",
            "76.49009832950172\n",
            "75.27933075461021\n",
            "76.88588959211843\n",
            "75.05052112711265\n",
            "75.56202734152532\n",
            "74.9190306394474\n",
            "74.02288987893473\n",
            "76.08797519274074\n",
            "73.3688295770327\n",
            "73.46064126059899\n",
            "71.97225147598533\n",
            "73.95521871449105\n",
            "74.54109259384012\n",
            "72.82711728653409\n",
            "72.88400605437336\n",
            "72.00908280894514\n",
            "73.55800919961298\n",
            "73.67796214110501\n",
            "76.11879321165891\n",
            "74.22950677235254\n",
            "74.6296006811012\n",
            "76.29840341265346\n",
            "75.15268506288588\n",
            "76.18830460755497\n",
            "73.62576600323631\n",
            "75.77001621429538\n",
            "73.4470736689341\n",
            "74.5005135976368\n",
            "74.373232416758\n",
            "72.78580392161213\n",
            "72.86170003539821\n",
            "74.41296925582878\n",
            "75.19703056350653\n",
            "72.47221410777954\n",
            "76.30611432946748\n",
            "74.12453187070943\n",
            "74.56849513883989\n",
            "73.0632327503684\n",
            "73.89636720885967\n",
            "74.05873504091034\n",
            "73.3719953689481\n",
            "76.7899899637156\n",
            "73.747093611051\n",
            "74.25200437573953\n",
            "74.03980816728166\n",
            "71.7309521108373\n",
            "73.59748763511953\n",
            "74.34167428784961\n",
            "74.94207005513664\n",
            "75.70985498178851\n",
            "75.49453666107263\n",
            "73.73928835190935\n",
            "74.8529269707629\n",
            "75.02299648983988\n",
            "73.31721493020821\n",
            "75.2417248714147\n",
            "74.8628429998068\n",
            "74.03762539257602\n",
            "74.8732246630014\n",
            "73.93122881147067\n",
            "74.9200983568007\n",
            "72.46037062579857\n",
            "75.94164712937024\n",
            "75.97192111165317\n",
            "72.80991702857027\n",
            "72.50043699345466\n",
            "73.78586112116075\n",
            "72.77683413201646\n",
            "73.75716236301267\n",
            "76.59721419920506\n",
            "73.33796243753368\n",
            "70.01021459582212\n",
            "74.49193085532013\n",
            "73.31163682270335\n",
            "74.54831254593911\n",
            "75.25124373220649\n",
            "75.23799063552121\n",
            "72.63049883651532\n",
            "73.78246381342157\n",
            "73.82934904735832\n",
            "76.71501396105684\n",
            "74.72276915163879\n",
            "75.67550083931053\n",
            "75.29981243562294\n",
            "77.0544398548808\n",
            "74.2389692297705\n",
            "73.09768700922551\n",
            "74.90138412697786\n",
            "77.64346751271083\n",
            "73.3612827342269\n",
            "75.36024747485862\n",
            "75.41492697841502\n",
            "73.62314315421861\n",
            "74.6573875409822\n",
            "73.86459914196966\n",
            "76.01521262482163\n",
            "74.49776906192307\n",
            "76.21359183623507\n",
            "76.00301986099072\n",
            "74.31948476838937\n",
            "74.12577879154809\n",
            "77.33821829443033\n",
            "74.46828832739884\n",
            "72.30300114223348\n",
            "75.97296140320053\n",
            "75.04478442782089\n",
            "75.9922333291311\n",
            "74.23271072041321\n",
            "75.963566258587\n",
            "75.21833423548314\n",
            "75.97824549011183\n",
            "74.69335696114982\n",
            "77.51749928213002\n",
            "76.21452422141857\n",
            "75.51216175827307\n",
            "73.59624510196831\n",
            "72.97597563207266\n",
            "72.00575602892411\n",
            "72.68473083607107\n",
            "75.81944433981944\n",
            "75.586274007762\n",
            "75.53866940599104\n",
            "75.15088750704825\n",
            "72.88689499084575\n",
            "75.00901049325822\n",
            "72.64814201946047\n",
            "75.11008424360465\n",
            "74.97758027704145\n",
            "77.12163824146941\n",
            "75.96527405741473\n",
            "75.80783471285734\n",
            "73.82648280105889\n",
            "76.64273184434313\n",
            "75.71768760803938\n",
            "73.44380101410351\n",
            "74.65861113118339\n",
            "74.5173082781477\n",
            "75.06230001329419\n",
            "75.77234241370773\n",
            "72.3748121895895\n",
            "76.4527899911382\n",
            "74.59011742622857\n",
            "74.89311092698883\n",
            "73.49850549738709\n",
            "74.496113535578\n",
            "75.17940312909431\n",
            "75.16663435502984\n",
            "76.13210582024082\n",
            "74.55715734930655\n",
            "78.4010798575036\n",
            "73.24288402038164\n",
            "75.37462223866828\n",
            "78.47873731204031\n",
            "75.96042341325533\n",
            "73.29632990767364\n",
            "72.84523984208501\n",
            "76.37960828355882\n",
            "75.92452045534603\n",
            "74.69000188444208\n",
            "73.33305273751874\n",
            "74.04180915797754\n",
            "71.66784152119139\n",
            "75.24746397899675\n",
            "73.8828357917233\n",
            "74.54892483093455\n",
            "74.70863539602261\n",
            "70.03711661351316\n",
            "73.24686114259312\n",
            "74.64038924058481\n",
            "75.68227252711476\n",
            "75.9037512805647\n",
            "74.58446429069305\n",
            "74.26279625475583\n",
            "76.26085953933125\n",
            "75.19481683738151\n",
            "74.55642069775085\n",
            "75.77531812987061\n",
            "75.74294692876997\n",
            "76.95599210816971\n",
            "74.15602144333468\n",
            "75.59399939678654\n",
            "74.93350315384552\n",
            "74.94655099178873\n",
            "74.65971437688818\n",
            "76.50186788195096\n",
            "73.22379164419185\n",
            "73.63165448896862\n",
            "74.4796204083847\n",
            "74.12056824313389\n",
            "74.40449968925267\n",
            "74.50676485030054\n",
            "73.54558209069502\n",
            "75.3700635304396\n",
            "73.13892601169684\n",
            "74.92493862135484\n",
            "71.98366159881611\n",
            "74.83166139243143\n",
            "74.39681373505815\n",
            "74.50897457786054\n",
            "74.637926160416\n",
            "74.03342335593372\n",
            "74.20110428321703\n",
            "74.30325756519721\n",
            "75.64620353852678\n",
            "75.9018166621134\n",
            "75.19332510683718\n",
            "76.87319804506232\n",
            "74.24999590166955\n",
            "77.05272368536102\n",
            "76.25162451091633\n",
            "74.3105214730305\n",
            "71.56993658844154\n",
            "74.77629554364526\n",
            "72.2645831292423\n",
            "72.92722236888257\n",
            "74.95653868274755\n",
            "75.07464123544246\n",
            "75.75400218905112\n",
            "72.08900733713175\n",
            "72.78958710331742\n",
            "75.13015331722052\n",
            "72.59610249129715\n",
            "76.97934364574635\n",
            "75.30828308995112\n",
            "77.29401227564506\n",
            "76.01676312024962\n",
            "74.8395632760384\n",
            "75.71026419044094\n",
            "74.2361957644814\n",
            "75.69743618051622\n",
            "74.4835917750745\n",
            "75.68849636391492\n",
            "75.75191964186648\n",
            "75.15077114286248\n",
            "77.42688463580127\n",
            "74.70688370294873\n",
            "74.40744617296134\n",
            "74.47767314084464\n",
            "75.60884821101827\n",
            "75.81783945394305\n",
            "73.46167360982618\n",
            "74.75018271606963\n",
            "75.37331747196467\n",
            "74.01419298831551\n",
            "75.72949342359681\n",
            "73.19731606653139\n",
            "74.7496307328772\n",
            "76.74840792647554\n",
            "73.90469401612192\n",
            "72.51010828962735\n",
            "75.65262028616932\n",
            "73.07165051327044\n",
            "74.68205999878455\n",
            "71.70301922977723\n",
            "75.08769196797343\n",
            "76.2884173402989\n",
            "73.7129724949968\n",
            "76.4939031658522\n",
            "76.18074212966535\n",
            "75.83276798906769\n",
            "75.47009690561003\n",
            "77.23713747714513\n",
            "74.04510612597686\n",
            "76.34901569491095\n",
            "74.77450489271378\n",
            "75.02695543265956\n",
            "74.18808096969518\n",
            "76.02982856837856\n",
            "72.95820851918114\n",
            "75.12616794038696\n",
            "75.21877116798584\n",
            "74.67800872080632\n",
            "74.76141640432894\n",
            "74.63314511225988\n",
            "74.80494496851848\n",
            "73.30321852877123\n",
            "75.75217930728539\n",
            "72.14847534605117\n",
            "72.2315049676093\n",
            "75.17757142280585\n",
            "74.04501270791157\n",
            "73.53477375285739\n",
            "76.35199258127278\n",
            "76.10211116063162\n",
            "76.37099809702217\n",
            "75.44000329568\n",
            "74.8027950890244\n",
            "71.7561832653141\n",
            "75.39721145989283\n",
            "73.7369205525716\n",
            "75.9883797650318\n",
            "72.14804275461924\n",
            "75.6520181786954\n",
            "73.11958641310034\n",
            "72.94482703868692\n",
            "73.84884509857143\n",
            "75.3590079069043\n",
            "75.19229323107005\n",
            "75.74569113902649\n",
            "72.42598645133897\n",
            "74.38341266525644\n",
            "72.38922807064881\n",
            "74.92894827546856\n",
            "75.77556066423243\n",
            "75.76663720349768\n",
            "73.41994124551478\n",
            "72.80721434528874\n",
            "74.09991397494301\n",
            "76.70319102364455\n",
            "74.77081773898497\n",
            "73.28535686757812\n",
            "74.6214111472246\n",
            "74.10377814022885\n",
            "74.16832978045888\n",
            "76.13328406647788\n",
            "73.70303769519127\n",
            "74.94967468076491\n",
            "76.03238647145099\n",
            "75.07582114319412\n",
            "72.3916561914442\n",
            "75.11479431017261\n",
            "77.6358207290999\n",
            "71.13679105964692\n",
            "74.75923487496777\n",
            "72.75149615539286\n",
            "74.37105351563125\n",
            "74.15356489246676\n",
            "75.34153025032182\n",
            "73.64096599854341\n",
            "74.0191807205471\n",
            "74.25669823995942\n",
            "74.51475224276548\n",
            "73.49817647867472\n",
            "75.38128447806962\n",
            "75.68618776089977\n",
            "76.20430776166782\n",
            "74.32425899994094\n",
            "72.43696122436218\n",
            "74.46624079615562\n",
            "74.21755410781257\n",
            "74.72986242268529\n",
            "75.1502114746675\n",
            "76.07525834727696\n",
            "72.932968547311\n",
            "74.23611987138962\n",
            "71.91202835823819\n",
            "76.30460257998203\n",
            "75.8248991477592\n",
            "77.1828378091299\n",
            "72.67148762146243\n",
            "71.11238564972982\n",
            "73.35000462110405\n",
            "72.54726399305784\n",
            "75.76139368032713\n",
            "76.53584268611321\n",
            "75.58360597466339\n",
            "73.81472127725068\n",
            "75.24216044717171\n",
            "75.7855991528006\n",
            "75.67044055257702\n",
            "75.53244222584463\n",
            "75.17595866653322\n",
            "72.41303610507413\n",
            "75.29114468645724\n",
            "74.32259937146043\n",
            "72.35914504506077\n",
            "73.60017125242356\n",
            "72.19380185765783\n",
            "74.92786649960115\n",
            "72.18475984004598\n",
            "75.32090183426072\n",
            "77.39319054380904\n",
            "74.16129494075426\n",
            "75.093344778139\n",
            "73.88743386632581\n",
            "75.8427000695729\n",
            "74.89766144282142\n",
            "73.2023529013094\n",
            "71.59372755002033\n",
            "73.10609700779943\n",
            "73.88432769085227\n",
            "73.41516139410761\n",
            "75.82567751913069\n",
            "76.614682360867\n",
            "73.94122769765305\n",
            "75.11929086962816\n",
            "73.22501798092115\n",
            "72.53926165139178\n",
            "73.36004636302523\n",
            "72.09511737439345\n",
            "71.74318191869317\n",
            "72.54435324557238\n",
            "74.91253197834173\n",
            "76.76125813879777\n",
            "74.66546256575064\n",
            "73.79068633178031\n",
            "76.40844715933822\n",
            "75.31034286175695\n",
            "74.3658391488318\n",
            "75.16650513235147\n",
            "77.19030014926773\n",
            "75.7715681286785\n",
            "75.8377070600007\n",
            "74.47140906303558\n",
            "74.79959613709441\n",
            "74.13896605255803\n",
            "1 0.04571050740501744 27.172927033205173\n",
            "100 0.0005808246982423917 0.7492667429476506\n",
            "200 0.0017705363868158021 1.9048402506190336\n",
            "300 0.0005848885308334011 0.7441474505351141\n",
            "400 0.0005966229539645021 0.7492000817721629\n",
            "500 0.0013436940481676139 1.4495728320537946\n"
          ]
        }
      ],
      "source": [
        "epoch         = 0\n",
        "expected_epoch = 20000\n",
        "num_samp      = N_samp * len(SNR)\n",
        "best_nmse     = 1e9\n",
        "early_stop    = 0\n",
        "best_model    = ''\n",
        "batch_size    = int(num_samp / 512)\n",
        "\n",
        "# Kiểm tra nếu file tĩnh tồn tại\n",
        "if os.path.exists('dataset_ISDNN.pkl'):\n",
        "    with open('dataset_ISDNN.pkl', 'rb') as f:\n",
        "        DataSet_x, DataSet_y, DataSet_H, DataSet_HH, H_true, H_raw, H_in, e, xTx, xTy, Di = pickle.load(f)\n",
        "    print(\"Dữ liệu đã được tải từ file tĩnh!\")\n",
        "else:\n",
        "    # Sinh dữ liệu nếu file tĩnh không tồn tại\n",
        "    DataSet_x, DataSet_y, DataSet_H, DataSet_HH = Gen_dataset('train', 0, 0, N_samp)\n",
        "    H_true, H_raw, H_in, e, xTx, xTy, Di = Input_ISDNN('train', DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_samp)\n",
        "\n",
        "    # Lưu dữ liệu để lần sau không phải sinh lại\n",
        "    with open('dataset_ISDNN.pkl', 'wb') as f:\n",
        "        pickle.dump((DataSet_x, DataSet_y, DataSet_H, DataSet_HH, H_true, H_raw, H_in, e, xTx, xTy, Di), f)\n",
        "    print(\"Dữ liệu đã được sinh và lưu lại!\")\n",
        "\n",
        "print(\"Begin training...\")\n",
        "starttime = timeit.default_timer()\n",
        "\n",
        "while(True):\n",
        "        epoch = epoch + 1\n",
        "\n",
        "        init_loss = 1e9\n",
        "        while( epoch == 1 and init_loss > 200):\n",
        "\n",
        "                model, loss, optimizer, record_file, file_model = def_model()\n",
        "                for bs in range (int(num_samp / batch_size)):\n",
        "                    H_1, e_1 = model(Di[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 H_in[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 e[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTx[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTy[0 + batch_size * bs:batch_size * (bs+1), :, :])   # predict output from the model\n",
        "                    init_loss = loss(H_1, H_true[0 + batch_size * bs:batch_size * (bs+1), :, :]).item()\n",
        "                    print(init_loss)\n",
        "\n",
        "        optimizer.zero_grad()   # zero the parameter gradients\n",
        "        train_loss = 0\n",
        "        H_f = torch.empty([num_samp, 2*Nt, 2*Nr])\n",
        "        for bs in range (int(num_samp / batch_size)):\n",
        "                H_o, e_o = model(Di[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 H_in[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 e[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTx[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTy[0 + batch_size * bs:batch_size * (bs+1), :, :])   # predict output from the model\n",
        "                H_f[0 + batch_size * bs:batch_size * (bs+1), :, :] = H_o\n",
        "                train_loss = loss(H_o,\n",
        "                                  H_true[0 + batch_size * bs:batch_size * (bs+1), :, :])   # calculate loss for the predicted output\n",
        "                train_loss.backward()   # backpropagate the loss\n",
        "                optimizer.step()        # adjust parameters based on the calculated gradients\n",
        "\n",
        "        if (epoch % 100 == 0 or epoch == 1):\n",
        "                nmse = 0\n",
        "                for j in range (num_samp):\n",
        "                        nmse += NMSE(H_f[j], H_raw[j])\n",
        "                nmse = nmse / num_samp\n",
        "\n",
        "                if (nmse <= best_nmse):\n",
        "                        torch.save(model.state_dict(), file_model + '_' + str(epoch) + '.pth')\n",
        "                        best_model = file_model + '_' + str(epoch) + '.pth'\n",
        "                        best_nmse = nmse\n",
        "                        early_stop = 0\n",
        "                else:\n",
        "                        early_stop += 1\n",
        "\n",
        "                if (nmse > best_nmse and early_stop == 3):\n",
        "                        with Record(record_file + '_log.txt'):\n",
        "                                print(epoch, nmse.item(), train_loss.item())\n",
        "                                print(str(timeit.default_timer()-starttime))\n",
        "                        break\n",
        "\n",
        "                with Record(record_file + '_log.txt'):\n",
        "                        print(epoch, nmse.item(), train_loss.item())\n",
        "\n",
        "        if epoch  == expected_epoch:\n",
        "                torch.save(model.state_dict(), file_model + '_' + str(epoch) + '.pth')\n",
        "                best_model = file_model + '_' + str(epoch) + '.pth'\n",
        "                with Record(record_file + '_log.txt'):\n",
        "                        print(\"epoch:\\n\", epoch)\n",
        "                        print(\"Latest NMSE:\\n\", nmse.item())\n",
        "                        print(\"Latest Loss:\\n\", train_loss.item())\n",
        "                        print(str(timeit.default_timer()-starttime))\n",
        "\n",
        "                break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOBK-_l-tRMO"
      },
      "source": [
        "# Test function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRRFGAX4tg_6"
      },
      "source": [
        "# Function to test the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#best_model = './model/H_500.pth'"
      ],
      "metadata": {
        "id": "hbzTeVvCcKJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfvpexfwthNq"
      },
      "outputs": [],
      "source": [
        "def test(H_raw, Di, H_in, e, xTx, xTy, N_test, log):\n",
        "    # Load the model that we saved at the end of the training loop\n",
        "    model = model_driven()\n",
        "    model.load_state_dict(torch.load(best_model, map_location=torch.device('cpu')))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        H_o, e_o = model(Di, H_in, e, xTx, xTy)\n",
        "\n",
        "        nmse = 0\n",
        "        for j in range (N_test):\n",
        "            nmse += NMSE(H_o[j], H_raw[j])\n",
        "\n",
        "        nmse = nmse / N_test\n",
        "        with Record(log):\n",
        "            print(format(nmse.item(), '.7f'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlrHA2aWimlC"
      },
      "outputs": [],
      "source": [
        "## Generate dataset for test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE_IiAiKimlC"
      },
      "outputs": [],
      "source": [
        "def LS(DataSet_x, DataSet_y):\n",
        "    start = timeit.default_timer()\n",
        "    for i in range (len(DataSet_x)):\n",
        "        H_hat = np.matmul(\n",
        "                    np.matmul(\n",
        "                        np.linalg.pinv(np.matmul(DataSet_x[i].transpose(), DataSet_x[i])),\n",
        "                        DataSet_x[i].transpose()),\n",
        "                        DataSet_y[i])\n",
        "    print(timeit.default_timer() - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "PAf8LBUFimlC"
      },
      "outputs": [],
      "source": [
        "SNR_min_dB  = 0\n",
        "SNR_max_dB  = 20\n",
        "step_dB     = 5\n",
        "num_dB      = int((SNR_max_dB - SNR_min_dB) / step_dB) + 1\n",
        "\n",
        "SNR         = np.linspace(SNR_min_dB, SNR_max_dB, num=num_dB)\n",
        "log         = './model/log_test.txt'\n",
        "\n",
        "N_test = int(num_samp * 30/100)\n",
        "\n",
        "for i in range (100):\n",
        "    for snr in SNR:\n",
        "        # with Record(log):\n",
        "        #     print(snr)\n",
        "        DataSet_x, DataSet_y, DataSet_H, DataSet_HH = Gen_dataset('test', snr, 0, N_test)\n",
        "        H_true, H_raw, H_in, e, xTx, xTy, Di = Input_ISDNN('test', DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_test)\n",
        "\n",
        "        # LS(DataSet_x, DataSet_y)\n",
        "        test(H_raw, Di, H_in, e, xTx, xTy, N_test, log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6_n95BiimlC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}