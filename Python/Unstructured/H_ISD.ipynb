{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSmcwLZuptDU"
      },
      "source": [
        "# Proposed Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG5fZn_ypunP"
      },
      "source": [
        "# **Biblioheque**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "--SvzvIspu-U",
        "outputId": "80234aee-1050-43af-904c-672e1f9dd387",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
            "  _C._set_default_tensor_type(t)\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from scipy.stats import rice\n",
        "import pickle\n",
        "# import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import sys\n",
        "import timeit\n",
        "import os\n",
        "\n",
        "torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
        "torch.set_default_dtype(torch.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBk01Vu0pvMm"
      },
      "source": [
        "# class to save results in file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "06IymVCkpvYS"
      },
      "outputs": [],
      "source": [
        "class Record:\n",
        "    def __init__(self, TextName):\n",
        "        self.out_file = open(TextName, 'a')\n",
        "        self.old_stdout = sys.stdout\n",
        "        sys.stdout = self\n",
        "\n",
        "    def write(self, text):\n",
        "        self.old_stdout.write(text)\n",
        "        self.out_file.write(text)\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout = self.old_stdout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xPamGjNpv8E"
      },
      "source": [
        "# **slicer the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dlQdA0xHpwF7"
      },
      "outputs": [],
      "source": [
        "def slicer(data):\n",
        "    dataI = data[slice(0, len(data), 2)]\n",
        "    dataQ = data[slice(1, len(data), 2)]\n",
        "    return(dataI, dataQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOoiIo_LpwQl"
      },
      "source": [
        "# **Modulation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xIftC8fvpwir"
      },
      "outputs": [],
      "source": [
        "def mapper_16QAM(QAM16, data):\n",
        "    map0 = 2*data[slice(0, len(data), 2)] + data[slice(1, len(data), 2)]\n",
        "    map0 = list(map(int, map0))\n",
        "    dataMapped = []\n",
        "    for i in range(len(map0)):\n",
        "        dataMapped.append(QAM16[map0[i]])\n",
        "    return(dataMapped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AYhPgAIcqtGT"
      },
      "outputs": [],
      "source": [
        "def calculate_bits(Modulation,NumSubcarriers,NumDataSymb):\n",
        "    if Modulation=='QPSK':\n",
        "        Nbpscs=2\n",
        "    elif Modulation=='16QAM':\n",
        "        Nbpscs=4\n",
        "    return NumDataSymb*NumSubcarriers*Nbpscs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgX6tiqFpwvb"
      },
      "source": [
        "# **generate noise**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2LQwZvGaprKZ"
      },
      "outputs": [],
      "source": [
        "def AWGN(IFsig, SNR):\n",
        "    dP = np.zeros(len(IFsig))\n",
        "    P = 0\n",
        "\n",
        "    for i in range(len(IFsig)):\n",
        "        dP[i] = abs(IFsig[i])**2\n",
        "        P = P + dP[i]\n",
        "\n",
        "    P = P/len(IFsig)\n",
        "    gamma = 10**(SNR/10)\n",
        "    N0 = P/gamma\n",
        "    n = ((N0/2)**(0.5))*np.random.standard_normal(len(IFsig))\n",
        "    IF_n = np.zeros((len(IFsig),1))\n",
        "\n",
        "    for i in range(len(IFsig)):\n",
        "        IF_n[i,:] = IFsig[i] + n[i]\n",
        "\n",
        "    return(IF_n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDx6TEAyimk9"
      },
      "source": [
        "# Generate channel model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DnoArgs7imk9"
      },
      "outputs": [],
      "source": [
        "def Generate_channel(Nr, Nt, type):\n",
        "    if (type == 'gauss'):\n",
        "        return (np.random.normal(size=(Nr,Nt))+1j*np.random.normal(size=(Nr,Nt)))/np.sqrt(2)\n",
        "    if (type == 'rayleigh'):\n",
        "        return (np.random.rayleigh(scale=(1/np.sqrt(2)), size=(Nr,Nt)) + 1j*np.random.rayleigh(scale=(1/np.sqrt(2)), size=(Nr,Nt)))/np.sqrt(2)\n",
        "    if (type == 'rician'):\n",
        "        b = 1/np.sqrt(2)\n",
        "        return (rice.rvs(b, size=(Nr,Nt)) + 1j*rice.rvs(b, size=(Nr,Nt)))/np.sqrt(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eciNtnFjq2yd"
      },
      "source": [
        "# **Generate Dataset**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AE4Q6jZCq3CY"
      },
      "outputs": [],
      "source": [
        "DataSet_x   = []  # x dataset after modulation\n",
        "DataSet_y   = []  # y dataset\n",
        "DataSet_HH  = []  # H dataset\n",
        "DataSet_b   = []  # binary dataset\n",
        "SNR_min_dB  = 0\n",
        "SNR_max_dB  = 20\n",
        "step_dB     = 5\n",
        "num_dB      = int((SNR_max_dB - SNR_min_dB) / step_dB) + 1\n",
        "\n",
        "SNR         = np.linspace(SNR_min_dB, SNR_max_dB, num=num_dB)\n",
        "\n",
        "\n",
        "Nt = 8             # Tx: 8\n",
        "Nr = 64            # Rx: 128\n",
        "N_samp = 4000\n",
        "\n",
        "\n",
        "def Gen_dataset(mode, snr, imperfect, N_samp):\n",
        "    DataSet_x   = []  # x dataset after modulation\n",
        "    DataSet_y   = []  # y dataset\n",
        "    DataSet_H   = []\n",
        "    DataSet_HH  = []\n",
        "\n",
        "    NumSubcarriers = 1\n",
        "    Modulation = '16QAM'\n",
        "    QAM16 = [-1, -0.333, 0.333, 1]\n",
        "    NumDataSymb = 1\n",
        "    N_type = 'gauss'\n",
        "\n",
        "    if mode == 'train':\n",
        "        for snr in SNR:\n",
        "            for runIdx in range(0, N_samp):      # ! 20000 x Nt: samples\n",
        "                H = Generate_channel(Nt, Nr, N_type)\n",
        "                HH = np.concatenate((np.concatenate((H.real, H.imag), axis=1),\n",
        "                                    np.concatenate((-H.imag, H.real), axis=1)), axis=0)\n",
        "                x = np.zeros((2*Nt, NumSubcarriers))\n",
        "                a = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "                DataRaw = np.zeros((Nt, a))\n",
        "                for t in range(Nt):\n",
        "                    #\"data symbol generate\"\n",
        "                    NumBits = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "                    bit = np.random.randint(1, 3, NumBits)-1\n",
        "                    DataRaw[t, :] = bit\n",
        "                    for j in range(4):\n",
        "                        DataSet_b.append(bit[j])\n",
        "                    I = np.zeros((1, a))\n",
        "                    I[0, :] = DataRaw[t, :]\n",
        "                    (dataI, dataQ) = slicer(I[0])\n",
        "\n",
        "                    # Mapper\n",
        "                    mapI = mapper_16QAM(QAM16, dataI)\n",
        "                    mapQ = mapper_16QAM(QAM16, dataQ)\n",
        "                    x[t] = mapI[0]\n",
        "                    x[t+Nt] = mapQ[0]\n",
        "\n",
        "                # transpose\n",
        "                x = x.transpose()\n",
        "\n",
        "                y_wo_noise = np.matmul(x, HH)\n",
        "\n",
        "                # noise\n",
        "                noise = AWGN(y_wo_noise.transpose(), snr)\n",
        "\n",
        "                y = y_wo_noise + noise.transpose()\n",
        "\n",
        "                DataSet_x.append(x)    # ! I, Q sample distance by Nt.\n",
        "                DataSet_y.append(y)                 # ! output sample\n",
        "\n",
        "                # Imperfect channel: 5%\n",
        "                # coef = (2*np.random.randint(0,2,size=HH.shape) - 1)\n",
        "                # HH = HH + coef * HH * 0.05\n",
        "                DataSet_HH.append(HH)\n",
        "                DataSet_H.append(H)               # ! Generated channel\n",
        "\n",
        "    else:\n",
        "        for runIdx in range(0, N_samp):      # ! 20000 x Nt: samples\n",
        "            H = Generate_channel(Nt, Nr, N_type)\n",
        "            HH = np.concatenate((np.concatenate((H.real, H.imag), axis=1),\n",
        "                                np.concatenate((-H.imag, H.real), axis=1)), axis=0)\n",
        "            x = np.zeros((2*Nt, NumSubcarriers))\n",
        "            a = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "            DataRaw = np.zeros((Nt, a))\n",
        "            for t in range(Nt):\n",
        "                #\"data symbol generate\"\n",
        "                NumBits = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "                bit = np.random.randint(1, 3, NumBits)-1\n",
        "                DataRaw[t, :] = bit\n",
        "                for j in range(4):\n",
        "                    DataSet_b.append(bit[j])\n",
        "                I = np.zeros((1, a))\n",
        "                I[0, :] = DataRaw[t, :]\n",
        "                (dataI, dataQ) = slicer(I[0])\n",
        "\n",
        "                # Mapper\n",
        "                mapI = mapper_16QAM(QAM16, dataI)\n",
        "                mapQ = mapper_16QAM(QAM16, dataQ)\n",
        "                x[t] = mapI[0]\n",
        "                x[t+Nt] = mapQ[0]\n",
        "\n",
        "            # transpose\n",
        "            x = x.transpose()\n",
        "\n",
        "            y_wo_noise = np.matmul(x, HH)\n",
        "\n",
        "            # noise\n",
        "            noise = AWGN(y_wo_noise.transpose(), snr)\n",
        "\n",
        "            y = y_wo_noise + noise.transpose()\n",
        "\n",
        "            DataSet_x.append(x)    # ! I, Q sample distance by Nt.\n",
        "            DataSet_y.append(y)                 # ! output sample\n",
        "\n",
        "            # Imperfect channel: 5%\n",
        "            DataSet_HH.append(HH)\n",
        "            DataSet_H.append(H)               # ! Generated channel\n",
        "\n",
        "\n",
        "    # Shuffle dataset\n",
        "    random.seed(1)\n",
        "    temp = list(zip(DataSet_x, DataSet_y, DataSet_H, DataSet_HH))\n",
        "    random.shuffle(temp)\n",
        "    DataSet_x, DataSet_y, DataSet_H, DataSet_HH = zip(*temp)\n",
        "\n",
        "    return DataSet_x, DataSet_y, DataSet_H, DataSet_HH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1nWo2pTrimk-"
      },
      "outputs": [],
      "source": [
        "def reconstruct_channel (H):\n",
        "# H_raw = [R(H) I(H); -I(H) R(H)]\n",
        "# we have four version of H_est\n",
        "    H_est_1 = []\n",
        "    H_est_2 = []\n",
        "    H_est_3 = []\n",
        "    H_est_4 = []\n",
        "\n",
        "    H_est_Re_1 = H[0:Nt, 0:Nr]\n",
        "    H_est_Im_1 = H[0:Nt, Nr:2*Nr]\n",
        "    H_est_Im_2 = - H[Nt:2*Nt, 0:Nr]\n",
        "    H_est_Re_2 = H[Nt:2*Nt, Nr:2*Nr]\n",
        "\n",
        "    H_est_1 = H_est_Re_1 + 1j * H_est_Im_1\n",
        "    H_est_2 = H_est_Re_1 + 1j * H_est_Im_2\n",
        "    H_est_3 = H_est_Re_2 + 1j * H_est_Im_1\n",
        "    H_est_4 = H_est_Re_2 + 1j * H_est_Im_2\n",
        "\n",
        "    return H_est_1, H_est_2, H_est_3, H_est_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mQUDE5Q1imk-"
      },
      "outputs": [],
      "source": [
        "# def NMSE(H_est, H_raw):\n",
        "#     H_est_1, H_est_2, H_est_3, H_est_4 = reconstruct_channel(H_est)\n",
        "#     H_est_vec_1 = torch.reshape(H_est_1, [Nt * Nr, 1])\n",
        "#     H_est_vec_2 = torch.reshape(H_est_2, [Nt * Nr, 1])\n",
        "#     H_est_vec_3 = torch.reshape(H_est_3, [Nt * Nr, 1])\n",
        "#     H_est_vec_4 = torch.reshape(H_est_4, [Nt * Nr, 1])\n",
        "\n",
        "#     H_raw_vec = torch.reshape(H_raw, [Nt * Nr, 1])\n",
        "\n",
        "#     mse_1       = (torch.norm(H_raw_vec - H_est_vec_1)**2) / len(H_raw_vec)\n",
        "#     mse_2       = (torch.norm(H_raw_vec - H_est_vec_2)**2) / len(H_raw_vec)\n",
        "#     mse_3       = (torch.norm(H_raw_vec - H_est_vec_3)**2) / len(H_raw_vec)\n",
        "#     mse_4       = (torch.norm(H_raw_vec - H_est_vec_4)**2) / len(H_raw_vec)\n",
        "\n",
        "#     sigEner   = torch.norm(H_raw_vec)**2\n",
        "\n",
        "#     nmse_1      = mse_1 / sigEner\n",
        "#     nmse_2      = mse_2 / sigEner\n",
        "#     nmse_3      = mse_3 / sigEner\n",
        "#     nmse_4      = mse_4 / sigEner\n",
        "\n",
        "#     # Best nmse\n",
        "#     nmse        = min([nmse_1, nmse_2, nmse_3, nmse_4])\n",
        "\n",
        "#     return torch.abs(nmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "knU4OnL3imk_"
      },
      "outputs": [],
      "source": [
        "def NMSE(H_est, H_raw):\n",
        "    H_est_1, H_est_2, H_est_3, H_est_4 = reconstruct_channel(H_est)\n",
        "\n",
        "    # Lấy phần thực của các tensor nếu chúng là complex\n",
        "    H_est_vec_1 = torch.reshape(H_est_1, [Nt * Nr, 1]).abs()\n",
        "    H_est_vec_2 = torch.reshape(H_est_2, [Nt * Nr, 1]).abs()\n",
        "    H_est_vec_3 = torch.reshape(H_est_3, [Nt * Nr, 1]).abs()\n",
        "    H_est_vec_4 = torch.reshape(H_est_4, [Nt * Nr, 1]).abs()\n",
        "\n",
        "    H_raw_vec = torch.reshape(H_raw, [Nt * Nr, 1]).abs()\n",
        "\n",
        "    mse_1 = (torch.norm(H_raw_vec - H_est_vec_1)**2) / len(H_raw_vec)\n",
        "    mse_2 = (torch.norm(H_raw_vec - H_est_vec_2)**2) / len(H_raw_vec)\n",
        "    mse_3 = (torch.norm(H_raw_vec - H_est_vec_3)**2) / len(H_raw_vec)\n",
        "    mse_4 = (torch.norm(H_raw_vec - H_est_vec_4)**2) / len(H_raw_vec)\n",
        "\n",
        "    sigEner = torch.norm(H_raw_vec)**2\n",
        "\n",
        "    nmse_1 = mse_1 / sigEner\n",
        "    nmse_2 = mse_2 / sigEner\n",
        "    nmse_3 = mse_3 / sigEner\n",
        "    nmse_4 = mse_4 / sigEner\n",
        "\n",
        "    # Chọn NMSE tốt nhất\n",
        "    nmse = min([nmse_1, nmse_2, nmse_3, nmse_4])\n",
        "\n",
        "    return torch.abs(nmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pyjIgxUurU0P"
      },
      "outputs": [],
      "source": [
        "def Input_ISDNN(mode, DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_samp):\n",
        "    H_in = []        # ! H_in    , np.diag(np.diag()) return a diag matrix instead of diag components.\n",
        "    H_true = []   # ! generated s\n",
        "    H_raw = []\n",
        "    e = []        # ! vector errors\n",
        "    xTx = []\n",
        "    xTy = []\n",
        "    Di = []\n",
        "    # steering = [] # ! Steering vector: ZoA and AoA\n",
        "\n",
        "    if mode == 'train':\n",
        "        n_sample = N_samp * len(SNR)\n",
        "    else:\n",
        "        n_sample = N_samp\n",
        "\n",
        "    for i in range (n_sample):\n",
        "        H_true.append(torch.tensor(DataSet_HH[i]))\n",
        "        H_raw.append(torch.tensor(DataSet_H[i]))\n",
        "        Di.append(torch.tensor(np.linalg.pinv(np.diag(np.diag(np.dot(DataSet_x[i].transpose(), DataSet_x[i]))))))\n",
        "        xTy.append(torch.tensor(np.dot(DataSet_x[i].transpose(), DataSet_y[i])))\n",
        "        H_in.append(torch.matmul(Di[i], xTy[i]))\n",
        "        e.append(torch.rand([2*Nt, 2*Nr]))\n",
        "        xTx.append(torch.tensor(np.dot(DataSet_x[i].transpose(), DataSet_x[i])))\n",
        "        # steering.append(torch.tensor(DataSet_Steering[i]))\n",
        "\n",
        "    H_true = torch.stack(H_true, dim=0)\n",
        "    H_raw = torch.stack(H_raw, dim=0)\n",
        "    H_in = torch.stack(H_in, dim=0)\n",
        "    e = torch.stack(e, dim=0)\n",
        "    xTx = torch.stack(xTx, dim=0)\n",
        "    xTy = torch.stack(xTy, dim=0)\n",
        "    Di = torch.stack(Di, dim=0)\n",
        "    # steering = torch.stack(steering, dim=0)\n",
        "\n",
        "    return H_true, H_raw, H_in, e, xTx, xTy, Di"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGhdBsghq3M9"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GHceh5kuq3ZD"
      },
      "outputs": [],
      "source": [
        "class xv(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(xv, self).__init__()\n",
        "\n",
        "        self.alpha1 = torch.nn.parameter.Parameter(torch.rand(1))\n",
        "        self.alpha2 = torch.nn.parameter.Parameter(torch.tensor([0.5]))\n",
        "\n",
        "    def forward(self, Di, H, e, xTx, xTy):\n",
        "\n",
        "        xTxH = torch.bmm(xTx, H)\n",
        "\n",
        "        z    = H + torch.bmm(Di, torch.sub(xTy, xTxH)) + self.alpha1 * e\n",
        "\n",
        "        e    = torch.sub(xTy, xTxH)\n",
        "\n",
        "        H    = torch.add((1 - self.alpha2) * z, self.alpha2 * H)\n",
        "\n",
        "        return H, e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "woRjD7lJssRq"
      },
      "outputs": [],
      "source": [
        "class model_driven(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(model_driven, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc2 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc3 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc4 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc5 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc6 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc7 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc8 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        # self.fc9 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        # self.fc10 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "\n",
        "        self.layer1=xv()\n",
        "        self.layer2=xv()\n",
        "        self.layer3=xv()\n",
        "        self.layer4=xv()\n",
        "        # self.layer5=xv()\n",
        "\n",
        "    def forward(self, Di, H_in, e, xTx, xTy):\n",
        "        e = self.fc1(e)\n",
        "        e = self.fc2(e)\n",
        "        H, e = self.layer1(Di, H_in, e, xTx, xTy)\n",
        "        H = torch.tanh(H)\n",
        "\n",
        "        e = self.fc3(e)\n",
        "        e = self.fc4(e)\n",
        "        H, e = self.layer2(Di, H, e, xTx, xTy)\n",
        "        H = torch.tanh(H)\n",
        "\n",
        "        e = self.fc5(e)\n",
        "        e = self.fc6(e)\n",
        "        H, e = self.layer3(Di, H, e, xTx, xTy)\n",
        "        H = torch.tanh(H)\n",
        "\n",
        "        e = self.fc7(e)\n",
        "        e = self.fc8(e)\n",
        "        H, e = self.layer4(Di, H, e, xTx, xTy)\n",
        "        # H = torch.tanh(H)\n",
        "\n",
        "        # e = self.fc9(e)\n",
        "        # e = self.fc10(e)\n",
        "        # H, e = self.layer5(Di, H, e, xTx, xTy)\n",
        "\n",
        "        return H, e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZjihTOXq3kG"
      },
      "source": [
        "# Define model, optimizer, and loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Vp9fRd3gq3tw"
      },
      "outputs": [],
      "source": [
        "def def_model():\n",
        "    model = model_driven()\n",
        "    loss = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    folder_model = './model/'\n",
        "\n",
        "    if not os.path.isdir(folder_model):\n",
        "        os.makedirs(folder_model)\n",
        "\n",
        "    file_model = folder_model + 'H'\n",
        "    # if os.path.isfile(file_model):\n",
        "    #     generator = torch.load(file_model)\n",
        "\n",
        "    record_file = 'H'\n",
        "    return model, loss, optimizer, record_file, file_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYWM7SzItKzS"
      },
      "source": [
        "# Main program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jv7lDwyxtFe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd9ce874-699d-4d97-e07c-dcfd14679c5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-3e5f3c45b18a>:6: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  dP[i] = abs(IFsig[i])**2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dữ liệu đã được sinh và lưu lại!\n",
            "Begin training...\n",
            "152.89435574452386\n",
            "154.29157491659709\n",
            "153.0473556977317\n",
            "155.5322835386965\n",
            "157.59249006769272\n",
            "151.95272476490973\n",
            "154.28851762978934\n",
            "151.7172941231516\n",
            "151.8158465272164\n",
            "154.13886405254087\n",
            "158.74131196729965\n",
            "150.82712440478943\n",
            "152.54667590996223\n",
            "154.82154900250535\n",
            "153.11453081330887\n",
            "155.17338596676834\n",
            "155.4569966612316\n",
            "153.38816775495002\n",
            "152.97694639527666\n",
            "157.28221708905366\n",
            "150.8945579054071\n",
            "154.95786747875587\n",
            "158.02627942673257\n",
            "150.43887119230487\n",
            "154.4827665363413\n",
            "158.19394320400096\n",
            "158.15619370696436\n",
            "154.5289684382301\n",
            "154.03442833285956\n",
            "158.48303990917805\n",
            "150.34660973615456\n",
            "150.86978061362328\n",
            "152.66843652256543\n",
            "154.62104220822445\n",
            "153.13217225637473\n",
            "152.2834733987019\n",
            "150.56473645659753\n",
            "149.4305447457711\n",
            "154.84640463408672\n",
            "150.93907723791767\n",
            "154.01060611132453\n",
            "153.92657555392006\n",
            "152.02507832142877\n",
            "152.51284210150308\n",
            "152.02791353796835\n",
            "153.34421556972785\n",
            "155.34421998163413\n",
            "153.63733989870124\n",
            "154.9298847126988\n",
            "155.25679486409987\n",
            "151.14229036959824\n",
            "149.2992303840789\n",
            "150.0607535365168\n",
            "154.52292928377076\n",
            "150.74622965559342\n",
            "153.8025636160673\n",
            "152.73996395345455\n",
            "156.23338266324484\n",
            "151.62526751836012\n",
            "151.79922903066682\n",
            "151.86029650194433\n",
            "152.76556013649426\n",
            "155.33236606042286\n",
            "153.94078413884927\n",
            "153.51897080474092\n",
            "149.05693683080602\n",
            "148.0333249330742\n",
            "154.76256552286992\n",
            "151.37746685648028\n",
            "153.19943455480697\n",
            "154.16636005057958\n",
            "155.42739249949722\n",
            "151.81988381995302\n",
            "150.452657967783\n",
            "151.3637929349764\n",
            "153.7956864216074\n",
            "156.00888420192388\n",
            "158.06784398626021\n",
            "153.39365618892882\n",
            "157.8869023099301\n",
            "155.50552400591317\n",
            "155.55316882990877\n",
            "156.89773065081295\n",
            "158.29611413628066\n",
            "151.82859905740696\n",
            "149.27372913711915\n",
            "151.22630045344684\n",
            "150.06366530039796\n",
            "151.74229253505123\n",
            "154.50697604379275\n",
            "147.06080487507805\n",
            "152.98054607046456\n",
            "151.51555511404595\n",
            "152.82502269350636\n",
            "154.04335924907744\n",
            "152.40345110173828\n",
            "152.78049811072543\n",
            "156.67751963248523\n",
            "156.37058958162666\n",
            "155.80522988064922\n",
            "153.2134875889376\n",
            "152.4852933695476\n",
            "153.27259223126842\n",
            "153.2800746497862\n",
            "153.74349254754802\n",
            "153.62634711367903\n",
            "158.78065839690598\n",
            "150.4827011947144\n",
            "148.80546849347948\n",
            "159.46432200736965\n",
            "153.12094517505517\n",
            "148.23299349205382\n",
            "153.05896028335107\n",
            "154.5413612272712\n",
            "149.7671063746726\n",
            "151.81559812701445\n",
            "152.55262751614399\n",
            "144.4088246695117\n",
            "151.19454298798547\n",
            "152.05681734659197\n",
            "152.31472138573068\n",
            "150.76594976307572\n",
            "152.19860151956092\n",
            "153.0245153927576\n",
            "154.74945822172248\n",
            "154.461648277519\n",
            "151.50075259973522\n",
            "150.8614228740635\n",
            "155.10833505955335\n",
            "157.41673996427824\n",
            "151.05316588583017\n",
            "151.7620533113706\n",
            "151.89686319845126\n",
            "152.8347666455057\n",
            "148.64407484426195\n",
            "155.42947091278327\n",
            "156.5337381420472\n",
            "151.4617344096143\n",
            "151.40981397625947\n",
            "155.26279716391414\n",
            "152.73247783679267\n",
            "155.00928684171066\n",
            "155.39443026972148\n",
            "154.18054857957526\n",
            "156.64229100094076\n",
            "157.1297111665654\n",
            "152.1363679937289\n",
            "149.44036315804877\n",
            "153.03087064987452\n",
            "152.95957442026173\n",
            "154.53067897811252\n",
            "153.60226245930463\n",
            "155.6788844082096\n",
            "152.5055987782944\n",
            "154.30120968343638\n",
            "155.06842465357795\n",
            "150.50235588531416\n",
            "149.3925914198527\n",
            "156.63489273459422\n",
            "148.41516092754972\n",
            "149.97442166517732\n",
            "154.09222046747792\n",
            "152.23859169281954\n",
            "157.50673553420572\n",
            "154.08480011997096\n",
            "149.91696794632082\n",
            "151.18482493981287\n",
            "148.27735987743384\n",
            "154.51445320893686\n",
            "154.45290178813687\n",
            "157.7026421617212\n",
            "157.9218986505178\n",
            "150.2412381969483\n",
            "154.86736032515498\n",
            "149.40628810318287\n",
            "151.07113439940784\n",
            "147.797277278544\n",
            "150.49324322190472\n",
            "154.10945687436043\n",
            "152.8854909575281\n",
            "149.94977258214507\n",
            "156.30631091296192\n",
            "158.29507664793778\n",
            "152.69924969189967\n",
            "151.837758732202\n",
            "153.72853598955234\n",
            "152.73286202258703\n",
            "153.47929347893742\n",
            "152.28879185050656\n",
            "150.80242272319563\n",
            "151.55358448015875\n",
            "153.41054642636277\n",
            "152.03098484505247\n",
            "151.29334470091402\n",
            "154.11169284713674\n",
            "152.4200262361554\n",
            "150.28153572758313\n",
            "151.83347967868707\n",
            "154.042864156888\n",
            "151.36586497024393\n",
            "153.4083640594655\n",
            "155.0353368967778\n",
            "148.50497061022043\n",
            "154.98315158942262\n",
            "151.45118789515635\n",
            "154.79716460100292\n",
            "152.12000523494825\n",
            "151.77788406391875\n",
            "151.7650990813805\n",
            "153.39031920926348\n",
            "151.6882395928274\n",
            "157.07055295646796\n",
            "152.45376320872742\n",
            "157.2725686932795\n",
            "154.60399085617095\n",
            "153.03289688930937\n",
            "152.2691387278875\n",
            "158.41378496957287\n",
            "151.6877862330094\n",
            "157.10244512394314\n",
            "150.3271284239229\n",
            "153.491302033427\n",
            "157.47103579480174\n",
            "155.98545061684385\n",
            "151.92242280309338\n",
            "152.54620515159723\n",
            "153.62440894662652\n",
            "151.35161238323823\n",
            "155.043752166391\n",
            "156.71677680965936\n",
            "156.1903010893611\n",
            "147.68225878750854\n",
            "151.94259561816074\n",
            "152.67598997217036\n",
            "154.09003277127152\n",
            "152.2078591966689\n",
            "155.52772633076\n",
            "155.93403169393588\n",
            "156.72900307882148\n",
            "159.2488930882867\n",
            "148.8722838706238\n",
            "145.05120580228507\n",
            "159.55001999452804\n",
            "151.82965271229793\n",
            "155.69071963598242\n",
            "154.03620777263805\n",
            "156.87820205347862\n",
            "154.26180840694332\n",
            "151.8899565310233\n",
            "153.70117570607152\n",
            "151.7280393654389\n",
            "158.12284431182476\n",
            "150.68654160824386\n",
            "155.93851741433022\n",
            "154.15809199205046\n",
            "152.58422324068363\n",
            "157.55397380980622\n",
            "152.55494939154758\n",
            "151.2334532144404\n",
            "152.97393302474836\n",
            "155.59645683365918\n",
            "152.74482965853886\n",
            "155.84648876102207\n",
            "156.36878988777235\n",
            "155.0862016486416\n",
            "150.0218557165312\n",
            "155.1824322850574\n",
            "151.3411920177017\n",
            "154.94933137416913\n",
            "156.16076857365138\n",
            "155.63891658693956\n",
            "150.57604335651604\n",
            "149.5922296931928\n",
            "154.74950363369797\n",
            "156.3768556802756\n",
            "155.95138292118247\n",
            "157.5135744748293\n",
            "152.88162822236575\n",
            "155.72407625275267\n",
            "156.4560514733292\n",
            "150.64150548735037\n",
            "151.8221393859264\n",
            "153.91287534469055\n",
            "149.88136027694395\n",
            "152.05379778618752\n",
            "153.01794592190046\n",
            "150.90690351603098\n",
            "150.08361615067153\n",
            "147.85049690271998\n",
            "150.1506744844292\n",
            "149.06316527213005\n",
            "153.94167781755962\n",
            "157.05589073252014\n",
            "153.75175618615498\n",
            "150.73034634733557\n",
            "150.7579042756601\n",
            "152.62803027429374\n",
            "155.94344603420504\n",
            "155.14092274903115\n",
            "150.9259185134412\n",
            "155.4709259116374\n",
            "151.48207938413768\n",
            "154.1304098829056\n",
            "158.38769610515433\n",
            "151.20470723276182\n",
            "152.41191453716368\n",
            "155.00774206259777\n",
            "154.70802320645984\n",
            "153.46445884115684\n",
            "153.13688832917617\n",
            "153.59713559341333\n",
            "150.74442460502286\n",
            "152.34188320963347\n",
            "157.22356962348437\n",
            "152.11845125063488\n",
            "147.8359374588894\n",
            "150.77387104178308\n",
            "157.53069733010759\n",
            "146.5701202722775\n",
            "152.70551195672485\n",
            "146.7059310808331\n",
            "156.47943341559966\n",
            "155.3124054575065\n",
            "150.96358116479183\n",
            "156.52349224280263\n",
            "156.26803746845587\n",
            "153.52553552144403\n",
            "156.47627369782504\n",
            "153.36225372660604\n",
            "152.91325136596024\n",
            "153.27005162624127\n",
            "151.71499587488094\n",
            "154.20202798392611\n",
            "153.80763428854124\n",
            "155.0272951713999\n",
            "155.81157609356686\n",
            "155.68020583277942\n",
            "151.40196608234024\n",
            "155.88855720755157\n",
            "158.37099888065097\n",
            "153.7582109375452\n",
            "157.9069526563601\n",
            "146.73362580865543\n",
            "151.57327391644753\n",
            "155.59155120773104\n",
            "150.80134590473665\n",
            "157.33111963638274\n",
            "151.81723099727367\n",
            "155.62815412610908\n",
            "154.6231263772069\n",
            "151.0504954140418\n",
            "153.33295493454935\n",
            "157.04458875933184\n",
            "154.56232640941775\n",
            "156.18983541371662\n",
            "154.8691289259545\n",
            "150.56348943487882\n",
            "151.4711816180853\n",
            "154.46616748395473\n",
            "151.37119008432927\n",
            "156.27120177531862\n",
            "149.30530117411666\n",
            "153.73114804529143\n",
            "151.77343286895993\n",
            "149.0582910859433\n",
            "156.79719999763864\n",
            "153.0417638740977\n",
            "151.008331157995\n",
            "152.82460909851812\n",
            "151.05420047076868\n",
            "148.92969320880243\n",
            "155.32977888520176\n",
            "151.01721085883014\n",
            "155.07682066925247\n",
            "150.84709662275657\n",
            "153.04978045837768\n",
            "156.72928710270418\n",
            "153.3409949487348\n",
            "156.19671368221955\n",
            "154.56373660601227\n",
            "153.370769197985\n",
            "154.4445043668521\n",
            "150.660095340404\n",
            "150.4629877999493\n",
            "152.70821320319322\n",
            "152.43183279144853\n",
            "153.09806197643596\n",
            "152.79730924266005\n",
            "151.1983152017694\n",
            "157.51468957007862\n",
            "151.68848826239932\n",
            "157.26696446610308\n",
            "154.67016445031268\n",
            "149.13462467861666\n",
            "151.3549559858518\n",
            "151.17863003060427\n",
            "155.0384346227922\n",
            "157.59202545228817\n",
            "156.2574847382163\n",
            "150.15177473422156\n",
            "150.2082082717095\n",
            "149.0949335167382\n",
            "152.6202113431121\n",
            "152.70315653624195\n",
            "149.5538735488111\n",
            "156.63866543217193\n",
            "153.4394499490959\n",
            "154.84048255161963\n",
            "150.52945973957253\n",
            "152.82925436777444\n",
            "153.72369473269185\n",
            "150.36505252472728\n",
            "153.26107269641952\n",
            "152.4049373093833\n",
            "151.87808929425\n",
            "154.40838927579034\n",
            "154.17541711469437\n",
            "154.96964070335554\n",
            "152.8709263103838\n",
            "156.6090118889487\n",
            "153.75473443283127\n",
            "149.48967829381888\n",
            "154.23964410054543\n",
            "152.42918043738803\n",
            "152.88117362534163\n",
            "150.53057533082503\n",
            "154.1028543876324\n",
            "155.22549514651686\n",
            "154.2060389059241\n",
            "154.49204584157394\n",
            "152.74971852979195\n",
            "149.08904980166812\n",
            "154.62033433380952\n",
            "155.9407069467283\n",
            "158.19744135487184\n",
            "153.34907339409204\n",
            "152.3150902619608\n",
            "146.70078477883757\n",
            "157.5582563851621\n",
            "153.1232242293672\n",
            "156.12612175535148\n",
            "152.49626449353244\n",
            "148.56138781063416\n",
            "155.33614772602266\n",
            "149.96721207853642\n",
            "151.99520540878163\n",
            "152.45572155601468\n",
            "151.63292656355543\n",
            "155.61281094707977\n",
            "154.51966916574617\n",
            "154.17529150670646\n",
            "154.54931045310536\n",
            "154.30981741231597\n",
            "151.2389741457608\n",
            "155.48153305164487\n",
            "155.02752692604057\n",
            "151.0687322047022\n",
            "149.80303944171624\n",
            "151.8344046602861\n",
            "152.47847075520073\n",
            "155.26760550288745\n",
            "152.9825302582509\n",
            "156.7453143998061\n",
            "155.64200238192143\n",
            "153.07187308905083\n",
            "151.22316612311545\n",
            "151.5694558374479\n",
            "155.05797088415613\n",
            "153.02411993956994\n",
            "148.85729737805494\n",
            "152.42276423497552\n",
            "149.01541489181642\n",
            "152.10418679675283\n",
            "152.8689379075005\n",
            "154.6685410453416\n",
            "152.93833200420391\n",
            "153.00683469226314\n",
            "152.59053195406432\n",
            "150.11647976822724\n",
            "150.36699489547786\n",
            "154.18687236567612\n",
            "151.08088730063943\n",
            "153.09329744417295\n",
            "151.2594596995621\n",
            "154.75706091340774\n",
            "148.2348253420808\n",
            "154.75510687697442\n",
            "158.58172951724453\n",
            "151.78284288608137\n",
            "149.79615498513422\n",
            "154.38722172342207\n",
            "151.0790554609134\n",
            "150.76250863829452\n",
            "149.17158319524256\n",
            "153.47677862567545\n",
            "151.23754646707292\n",
            "152.33707720773566\n",
            "150.80669825330932\n",
            "151.54085513262166\n",
            "154.09621700201907\n",
            "155.3720025556034\n",
            "149.63262985412047\n",
            "153.31396984610413\n",
            "155.92372076729902\n",
            "155.11929042248732\n",
            "153.78609945132226\n",
            "153.777170050152\n",
            "151.88015862667976\n",
            "148.7194881746632\n",
            "156.22756524178533\n",
            "159.0304214576664\n",
            "153.52527517734757\n",
            "1 0.056437878908717966 21.164791753432276\n",
            "100 0.0014860648218495821 0.9059144503795284\n",
            "200 1.0582095676083585 771.0421399440058\n",
            "300 0.025639650242534232 1.39641808644963\n",
            "400 0.014873441942806986 1.1395952918105936\n",
            "1335.027685584\n"
          ]
        }
      ],
      "source": [
        "epoch         = 0\n",
        "expected_epoch = 20000\n",
        "num_samp      = N_samp * len(SNR)\n",
        "best_nmse     = 1e9\n",
        "early_stop    = 0\n",
        "best_model    = ''\n",
        "batch_size    = int(num_samp / 512)\n",
        "\n",
        "# Kiểm tra nếu file tĩnh tồn tại\n",
        "if os.path.exists('dataset_ISDNN.pkl'):\n",
        "    with open('dataset_ISDNN.pkl', 'rb') as f:\n",
        "        DataSet_x, DataSet_y, DataSet_H, DataSet_HH, H_true, H_raw, H_in, e, xTx, xTy, Di = pickle.load(f)\n",
        "    print(\"Dữ liệu đã được tải từ file tĩnh!\")\n",
        "else:\n",
        "    # Sinh dữ liệu nếu file tĩnh không tồn tại\n",
        "    DataSet_x, DataSet_y, DataSet_H, DataSet_HH = Gen_dataset('train', 0, 0, N_samp)\n",
        "    H_true, H_raw, H_in, e, xTx, xTy, Di = Input_ISDNN('train', DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_samp)\n",
        "\n",
        "    # Lưu dữ liệu để lần sau không phải sinh lại\n",
        "    with open('dataset_ISDNN.pkl', 'wb') as f:\n",
        "        pickle.dump((DataSet_x, DataSet_y, DataSet_H, DataSet_HH, H_true, H_raw, H_in, e, xTx, xTy, Di), f)\n",
        "    print(\"Dữ liệu đã được sinh và lưu lại!\")\n",
        "\n",
        "print(\"Begin training...\")\n",
        "starttime = timeit.default_timer()\n",
        "\n",
        "while(True):\n",
        "        epoch = epoch + 1\n",
        "\n",
        "        init_loss = 1e9\n",
        "        while( epoch == 1 and init_loss > 180):\n",
        "\n",
        "                model, loss, optimizer, record_file, file_model = def_model()\n",
        "                for bs in range (int(num_samp / batch_size)):\n",
        "                    H_1, e_1 = model(Di[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 H_in[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 e[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTx[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTy[0 + batch_size * bs:batch_size * (bs+1), :, :])   # predict output from the model\n",
        "                    init_loss = loss(H_1, H_true[0 + batch_size * bs:batch_size * (bs+1), :, :]).item()\n",
        "                    print(init_loss)\n",
        "\n",
        "        optimizer.zero_grad()   # zero the parameter gradients\n",
        "        train_loss = 0\n",
        "        H_f = torch.empty([num_samp, 2*Nt, 2*Nr])\n",
        "        for bs in range (int(num_samp / batch_size)):\n",
        "                H_o, e_o = model(Di[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 H_in[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 e[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTx[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTy[0 + batch_size * bs:batch_size * (bs+1), :, :])   # predict output from the model\n",
        "                H_f[0 + batch_size * bs:batch_size * (bs+1), :, :] = H_o\n",
        "                train_loss = loss(H_o,\n",
        "                                  H_true[0 + batch_size * bs:batch_size * (bs+1), :, :])   # calculate loss for the predicted output\n",
        "                train_loss.backward()   # backpropagate the loss\n",
        "                optimizer.step()        # adjust parameters based on the calculated gradients\n",
        "\n",
        "        if (epoch % 100 == 0 or epoch == 1):\n",
        "                nmse = 0\n",
        "                for j in range (num_samp):\n",
        "                        nmse += NMSE(H_f[j], H_raw[j])\n",
        "                nmse = nmse / num_samp\n",
        "\n",
        "                if (nmse <= best_nmse):\n",
        "                        torch.save(model.state_dict(), file_model + '_' + str(epoch) + '.pth')\n",
        "                        best_model = file_model + '_' + str(epoch) + '.pth'\n",
        "                        best_nmse = nmse\n",
        "                        early_stop = 0\n",
        "                else:\n",
        "                        early_stop += 1\n",
        "\n",
        "                if (nmse > best_nmse and early_stop == 3):\n",
        "                        with Record(record_file + '_log.txt'):\n",
        "                                print(epoch, nmse.item(), train_loss.item())\n",
        "                                print(str(timeit.default_timer()-starttime))\n",
        "                        break\n",
        "\n",
        "                with Record(record_file + '_log.txt'):\n",
        "                        print(epoch, nmse.item(), train_loss.item())\n",
        "\n",
        "        if epoch  == expected_epoch:\n",
        "                torch.save(model.state_dict(), file_model + '_' + str(epoch) + '.pth')\n",
        "                best_model = file_model + '_' + str(epoch) + '.pth'\n",
        "                with Record(record_file + '_log.txt'):\n",
        "                        print(\"epoch:\\n\", epoch)\n",
        "                        print(\"Latest NMSE:\\n\", nmse.item())\n",
        "                        print(\"Latest Loss:\\n\", train_loss.item())\n",
        "                        print(str(timeit.default_timer()-starttime))\n",
        "\n",
        "                break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOBK-_l-tRMO"
      },
      "source": [
        "# Test function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRRFGAX4tg_6"
      },
      "source": [
        "# Function to test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SfvpexfwthNq"
      },
      "outputs": [],
      "source": [
        "def test(H_raw, Di, H_in, e, xTx, xTy, N_test, log):\n",
        "    # Load the model that we saved at the end of the training loop\n",
        "    model = model_driven()\n",
        "    model.load_state_dict(torch.load(best_model, map_location=torch.device('cpu')))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        H_o, e_o = model(Di, H_in, e, xTx, xTy)\n",
        "\n",
        "        nmse = 0\n",
        "        for j in range (N_test):\n",
        "            nmse += NMSE(H_o[j], H_raw[j])\n",
        "\n",
        "        nmse = nmse / N_test\n",
        "        with Record(log):\n",
        "            print(format(nmse.item(), '.7f'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FlrHA2aWimlC"
      },
      "outputs": [],
      "source": [
        "## Generate dataset for test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vE_IiAiKimlC"
      },
      "outputs": [],
      "source": [
        "def LS(DataSet_x, DataSet_y):\n",
        "    start = timeit.default_timer()\n",
        "    for i in range (len(DataSet_x)):\n",
        "        H_hat = np.matmul(\n",
        "                    np.matmul(\n",
        "                        np.linalg.pinv(np.matmul(DataSet_x[i].transpose(), DataSet_x[i])),\n",
        "                        DataSet_x[i].transpose()),\n",
        "                        DataSet_y[i])\n",
        "    print(timeit.default_timer() - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "scrolled": false,
        "id": "PAf8LBUFimlC",
        "outputId": "401f46bd-4cb7-47ad-a29b-ae46710edcbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-3e5f3c45b18a>:6: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  dP[i] = abs(IFsig[i])**2\n",
            "<ipython-input-17-173530151950>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model, map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0005310\n",
            "0.0005332\n",
            "0.0005311\n",
            "0.0005275\n",
            "0.0005244\n",
            "0.0005242\n",
            "0.0005278\n",
            "0.0005170\n",
            "0.0005256\n",
            "0.0005228\n",
            "0.0005259\n",
            "0.0005274\n",
            "0.0005293\n",
            "0.0005306\n",
            "0.0005279\n",
            "0.0005302\n",
            "0.0005281\n",
            "0.0005259\n",
            "0.0005282\n",
            "0.0005259\n",
            "0.0005234\n",
            "0.0005324\n",
            "0.0005269\n",
            "0.0005260\n",
            "0.0005341\n",
            "0.0005232\n",
            "0.0005331\n",
            "0.0005285\n",
            "0.0005299\n",
            "0.0005269\n",
            "0.0005324\n",
            "0.0005232\n",
            "0.0005253\n",
            "0.0005304\n",
            "0.0005289\n",
            "0.0005261\n",
            "0.0005220\n",
            "0.0005259\n",
            "0.0005259\n",
            "0.0005211\n",
            "0.0005232\n",
            "0.0005284\n",
            "0.0005267\n",
            "0.0005338\n",
            "0.0005234\n",
            "0.0005380\n",
            "0.0005285\n",
            "0.0005313\n",
            "0.0005290\n",
            "0.0005211\n",
            "0.0005248\n",
            "0.0005227\n",
            "0.0005301\n",
            "0.0005245\n",
            "0.0005225\n",
            "0.0005277\n",
            "0.0005338\n",
            "0.0005283\n",
            "0.0005301\n",
            "0.0005236\n",
            "0.0005289\n",
            "0.0005230\n",
            "0.0005268\n",
            "0.0005270\n",
            "0.0005233\n",
            "0.0005295\n",
            "0.0005339\n",
            "0.0005296\n",
            "0.0005275\n",
            "0.0005294\n",
            "0.0005219\n",
            "0.0005302\n",
            "0.0005306\n",
            "0.0005303\n",
            "0.0005253\n",
            "0.0005240\n",
            "0.0005264\n",
            "0.0005394\n",
            "0.0005313\n",
            "0.0005254\n",
            "0.0005237\n",
            "0.0005224\n",
            "0.0005321\n",
            "0.0005315\n",
            "0.0005240\n",
            "0.0005185\n",
            "0.0005345\n",
            "0.0005216\n",
            "0.0005304\n",
            "0.0005324\n",
            "0.0005291\n",
            "0.0005298\n",
            "0.0005333\n",
            "0.0005242\n",
            "0.0005341\n",
            "0.0005304\n",
            "0.0005266\n",
            "0.0005222\n",
            "0.0005227\n",
            "0.0005394\n",
            "0.0005317\n",
            "0.0005300\n",
            "0.0005294\n",
            "0.0005293\n",
            "0.0005327\n",
            "0.0005305\n",
            "0.0005357\n",
            "0.0005195\n",
            "0.0005304\n",
            "0.0005259\n",
            "0.0005292\n",
            "0.0005270\n",
            "0.0005318\n",
            "0.0005272\n",
            "0.0005284\n",
            "0.0005308\n",
            "0.0005251\n",
            "0.0005226\n",
            "0.0005275\n",
            "0.0005286\n",
            "0.0005239\n",
            "0.0005349\n",
            "0.0005250\n",
            "0.0005299\n",
            "0.0005221\n",
            "0.0005270\n",
            "0.0005213\n",
            "0.0005296\n",
            "0.0005232\n",
            "0.0005272\n",
            "0.0005284\n",
            "0.0005297\n",
            "0.0005353\n",
            "0.0005292\n",
            "0.0005309\n",
            "0.0005313\n",
            "0.0005297\n",
            "0.0005230\n",
            "0.0005257\n",
            "0.0005166\n",
            "0.0005225\n",
            "0.0005216\n",
            "0.0005319\n",
            "0.0005254\n",
            "0.0005259\n",
            "0.0005353\n",
            "0.0005302\n",
            "0.0005258\n",
            "0.0005216\n",
            "0.0005254\n",
            "0.0005249\n",
            "0.0005224\n",
            "0.0005277\n",
            "0.0005295\n",
            "0.0005262\n",
            "0.0005276\n",
            "0.0005238\n",
            "0.0005306\n",
            "0.0005333\n",
            "0.0005268\n",
            "0.0005241\n",
            "0.0005294\n",
            "0.0005237\n",
            "0.0005339\n",
            "0.0005318\n",
            "0.0005308\n",
            "0.0005311\n",
            "0.0005279\n",
            "0.0005280\n",
            "0.0005270\n",
            "0.0005255\n",
            "0.0005220\n",
            "0.0005248\n",
            "0.0005319\n",
            "0.0005246\n",
            "0.0005311\n",
            "0.0005346\n",
            "0.0005238\n",
            "0.0005294\n",
            "0.0005366\n",
            "0.0005215\n",
            "0.0005308\n",
            "0.0005304\n",
            "0.0005279\n",
            "0.0005271\n",
            "0.0005281\n",
            "0.0005286\n",
            "0.0005411\n",
            "0.0005327\n",
            "0.0005260\n",
            "0.0005240\n",
            "0.0005280\n",
            "0.0005302\n",
            "0.0005254\n",
            "0.0005285\n",
            "0.0005336\n",
            "0.0005282\n",
            "0.0005329\n",
            "0.0005270\n",
            "0.0005311\n",
            "0.0005296\n",
            "0.0005274\n",
            "0.0005281\n",
            "0.0005267\n",
            "0.0005217\n",
            "0.0005255\n",
            "0.0005326\n",
            "0.0005291\n",
            "0.0005247\n",
            "0.0005295\n",
            "0.0005326\n",
            "0.0005179\n",
            "0.0005337\n",
            "0.0005208\n",
            "0.0005262\n",
            "0.0005315\n",
            "0.0005228\n",
            "0.0005288\n",
            "0.0005293\n",
            "0.0005284\n",
            "0.0005313\n",
            "0.0005295\n",
            "0.0005267\n",
            "0.0005278\n",
            "0.0005277\n",
            "0.0005240\n",
            "0.0005223\n",
            "0.0005303\n",
            "0.0005325\n",
            "0.0005158\n",
            "0.0005276\n",
            "0.0005291\n",
            "0.0005263\n",
            "0.0005252\n",
            "0.0005296\n",
            "0.0005277\n",
            "0.0005279\n",
            "0.0005254\n",
            "0.0005275\n",
            "0.0005299\n",
            "0.0005257\n",
            "0.0005286\n",
            "0.0005320\n",
            "0.0005290\n",
            "0.0005307\n",
            "0.0005289\n",
            "0.0005251\n",
            "0.0005207\n",
            "0.0005272\n",
            "0.0005266\n",
            "0.0005331\n",
            "0.0005274\n",
            "0.0005251\n",
            "0.0005365\n",
            "0.0005302\n",
            "0.0005300\n",
            "0.0005266\n",
            "0.0005281\n",
            "0.0005304\n",
            "0.0005263\n",
            "0.0005319\n",
            "0.0005287\n",
            "0.0005273\n",
            "0.0005257\n",
            "0.0005298\n",
            "0.0005339\n",
            "0.0005314\n",
            "0.0005343\n",
            "0.0005272\n",
            "0.0005268\n",
            "0.0005288\n",
            "0.0005307\n",
            "0.0005264\n",
            "0.0005279\n",
            "0.0005297\n",
            "0.0005322\n",
            "0.0005275\n",
            "0.0005299\n",
            "0.0005267\n",
            "0.0005227\n",
            "0.0005258\n",
            "0.0005311\n",
            "0.0005287\n",
            "0.0005297\n",
            "0.0005268\n",
            "0.0005337\n",
            "0.0005347\n",
            "0.0005287\n",
            "0.0005280\n",
            "0.0005287\n",
            "0.0005243\n",
            "0.0005344\n",
            "0.0005289\n",
            "0.0005229\n",
            "0.0005242\n",
            "0.0005284\n",
            "0.0005238\n",
            "0.0005290\n",
            "0.0005302\n",
            "0.0005284\n",
            "0.0005262\n",
            "0.0005301\n",
            "0.0005266\n",
            "0.0005262\n",
            "0.0005224\n",
            "0.0005278\n",
            "0.0005288\n",
            "0.0005214\n",
            "0.0005344\n",
            "0.0005305\n",
            "0.0005244\n",
            "0.0005317\n",
            "0.0005318\n",
            "0.0005268\n",
            "0.0005263\n",
            "0.0005236\n",
            "0.0005294\n",
            "0.0005273\n",
            "0.0005285\n",
            "0.0005318\n",
            "0.0005278\n",
            "0.0005236\n",
            "0.0005189\n",
            "0.0005239\n",
            "0.0005241\n",
            "0.0005324\n",
            "0.0005194\n",
            "0.0005292\n",
            "0.0005280\n",
            "0.0005271\n",
            "0.0005337\n",
            "0.0005335\n",
            "0.0005240\n",
            "0.0005272\n",
            "0.0005306\n",
            "0.0005334\n",
            "0.0005256\n",
            "0.0005243\n",
            "0.0005297\n",
            "0.0005206\n",
            "0.0005313\n",
            "0.0005329\n",
            "0.0005295\n",
            "0.0005358\n",
            "0.0005305\n",
            "0.0005230\n",
            "0.0005306\n",
            "0.0005294\n",
            "0.0005327\n",
            "0.0005338\n",
            "0.0005301\n",
            "0.0005307\n",
            "0.0005308\n",
            "0.0005371\n",
            "0.0005311\n",
            "0.0005286\n",
            "0.0005227\n",
            "0.0005236\n",
            "0.0005224\n",
            "0.0005287\n",
            "0.0005312\n",
            "0.0005287\n",
            "0.0005260\n",
            "0.0005288\n",
            "0.0005279\n",
            "0.0005255\n",
            "0.0005349\n",
            "0.0005285\n",
            "0.0005291\n",
            "0.0005274\n",
            "0.0005263\n",
            "0.0005248\n",
            "0.0005289\n",
            "0.0005261\n",
            "0.0005249\n",
            "0.0005258\n",
            "0.0005343\n",
            "0.0005260\n",
            "0.0005281\n",
            "0.0005266\n",
            "0.0005282\n",
            "0.0005296\n",
            "0.0005274\n",
            "0.0005276\n",
            "0.0005274\n",
            "0.0005319\n",
            "0.0005210\n",
            "0.0005273\n",
            "0.0005301\n",
            "0.0005256\n",
            "0.0005302\n",
            "0.0005308\n",
            "0.0005256\n",
            "0.0005304\n",
            "0.0005269\n",
            "0.0005227\n",
            "0.0005252\n",
            "0.0005318\n",
            "0.0005363\n",
            "0.0005257\n",
            "0.0005269\n",
            "0.0005311\n",
            "0.0005219\n",
            "0.0005347\n",
            "0.0005265\n",
            "0.0005319\n",
            "0.0005286\n",
            "0.0005265\n",
            "0.0005344\n",
            "0.0005350\n",
            "0.0005274\n",
            "0.0005291\n",
            "0.0005318\n",
            "0.0005293\n",
            "0.0005231\n",
            "0.0005288\n",
            "0.0005249\n",
            "0.0005297\n",
            "0.0005341\n",
            "0.0005283\n",
            "0.0005317\n",
            "0.0005303\n",
            "0.0005323\n",
            "0.0005306\n",
            "0.0005218\n",
            "0.0005265\n",
            "0.0005356\n",
            "0.0005227\n",
            "0.0005253\n",
            "0.0005301\n",
            "0.0005305\n",
            "0.0005242\n",
            "0.0005221\n",
            "0.0005311\n",
            "0.0005307\n",
            "0.0005235\n",
            "0.0005252\n",
            "0.0005237\n",
            "0.0005246\n",
            "0.0005278\n",
            "0.0005288\n",
            "0.0005350\n",
            "0.0005279\n",
            "0.0005298\n",
            "0.0005285\n",
            "0.0005281\n",
            "0.0005256\n",
            "0.0005296\n",
            "0.0005275\n",
            "0.0005266\n",
            "0.0005227\n",
            "0.0005301\n",
            "0.0005317\n",
            "0.0005173\n",
            "0.0005284\n",
            "0.0005208\n",
            "0.0005231\n",
            "0.0005303\n",
            "0.0005293\n",
            "0.0005269\n",
            "0.0005267\n",
            "0.0005269\n",
            "0.0005315\n",
            "0.0005265\n",
            "0.0005248\n",
            "0.0005301\n",
            "0.0005311\n",
            "0.0005282\n",
            "0.0005214\n",
            "0.0005296\n",
            "0.0005301\n",
            "0.0005163\n",
            "0.0005259\n",
            "0.0005317\n",
            "0.0005329\n",
            "0.0005278\n",
            "0.0005313\n",
            "0.0005316\n",
            "0.0005301\n",
            "0.0005352\n",
            "0.0005212\n",
            "0.0005268\n",
            "0.0005347\n",
            "0.0005266\n",
            "0.0005255\n",
            "0.0005240\n",
            "0.0005267\n",
            "0.0005206\n",
            "0.0005247\n",
            "0.0005267\n",
            "0.0005292\n",
            "0.0005312\n",
            "0.0005268\n",
            "0.0005300\n",
            "0.0005278\n",
            "0.0005358\n",
            "0.0005280\n",
            "0.0005358\n",
            "0.0005274\n",
            "0.0005248\n",
            "0.0005337\n",
            "0.0005310\n",
            "0.0005252\n",
            "0.0005324\n",
            "0.0005210\n",
            "0.0005253\n",
            "0.0005301\n",
            "0.0005318\n",
            "0.0005244\n",
            "0.0005286\n",
            "0.0005297\n",
            "0.0005237\n",
            "0.0005269\n",
            "0.0005247\n",
            "0.0005304\n",
            "0.0005249\n",
            "0.0005250\n",
            "0.0005278\n",
            "0.0005330\n",
            "0.0005307\n",
            "0.0005247\n",
            "0.0005211\n",
            "0.0005273\n",
            "0.0005269\n",
            "0.0005228\n",
            "0.0005257\n",
            "0.0005307\n",
            "0.0005266\n",
            "0.0005325\n",
            "0.0005249\n",
            "0.0005279\n",
            "0.0005254\n",
            "0.0005177\n",
            "0.0005229\n",
            "0.0005274\n",
            "0.0005348\n",
            "0.0005286\n",
            "0.0005312\n",
            "0.0005290\n",
            "0.0005328\n",
            "0.0005342\n",
            "0.0005275\n",
            "0.0005295\n",
            "0.0005202\n",
            "0.0005187\n",
            "0.0005341\n",
            "0.0005267\n",
            "0.0005352\n",
            "0.0005262\n",
            "0.0005268\n",
            "0.0005335\n",
            "0.0005245\n",
            "0.0005323\n",
            "0.0005281\n",
            "0.0005333\n",
            "0.0005273\n",
            "0.0005326\n",
            "0.0005299\n",
            "0.0005308\n",
            "0.0005235\n",
            "0.0005290\n",
            "0.0005260\n",
            "0.0005252\n",
            "0.0005314\n",
            "0.0005291\n",
            "0.0005277\n",
            "0.0005208\n",
            "0.0005266\n",
            "0.0005273\n",
            "0.0005228\n",
            "0.0005306\n",
            "0.0005210\n",
            "0.0005294\n",
            "0.0005335\n",
            "0.0005390\n",
            "0.0005269\n",
            "0.0005240\n",
            "0.0005252\n",
            "0.0005232\n",
            "0.0005313\n",
            "0.0005260\n",
            "0.0005280\n",
            "0.0005260\n",
            "0.0005294\n",
            "0.0005319\n",
            "0.0005358\n",
            "0.0005272\n",
            "0.0005238\n",
            "0.0005315\n",
            "0.0005218\n",
            "0.0005235\n",
            "0.0005209\n",
            "0.0005203\n",
            "0.0005273\n",
            "0.0005344\n",
            "0.0005321\n",
            "0.0005293\n",
            "0.0005351\n",
            "0.0005234\n",
            "0.0005351\n",
            "0.0005293\n",
            "0.0005238\n",
            "0.0005240\n",
            "0.0005220\n",
            "0.0005196\n",
            "0.0005381\n",
            "0.0005245\n",
            "0.0005305\n",
            "0.0005317\n",
            "0.0005270\n",
            "0.0005264\n",
            "0.0005295\n",
            "0.0005279\n",
            "0.0005186\n",
            "0.0005343\n",
            "0.0005233\n",
            "0.0005367\n",
            "0.0005347\n",
            "0.0005365\n",
            "0.0005243\n",
            "0.0005276\n",
            "0.0005239\n",
            "0.0005310\n",
            "0.0005197\n",
            "0.0005239\n",
            "0.0005211\n",
            "0.0005298\n",
            "0.0005340\n",
            "0.0005297\n",
            "0.0005256\n",
            "0.0005266\n",
            "0.0005249\n",
            "0.0005355\n",
            "0.0005306\n",
            "0.0005322\n",
            "0.0005248\n",
            "0.0005293\n",
            "0.0005239\n",
            "0.0005347\n",
            "0.0005279\n",
            "0.0005253\n",
            "0.0005297\n",
            "0.0005279\n",
            "0.0005231\n",
            "0.0005263\n",
            "0.0005260\n",
            "0.0005212\n",
            "0.0005194\n",
            "0.0005237\n",
            "0.0005243\n",
            "0.0005313\n",
            "0.0005211\n",
            "0.0005275\n",
            "0.0005321\n",
            "0.0005283\n",
            "0.0005315\n",
            "0.0005212\n",
            "0.0005255\n",
            "0.0005294\n",
            "0.0005261\n",
            "0.0005278\n",
            "0.0005320\n",
            "0.0005237\n",
            "0.0005304\n",
            "0.0005278\n",
            "0.0005236\n",
            "0.0005183\n",
            "0.0005232\n",
            "0.0005265\n",
            "0.0005327\n",
            "0.0005300\n",
            "0.0005278\n",
            "0.0005286\n",
            "0.0005370\n",
            "0.0005290\n",
            "0.0005320\n",
            "0.0005318\n",
            "0.0005285\n",
            "0.0005247\n",
            "0.0005311\n",
            "0.0005251\n",
            "0.0005307\n",
            "0.0005272\n",
            "0.0005331\n",
            "0.0005264\n",
            "0.0005311\n",
            "0.0005287\n",
            "0.0005276\n",
            "0.0005243\n",
            "0.0005318\n",
            "0.0005247\n",
            "0.0005294\n",
            "0.0005277\n",
            "0.0005368\n",
            "0.0005284\n",
            "0.0005229\n",
            "0.0005271\n",
            "0.0005294\n",
            "0.0005322\n",
            "0.0005246\n",
            "0.0005272\n",
            "0.0005325\n",
            "0.0005235\n",
            "0.0005254\n",
            "0.0005359\n",
            "0.0005277\n",
            "0.0005310\n",
            "0.0005192\n",
            "0.0005288\n",
            "0.0005303\n",
            "0.0005311\n",
            "0.0005287\n",
            "0.0005236\n",
            "0.0005335\n",
            "0.0005332\n",
            "0.0005373\n",
            "0.0005317\n",
            "0.0005339\n",
            "0.0005290\n",
            "0.0005260\n",
            "0.0005271\n",
            "0.0005291\n",
            "0.0005245\n",
            "0.0005318\n",
            "0.0005295\n",
            "0.0005311\n",
            "0.0005343\n",
            "0.0005332\n",
            "0.0005308\n",
            "0.0005282\n",
            "0.0005255\n",
            "0.0005316\n",
            "0.0005320\n",
            "0.0005237\n",
            "0.0005236\n",
            "0.0005245\n",
            "0.0005325\n",
            "0.0005292\n",
            "0.0005300\n",
            "0.0005308\n",
            "0.0005236\n",
            "0.0005271\n",
            "0.0005225\n",
            "0.0005248\n",
            "0.0005277\n",
            "0.0005301\n",
            "0.0005233\n",
            "0.0005301\n",
            "0.0005332\n",
            "0.0005331\n",
            "0.0005274\n",
            "0.0005320\n",
            "0.0005263\n",
            "0.0005305\n",
            "0.0005288\n",
            "0.0005354\n",
            "0.0005267\n",
            "0.0005254\n",
            "0.0005314\n",
            "0.0005303\n",
            "0.0005369\n",
            "0.0005278\n",
            "0.0005350\n",
            "0.0005305\n",
            "0.0005271\n",
            "0.0005317\n",
            "0.0005284\n",
            "0.0005320\n",
            "0.0005234\n",
            "0.0005283\n",
            "0.0005426\n",
            "0.0005296\n",
            "0.0005308\n",
            "0.0005239\n",
            "0.0005216\n",
            "0.0005283\n",
            "0.0005272\n",
            "0.0005257\n",
            "0.0005251\n",
            "0.0005267\n",
            "0.0005259\n",
            "0.0005313\n",
            "0.0005281\n",
            "0.0005269\n",
            "0.0005273\n",
            "0.0005323\n",
            "0.0005226\n",
            "0.0005236\n",
            "0.0005314\n",
            "0.0005252\n",
            "0.0005288\n",
            "0.0005307\n",
            "0.0005284\n",
            "0.0005299\n",
            "0.0005304\n",
            "0.0005346\n",
            "0.0005347\n",
            "0.0005267\n",
            "0.0005304\n",
            "0.0005261\n",
            "0.0005245\n",
            "0.0005272\n",
            "0.0005306\n",
            "0.0005341\n",
            "0.0005306\n",
            "0.0005258\n",
            "0.0005327\n",
            "0.0005201\n",
            "0.0005238\n",
            "0.0005245\n",
            "0.0005241\n",
            "0.0005281\n",
            "0.0005300\n",
            "0.0005237\n",
            "0.0005355\n",
            "0.0005286\n",
            "0.0005321\n",
            "0.0005352\n",
            "0.0005296\n",
            "0.0005268\n",
            "0.0005296\n",
            "0.0005271\n",
            "0.0005253\n",
            "0.0005287\n",
            "0.0005374\n",
            "0.0005350\n",
            "0.0005292\n",
            "0.0005300\n",
            "0.0005172\n",
            "0.0005265\n",
            "0.0005319\n",
            "0.0005280\n",
            "0.0005225\n",
            "0.0005278\n",
            "0.0005337\n",
            "0.0005256\n",
            "0.0005352\n",
            "0.0005312\n",
            "0.0005353\n",
            "0.0005231\n",
            "0.0005214\n",
            "0.0005278\n",
            "0.0005274\n",
            "0.0005316\n",
            "0.0005309\n",
            "0.0005297\n",
            "0.0005230\n",
            "0.0005338\n",
            "0.0005340\n",
            "0.0005244\n",
            "0.0005295\n",
            "0.0005278\n",
            "0.0005256\n",
            "0.0005242\n",
            "0.0005291\n",
            "0.0005240\n",
            "0.0005293\n",
            "0.0005213\n",
            "0.0005295\n",
            "0.0005329\n",
            "0.0005300\n",
            "0.0005249\n",
            "0.0005239\n",
            "0.0005235\n",
            "0.0005255\n",
            "0.0005290\n",
            "0.0005296\n",
            "0.0005225\n",
            "0.0005243\n",
            "0.0005375\n",
            "0.0005328\n",
            "0.0005327\n",
            "0.0005239\n",
            "0.0005224\n",
            "0.0005219\n",
            "0.0005272\n",
            "0.0005249\n",
            "0.0005281\n",
            "0.0005252\n",
            "0.0005275\n",
            "0.0005302\n",
            "0.0005382\n",
            "0.0005324\n",
            "0.0005310\n",
            "0.0005290\n",
            "0.0005349\n",
            "0.0005190\n",
            "0.0005279\n",
            "0.0005247\n",
            "0.0005349\n",
            "0.0005205\n",
            "0.0005342\n",
            "0.0005305\n",
            "0.0005351\n",
            "0.0005301\n",
            "0.0005249\n",
            "0.0005273\n",
            "0.0005265\n",
            "0.0005318\n",
            "0.0005283\n",
            "0.0005244\n",
            "0.0005273\n",
            "0.0005279\n",
            "0.0005285\n",
            "0.0005198\n",
            "0.0005332\n",
            "0.0005275\n",
            "0.0005305\n",
            "0.0005191\n",
            "0.0005271\n",
            "0.0005234\n",
            "0.0005366\n",
            "0.0005296\n",
            "0.0005315\n",
            "0.0005303\n",
            "0.0005292\n",
            "0.0005225\n",
            "0.0005202\n",
            "0.0005293\n",
            "0.0005284\n",
            "0.0005242\n",
            "0.0005282\n",
            "0.0005284\n",
            "0.0005251\n",
            "0.0005280\n",
            "0.0005298\n",
            "0.0005313\n",
            "0.0005286\n",
            "0.0005309\n",
            "0.0005298\n",
            "0.0005229\n",
            "0.0005280\n",
            "0.0005241\n",
            "0.0005334\n",
            "0.0005225\n",
            "0.0005338\n",
            "0.0005344\n",
            "0.0005271\n",
            "0.0005255\n",
            "0.0005264\n",
            "0.0005247\n",
            "0.0005287\n",
            "0.0005224\n",
            "0.0005269\n",
            "0.0005235\n",
            "0.0005230\n",
            "0.0005344\n",
            "0.0005219\n",
            "0.0005298\n",
            "0.0005197\n",
            "0.0005294\n",
            "0.0005286\n",
            "0.0005276\n",
            "0.0005239\n",
            "0.0005277\n",
            "0.0005305\n",
            "0.0005294\n",
            "0.0005294\n",
            "0.0005302\n",
            "0.0005286\n",
            "0.0005263\n",
            "0.0005291\n",
            "0.0005282\n",
            "0.0005330\n",
            "0.0005282\n",
            "0.0005201\n",
            "0.0005275\n",
            "0.0005292\n",
            "0.0005267\n",
            "0.0005261\n",
            "0.0005278\n",
            "0.0005347\n",
            "0.0005197\n",
            "0.0005249\n",
            "0.0005256\n",
            "0.0005337\n",
            "0.0005327\n",
            "0.0005271\n",
            "0.0005221\n",
            "0.0005310\n",
            "0.0005382\n",
            "0.0005302\n",
            "0.0005219\n",
            "0.0005261\n",
            "0.0005298\n",
            "0.0005279\n",
            "0.0005293\n",
            "0.0005302\n",
            "0.0005313\n",
            "0.0005244\n",
            "0.0005298\n",
            "0.0005313\n",
            "0.0005331\n",
            "0.0005293\n",
            "0.0005301\n",
            "0.0005296\n",
            "0.0005347\n",
            "0.0005242\n",
            "0.0005228\n",
            "0.0005266\n",
            "0.0005205\n",
            "0.0005310\n",
            "0.0005295\n",
            "0.0005260\n",
            "0.0005253\n",
            "0.0005296\n",
            "0.0005328\n",
            "0.0005285\n",
            "0.0005259\n",
            "0.0005280\n",
            "0.0005247\n",
            "0.0005334\n",
            "0.0005214\n",
            "0.0005339\n",
            "0.0005350\n",
            "0.0005283\n",
            "0.0005314\n",
            "0.0005278\n",
            "0.0005212\n",
            "0.0005316\n",
            "0.0005262\n",
            "0.0005175\n",
            "0.0005223\n",
            "0.0005326\n",
            "0.0005262\n",
            "0.0005218\n",
            "0.0005298\n",
            "0.0005267\n",
            "0.0005294\n",
            "0.0005271\n",
            "0.0005249\n",
            "0.0005353\n",
            "0.0005259\n",
            "0.0005233\n",
            "0.0005301\n",
            "0.0005265\n",
            "0.0005293\n",
            "0.0005293\n",
            "0.0005286\n",
            "0.0005213\n",
            "0.0005301\n",
            "0.0005294\n",
            "0.0005274\n",
            "0.0005308\n",
            "0.0005273\n",
            "0.0005249\n",
            "0.0005282\n",
            "0.0005352\n",
            "0.0005308\n",
            "0.0005262\n",
            "0.0005246\n",
            "0.0005257\n",
            "0.0005254\n",
            "0.0005352\n",
            "0.0005230\n",
            "0.0005257\n",
            "0.0005393\n",
            "0.0005228\n",
            "0.0005251\n",
            "0.0005237\n",
            "0.0005256\n",
            "0.0005275\n",
            "0.0005245\n",
            "0.0005260\n",
            "0.0005266\n",
            "0.0005260\n",
            "0.0005240\n",
            "0.0005310\n",
            "0.0005313\n",
            "0.0005215\n",
            "0.0005272\n",
            "0.0005252\n",
            "0.0005334\n",
            "0.0005308\n",
            "0.0005255\n",
            "0.0005285\n",
            "0.0005199\n",
            "0.0005245\n",
            "0.0005299\n",
            "0.0005245\n",
            "0.0005258\n",
            "0.0005280\n",
            "0.0005272\n",
            "0.0005325\n",
            "0.0005233\n",
            "0.0005355\n",
            "0.0005257\n",
            "0.0005253\n",
            "0.0005293\n",
            "0.0005343\n",
            "0.0005307\n",
            "0.0005290\n",
            "0.0005296\n",
            "0.0005253\n",
            "0.0005214\n",
            "0.0005307\n",
            "0.0005255\n",
            "0.0005248\n",
            "0.0005292\n",
            "0.0005274\n"
          ]
        }
      ],
      "source": [
        "SNR_min_dB  = 0\n",
        "SNR_max_dB  = 20\n",
        "step_dB     = 5\n",
        "num_dB      = int((SNR_max_dB - SNR_min_dB) / step_dB) + 1\n",
        "\n",
        "SNR         = np.linspace(SNR_min_dB, SNR_max_dB, num=num_dB)\n",
        "log         = './model/log_test.txt'\n",
        "\n",
        "N_test = int(num_samp * 30/100)\n",
        "\n",
        "for i in range (100):\n",
        "    for snr in SNR:\n",
        "        # with Record(log):\n",
        "        #     print(snr)\n",
        "        DataSet_x, DataSet_y, DataSet_H, DataSet_HH = Gen_dataset('test', snr, 0, N_test)\n",
        "        H_true, H_raw, H_in, e, xTx, xTy, Di = Input_ISDNN('test', DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_test)\n",
        "\n",
        "        # LS(DataSet_x, DataSet_y)\n",
        "        test(H_raw, Di, H_in, e, xTx, xTy, N_test, log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "t6_n95BiimlC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}