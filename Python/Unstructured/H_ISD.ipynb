{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSmcwLZuptDU"
      },
      "source": [
        "# Proposed Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG5fZn_ypunP"
      },
      "source": [
        "# **Biblioheque**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xxLcL1DSlXeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmdir model"
      ],
      "metadata": {
        "id": "xfYtDLlNmDpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "--SvzvIspu-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "395cece7-70da-4427-98d2-340ad223d194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
            "  _C._set_default_tensor_type(t)\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from scipy.stats import rice\n",
        "import pickle\n",
        "# import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import sys\n",
        "import timeit\n",
        "import os\n",
        "\n",
        "torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
        "torch.set_default_dtype(torch.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBk01Vu0pvMm"
      },
      "source": [
        "# class to save results in file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "06IymVCkpvYS"
      },
      "outputs": [],
      "source": [
        "class Record:\n",
        "    def __init__(self, TextName):\n",
        "        self.out_file = open(TextName, 'a')\n",
        "        self.old_stdout = sys.stdout\n",
        "        sys.stdout = self\n",
        "\n",
        "    def write(self, text):\n",
        "        self.old_stdout.write(text)\n",
        "        self.out_file.write(text)\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout = self.old_stdout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xPamGjNpv8E"
      },
      "source": [
        "# **slicer the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dlQdA0xHpwF7"
      },
      "outputs": [],
      "source": [
        "def slicer(data):\n",
        "    dataI = data[slice(0, len(data), 2)]\n",
        "    dataQ = data[slice(1, len(data), 2)]\n",
        "    return(dataI, dataQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOoiIo_LpwQl"
      },
      "source": [
        "# **Modulation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xIftC8fvpwir"
      },
      "outputs": [],
      "source": [
        "def mapper_16QAM(QAM16, data):\n",
        "    map0 = 2*data[slice(0, len(data), 2)] + data[slice(1, len(data), 2)]\n",
        "    map0 = list(map(int, map0))\n",
        "    dataMapped = []\n",
        "    for i in range(len(map0)):\n",
        "        dataMapped.append(QAM16[map0[i]])\n",
        "    return(dataMapped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AYhPgAIcqtGT"
      },
      "outputs": [],
      "source": [
        "def calculate_bits(Modulation,NumSubcarriers,NumDataSymb):\n",
        "    if Modulation=='QPSK':\n",
        "        Nbpscs=2\n",
        "    elif Modulation=='16QAM':\n",
        "        Nbpscs=4\n",
        "    return NumDataSymb*NumSubcarriers*Nbpscs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgX6tiqFpwvb"
      },
      "source": [
        "# **generate noise**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2LQwZvGaprKZ"
      },
      "outputs": [],
      "source": [
        "def AWGN(IFsig, SNR):\n",
        "    dP = np.zeros(len(IFsig))\n",
        "    P = 0\n",
        "\n",
        "    for i in range(len(IFsig)):\n",
        "        dP[i] = abs(IFsig[i])**2\n",
        "        P = P + dP[i]\n",
        "\n",
        "    P = P/len(IFsig)\n",
        "    gamma = 10**(SNR/10)\n",
        "    N0 = P/gamma\n",
        "    n = ((N0/2)**(0.5))*np.random.standard_normal(len(IFsig))\n",
        "    IF_n = np.zeros((len(IFsig),1))\n",
        "\n",
        "    for i in range(len(IFsig)):\n",
        "        IF_n[i,:] = IFsig[i] + n[i]\n",
        "\n",
        "    return(IF_n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDx6TEAyimk9"
      },
      "source": [
        "# Generate channel model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DnoArgs7imk9"
      },
      "outputs": [],
      "source": [
        "def Generate_channel(Nr, Nt, type):\n",
        "    if (type == 'gauss'):\n",
        "        return (np.random.normal(size=(Nr,Nt))+1j*np.random.normal(size=(Nr,Nt)))/np.sqrt(2)\n",
        "    if (type == 'rayleigh'):\n",
        "        return (np.random.rayleigh(scale=(1/np.sqrt(2)), size=(Nr,Nt)) + 1j*np.random.rayleigh(scale=(1/np.sqrt(2)), size=(Nr,Nt)))/np.sqrt(2)\n",
        "    if (type == 'rician'):\n",
        "        b = 1/np.sqrt(2)\n",
        "        return (rice.rvs(b, size=(Nr,Nt)) + 1j*rice.rvs(b, size=(Nr,Nt)))/np.sqrt(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eciNtnFjq2yd"
      },
      "source": [
        "# **Generate Dataset**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AE4Q6jZCq3CY"
      },
      "outputs": [],
      "source": [
        "DataSet_x   = []  # x dataset after modulation\n",
        "DataSet_y   = []  # y dataset\n",
        "DataSet_HH  = []  # H dataset\n",
        "DataSet_b   = []  # binary dataset\n",
        "SNR_min_dB  = 0\n",
        "SNR_max_dB  = 20\n",
        "step_dB     = 5\n",
        "num_dB      = int((SNR_max_dB - SNR_min_dB) / step_dB) + 1\n",
        "\n",
        "SNR         = np.linspace(SNR_min_dB, SNR_max_dB, num=num_dB)\n",
        "\n",
        "\n",
        "Nt = 8             # Tx: 8\n",
        "Nr = 64            # Rx: 128\n",
        "N_samp = 100\n",
        "\n",
        "\n",
        "def Gen_dataset(mode, snr, imperfect, N_samp):\n",
        "    DataSet_x   = []  # x dataset after modulation\n",
        "    DataSet_y   = []  # y dataset\n",
        "    DataSet_H   = []\n",
        "    DataSet_HH  = []\n",
        "\n",
        "    NumSubcarriers = 1\n",
        "    Modulation = '16QAM'\n",
        "    QAM16 = [-1, -0.333, 0.333, 1]\n",
        "    NumDataSymb = 1\n",
        "    N_type = 'gauss'\n",
        "\n",
        "    if mode == 'train':\n",
        "        for snr in SNR:\n",
        "            for runIdx in range(0, N_samp):      # ! 20000 x Nt: samples\n",
        "                H = Generate_channel(Nt, Nr, N_type)\n",
        "                HH = np.concatenate((np.concatenate((H.real, H.imag), axis=1),\n",
        "                                    np.concatenate((-H.imag, H.real), axis=1)), axis=0)\n",
        "                x = np.zeros((2*Nt, NumSubcarriers))\n",
        "                a = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "                DataRaw = np.zeros((Nt, a))\n",
        "                for t in range(Nt):\n",
        "                    #\"data symbol generate\"\n",
        "                    NumBits = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "                    bit = np.random.randint(1, 3, NumBits)-1\n",
        "                    DataRaw[t, :] = bit\n",
        "                    for j in range(4):\n",
        "                        DataSet_b.append(bit[j])\n",
        "                    I = np.zeros((1, a))\n",
        "                    I[0, :] = DataRaw[t, :]\n",
        "                    (dataI, dataQ) = slicer(I[0])\n",
        "\n",
        "                    # Mapper\n",
        "                    mapI = mapper_16QAM(QAM16, dataI)\n",
        "                    mapQ = mapper_16QAM(QAM16, dataQ)\n",
        "                    x[t] = mapI[0]\n",
        "                    x[t+Nt] = mapQ[0]\n",
        "\n",
        "                # transpose\n",
        "                x = x.transpose()\n",
        "\n",
        "                y_wo_noise = np.matmul(x, HH)\n",
        "\n",
        "                # noise\n",
        "                noise = AWGN(y_wo_noise.transpose(), snr)\n",
        "\n",
        "                y = y_wo_noise + noise.transpose()\n",
        "\n",
        "                DataSet_x.append(x)    # ! I, Q sample distance by Nt.\n",
        "                DataSet_y.append(y)                 # ! output sample\n",
        "\n",
        "                # Imperfect channel: 5%\n",
        "                # coef = (2*np.random.randint(0,2,size=HH.shape) - 1)\n",
        "                # HH = HH + coef * HH * 0.05\n",
        "                DataSet_HH.append(HH)\n",
        "                DataSet_H.append(H)               # ! Generated channel\n",
        "\n",
        "    else:\n",
        "        for runIdx in range(0, N_samp):      # ! 20000 x Nt: samples\n",
        "            H = Generate_channel(Nt, Nr, N_type)\n",
        "            HH = np.concatenate((np.concatenate((H.real, H.imag), axis=1),\n",
        "                                np.concatenate((-H.imag, H.real), axis=1)), axis=0)\n",
        "            x = np.zeros((2*Nt, NumSubcarriers))\n",
        "            a = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "            DataRaw = np.zeros((Nt, a))\n",
        "            for t in range(Nt):\n",
        "                #\"data symbol generate\"\n",
        "                NumBits = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "                bit = np.random.randint(1, 3, NumBits)-1\n",
        "                DataRaw[t, :] = bit\n",
        "                for j in range(4):\n",
        "                    DataSet_b.append(bit[j])\n",
        "                I = np.zeros((1, a))\n",
        "                I[0, :] = DataRaw[t, :]\n",
        "                (dataI, dataQ) = slicer(I[0])\n",
        "\n",
        "                # Mapper\n",
        "                mapI = mapper_16QAM(QAM16, dataI)\n",
        "                mapQ = mapper_16QAM(QAM16, dataQ)\n",
        "                x[t] = mapI[0]\n",
        "                x[t+Nt] = mapQ[0]\n",
        "\n",
        "            # transpose\n",
        "            x = x.transpose()\n",
        "\n",
        "            y_wo_noise = np.matmul(x, HH)\n",
        "\n",
        "            # noise\n",
        "            noise = AWGN(y_wo_noise.transpose(), snr)\n",
        "\n",
        "            y = y_wo_noise + noise.transpose()\n",
        "\n",
        "            DataSet_x.append(x)    # ! I, Q sample distance by Nt.\n",
        "            DataSet_y.append(y)                 # ! output sample\n",
        "\n",
        "            # Imperfect channel: 5%\n",
        "            DataSet_HH.append(HH)\n",
        "            DataSet_H.append(H)               # ! Generated channel\n",
        "\n",
        "\n",
        "    # Shuffle dataset\n",
        "    random.seed(1)\n",
        "    temp = list(zip(DataSet_x, DataSet_y, DataSet_H, DataSet_HH))\n",
        "    random.shuffle(temp)\n",
        "    DataSet_x, DataSet_y, DataSet_H, DataSet_HH = zip(*temp)\n",
        "\n",
        "    return DataSet_x, DataSet_y, DataSet_H, DataSet_HH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1nWo2pTrimk-"
      },
      "outputs": [],
      "source": [
        "def reconstruct_channel (H):\n",
        "# H_raw = [R(H) I(H); -I(H) R(H)]\n",
        "# we have four version of H_est\n",
        "    H_est_1 = []\n",
        "    H_est_2 = []\n",
        "    H_est_3 = []\n",
        "    H_est_4 = []\n",
        "\n",
        "    H_est_Re_1 = H[0:Nt, 0:Nr]\n",
        "    H_est_Im_1 = H[0:Nt, Nr:2*Nr]\n",
        "    H_est_Im_2 = - H[Nt:2*Nt, 0:Nr]\n",
        "    H_est_Re_2 = H[Nt:2*Nt, Nr:2*Nr]\n",
        "\n",
        "    H_est_1 = H_est_Re_1 + 1j * H_est_Im_1\n",
        "    H_est_2 = H_est_Re_1 + 1j * H_est_Im_2\n",
        "    H_est_3 = H_est_Re_2 + 1j * H_est_Im_1\n",
        "    H_est_4 = H_est_Re_2 + 1j * H_est_Im_2\n",
        "\n",
        "    return H_est_1, H_est_2, H_est_3, H_est_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQUDE5Q1imk-"
      },
      "outputs": [],
      "source": [
        "# def NMSE(H_est, H_raw):\n",
        "#     H_est_1, H_est_2, H_est_3, H_est_4 = reconstruct_channel(H_est)\n",
        "#     H_est_vec_1 = torch.reshape(H_est_1, [Nt * Nr, 1])\n",
        "#     H_est_vec_2 = torch.reshape(H_est_2, [Nt * Nr, 1])\n",
        "#     H_est_vec_3 = torch.reshape(H_est_3, [Nt * Nr, 1])\n",
        "#     H_est_vec_4 = torch.reshape(H_est_4, [Nt * Nr, 1])\n",
        "\n",
        "#     H_raw_vec = torch.reshape(H_raw, [Nt * Nr, 1])\n",
        "\n",
        "#     mse_1       = (torch.norm(H_raw_vec - H_est_vec_1)**2) / len(H_raw_vec)\n",
        "#     mse_2       = (torch.norm(H_raw_vec - H_est_vec_2)**2) / len(H_raw_vec)\n",
        "#     mse_3       = (torch.norm(H_raw_vec - H_est_vec_3)**2) / len(H_raw_vec)\n",
        "#     mse_4       = (torch.norm(H_raw_vec - H_est_vec_4)**2) / len(H_raw_vec)\n",
        "\n",
        "#     sigEner   = torch.norm(H_raw_vec)**2\n",
        "\n",
        "#     nmse_1      = mse_1 / sigEner\n",
        "#     nmse_2      = mse_2 / sigEner\n",
        "#     nmse_3      = mse_3 / sigEner\n",
        "#     nmse_4      = mse_4 / sigEner\n",
        "\n",
        "#     # Best nmse\n",
        "#     nmse        = min([nmse_1, nmse_2, nmse_3, nmse_4])\n",
        "\n",
        "#     return torch.abs(nmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "knU4OnL3imk_"
      },
      "outputs": [],
      "source": [
        "def NMSE(H_est, H_raw):\n",
        "    H_est_1, H_est_2, H_est_3, H_est_4 = reconstruct_channel(H_est)\n",
        "\n",
        "    # Lấy phần thực của các tensor nếu chúng là complex\n",
        "    H_est_vec_1 = torch.reshape(H_est_1, [Nt * Nr, 1]).abs()\n",
        "    H_est_vec_2 = torch.reshape(H_est_2, [Nt * Nr, 1]).abs()\n",
        "    H_est_vec_3 = torch.reshape(H_est_3, [Nt * Nr, 1]).abs()\n",
        "    H_est_vec_4 = torch.reshape(H_est_4, [Nt * Nr, 1]).abs()\n",
        "\n",
        "    H_raw_vec = torch.reshape(H_raw, [Nt * Nr, 1]).abs()\n",
        "\n",
        "    mse_1 = (torch.norm(H_raw_vec - H_est_vec_1)**2) / len(H_raw_vec)\n",
        "    mse_2 = (torch.norm(H_raw_vec - H_est_vec_2)**2) / len(H_raw_vec)\n",
        "    mse_3 = (torch.norm(H_raw_vec - H_est_vec_3)**2) / len(H_raw_vec)\n",
        "    mse_4 = (torch.norm(H_raw_vec - H_est_vec_4)**2) / len(H_raw_vec)\n",
        "\n",
        "    sigEner = torch.norm(H_raw_vec)**2\n",
        "\n",
        "    nmse_1 = mse_1 / sigEner\n",
        "    nmse_2 = mse_2 / sigEner\n",
        "    nmse_3 = mse_3 / sigEner\n",
        "    nmse_4 = mse_4 / sigEner\n",
        "\n",
        "    # Chọn NMSE tốt nhất\n",
        "    nmse = min([nmse_1, nmse_2, nmse_3, nmse_4])\n",
        "\n",
        "    return torch.abs(nmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pyjIgxUurU0P"
      },
      "outputs": [],
      "source": [
        "def Input_ISDNN(mode, DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_samp):\n",
        "    H_in = []        # ! H_in    , np.diag(np.diag()) return a diag matrix instead of diag components.\n",
        "    H_true = []   # ! generated s\n",
        "    H_raw = []\n",
        "    e = []        # ! vector errors\n",
        "    xTx = []\n",
        "    xTy = []\n",
        "    Di = []\n",
        "    # steering = [] # ! Steering vector: ZoA and AoA\n",
        "\n",
        "    if mode == 'train':\n",
        "        n_sample = N_samp * len(SNR)\n",
        "    else:\n",
        "        n_sample = N_samp\n",
        "\n",
        "    for i in range (n_sample):\n",
        "        H_true.append(torch.tensor(DataSet_HH[i]))\n",
        "        H_raw.append(torch.tensor(DataSet_H[i]))\n",
        "        Di.append(torch.tensor(np.linalg.pinv(np.diag(np.diag(np.dot(DataSet_x[i].transpose(), DataSet_x[i]))))))\n",
        "        xTy.append(torch.tensor(np.dot(DataSet_x[i].transpose(), DataSet_y[i])))\n",
        "        H_in.append(torch.matmul(Di[i], xTy[i]))\n",
        "        e.append(torch.rand([2*Nt, 2*Nr]))\n",
        "        xTx.append(torch.tensor(np.dot(DataSet_x[i].transpose(), DataSet_x[i])))\n",
        "        # steering.append(torch.tensor(DataSet_Steering[i]))\n",
        "\n",
        "    H_true = torch.stack(H_true, dim=0)\n",
        "    H_raw = torch.stack(H_raw, dim=0)\n",
        "    H_in = torch.stack(H_in, dim=0)\n",
        "    e = torch.stack(e, dim=0)\n",
        "    xTx = torch.stack(xTx, dim=0)\n",
        "    xTy = torch.stack(xTy, dim=0)\n",
        "    Di = torch.stack(Di, dim=0)\n",
        "    # steering = torch.stack(steering, dim=0)\n",
        "\n",
        "    return H_true, H_raw, H_in, e, xTx, xTy, Di"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGhdBsghq3M9"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GHceh5kuq3ZD"
      },
      "outputs": [],
      "source": [
        "class xv(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(xv, self).__init__()\n",
        "\n",
        "        self.alpha1 = torch.nn.parameter.Parameter(torch.rand(1))\n",
        "        self.alpha2 = torch.nn.parameter.Parameter(torch.tensor([0.5]))\n",
        "\n",
        "    def forward(self, Di, H, e, xTx, xTy):\n",
        "\n",
        "        xTxH = torch.bmm(xTx, H)\n",
        "\n",
        "        z    = H + torch.bmm(Di, torch.sub(xTy, xTxH)) + self.alpha1 * e\n",
        "\n",
        "        e    = torch.sub(xTy, xTxH)\n",
        "\n",
        "        H    = torch.add((1 - self.alpha2) * z, self.alpha2 * H)\n",
        "\n",
        "        return H, e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "woRjD7lJssRq"
      },
      "outputs": [],
      "source": [
        "class model_driven(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(model_driven, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc2 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc3 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc4 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc5 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc6 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc7 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc8 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        # self.fc9 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        # self.fc10 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "\n",
        "        self.layer1=xv()\n",
        "        self.layer2=xv()\n",
        "        self.layer3=xv()\n",
        "        self.layer4=xv()\n",
        "        # self.layer5=xv()\n",
        "\n",
        "    def forward(self, Di, H_in, e, xTx, xTy):\n",
        "        e = self.fc1(e)\n",
        "        e = self.fc2(e)\n",
        "        H, e = self.layer1(Di, H_in, e, xTx, xTy)\n",
        "        H = torch.tanh(H)\n",
        "\n",
        "        e = self.fc3(e)\n",
        "        e = self.fc4(e)\n",
        "        H, e = self.layer2(Di, H, e, xTx, xTy)\n",
        "        H = torch.tanh(H)\n",
        "\n",
        "        e = self.fc5(e)\n",
        "        e = self.fc6(e)\n",
        "        H, e = self.layer3(Di, H, e, xTx, xTy)\n",
        "        H = torch.tanh(H)\n",
        "\n",
        "        e = self.fc7(e)\n",
        "        e = self.fc8(e)\n",
        "        H, e = self.layer4(Di, H, e, xTx, xTy)\n",
        "        # H = torch.tanh(H)\n",
        "\n",
        "        # e = self.fc9(e)\n",
        "        # e = self.fc10(e)\n",
        "        # H, e = self.layer5(Di, H, e, xTx, xTy)\n",
        "\n",
        "        return H, e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZjihTOXq3kG"
      },
      "source": [
        "# Define model, optimizer, and loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Vp9fRd3gq3tw"
      },
      "outputs": [],
      "source": [
        "def def_model():\n",
        "    model = model_driven()\n",
        "    loss = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    folder_model = './model/'\n",
        "\n",
        "    if not os.path.isdir(folder_model):\n",
        "        os.makedirs(folder_model)\n",
        "\n",
        "    file_model = folder_model + 'H'\n",
        "    # if os.path.isfile(file_model):\n",
        "    #     generator = torch.load(file_model)\n",
        "\n",
        "    record_file = 'H'\n",
        "    return model, loss, optimizer, record_file, file_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYWM7SzItKzS"
      },
      "source": [
        "# Main program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jv7lDwyxtFe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64e2dcef-b12e-4a3f-92d2-9b6e3f8d4e35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-3e5f3c45b18a>:6: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  dP[i] = abs(IFsig[i])**2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dữ liệu đã được sinh và lưu lại!\n",
            "Begin training...\n",
            "136.00445238687908\n",
            "144.03389709917073\n",
            "142.71472024705525\n",
            "143.71738339132474\n",
            "137.45801233624437\n",
            "137.96568507915413\n",
            "138.65021942179325\n",
            "142.07997401876025\n",
            "138.1751459133856\n",
            "138.2475139649633\n",
            "1 0.4043497636249736 136.46959874215105\n",
            "100 0.08978477914162489 45.167696653135025\n",
            "200 0.04297455146486604 25.773158413959266\n",
            "300 0.022710206494291032 15.641961569546133\n",
            "400 0.011643093006836018 9.422234882066304\n",
            "500 0.004928512844623562 4.913427151759173\n",
            "600 0.0014912429466578604 1.9377860294486977\n",
            "700 0.0008909454346987844 1.0728209554843366\n",
            "800 0.0008450017863702629 0.8857495207203826\n",
            "900 0.000845034230672795 0.8013853316677967\n",
            "1000 0.0008537973022124635 0.7422706415191519\n",
            "1100 0.0008669094162067617 0.6918545046991216\n",
            "100.160619102\n"
          ]
        }
      ],
      "source": [
        "epoch         = 0\n",
        "expected_epoch = 20000\n",
        "num_samp      = N_samp * len(SNR)\n",
        "best_nmse     = 1e9\n",
        "early_stop    = 0\n",
        "best_model    = ''\n",
        "batch_size    = int(num_samp / 10)\n",
        "\n",
        "# Kiểm tra nếu file tĩnh tồn tại\n",
        "if os.path.exists('dataset_ISDNN.pkl'):\n",
        "    with open('dataset_ISDNN.pkl', 'rb') as f:\n",
        "        DataSet_x, DataSet_y, DataSet_H, DataSet_HH, H_true, H_raw, H_in, e, xTx, xTy, Di = pickle.load(f)\n",
        "    print(\"Dữ liệu đã được tải từ file tĩnh!\")\n",
        "else:\n",
        "    # Sinh dữ liệu nếu file tĩnh không tồn tại\n",
        "    DataSet_x, DataSet_y, DataSet_H, DataSet_HH = Gen_dataset('train', 0, 0, N_samp)\n",
        "    H_true, H_raw, H_in, e, xTx, xTy, Di = Input_ISDNN('train', DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_samp)\n",
        "\n",
        "    # Lưu dữ liệu để lần sau không phải sinh lại\n",
        "    with open('dataset_ISDNN.pkl', 'wb') as f:\n",
        "        pickle.dump((DataSet_x, DataSet_y, DataSet_H, DataSet_HH, H_true, H_raw, H_in, e, xTx, xTy, Di), f)\n",
        "    print(\"Dữ liệu đã được sinh và lưu lại!\")\n",
        "\n",
        "print(\"Begin training...\")\n",
        "starttime = timeit.default_timer()\n",
        "\n",
        "while(True):\n",
        "        epoch = epoch + 1\n",
        "\n",
        "        init_loss = 1e9\n",
        "        while( epoch == 1 and init_loss > 180):\n",
        "\n",
        "                model, loss, optimizer, record_file, file_model = def_model()\n",
        "                for bs in range (int(num_samp / batch_size)):\n",
        "                    H_1, e_1 = model(Di[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 H_in[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 e[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTx[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTy[0 + batch_size * bs:batch_size * (bs+1), :, :])   # predict output from the model\n",
        "                    init_loss = loss(H_1, H_true[0 + batch_size * bs:batch_size * (bs+1), :, :]).item()\n",
        "                    print(init_loss)\n",
        "\n",
        "        optimizer.zero_grad()   # zero the parameter gradients\n",
        "        train_loss = 0\n",
        "        H_f = torch.empty([num_samp, 2*Nt, 2*Nr])\n",
        "        for bs in range (int(num_samp / batch_size)):\n",
        "                H_o, e_o = model(Di[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 H_in[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 e[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTx[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTy[0 + batch_size * bs:batch_size * (bs+1), :, :])   # predict output from the model\n",
        "                H_f[0 + batch_size * bs:batch_size * (bs+1), :, :] = H_o\n",
        "                train_loss = loss(H_o,\n",
        "                                  H_true[0 + batch_size * bs:batch_size * (bs+1), :, :])   # calculate loss for the predicted output\n",
        "                train_loss.backward()   # backpropagate the loss\n",
        "                optimizer.step()        # adjust parameters based on the calculated gradients\n",
        "\n",
        "        if (epoch % 100 == 0 or epoch == 1):\n",
        "                nmse = 0\n",
        "                for j in range (num_samp):\n",
        "                        nmse += NMSE(H_f[j], H_raw[j])\n",
        "                nmse = nmse / num_samp\n",
        "\n",
        "                if (nmse <= best_nmse):\n",
        "                        torch.save(model.state_dict(), file_model + '_' + str(epoch) + '.pth')\n",
        "                        best_model = file_model + '_' + str(epoch) + '.pth'\n",
        "                        best_nmse = nmse\n",
        "                        early_stop = 0\n",
        "                else:\n",
        "                        early_stop += 1\n",
        "\n",
        "                if (nmse > best_nmse and early_stop == 3):\n",
        "                        with Record(record_file + '_log.txt'):\n",
        "                                print(epoch, nmse.item(), train_loss.item())\n",
        "                                print(str(timeit.default_timer()-starttime))\n",
        "                        break\n",
        "\n",
        "                with Record(record_file + '_log.txt'):\n",
        "                        print(epoch, nmse.item(), train_loss.item())\n",
        "\n",
        "        if epoch  == expected_epoch:\n",
        "                torch.save(model.state_dict(), file_model + '_' + str(epoch) + '.pth')\n",
        "                best_model = file_model + '_' + str(epoch) + '.pth'\n",
        "                with Record(record_file + '_log.txt'):\n",
        "                        print(\"epoch:\\n\", epoch)\n",
        "                        print(\"Latest NMSE:\\n\", nmse.item())\n",
        "                        print(\"Latest Loss:\\n\", train_loss.item())\n",
        "                        print(str(timeit.default_timer()-starttime))\n",
        "\n",
        "                break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOBK-_l-tRMO"
      },
      "source": [
        "# Test function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRRFGAX4tg_6"
      },
      "source": [
        "# Function to test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SfvpexfwthNq"
      },
      "outputs": [],
      "source": [
        "def test(H_raw, Di, H_in, e, xTx, xTy, N_test, log):\n",
        "    # Load the model that we saved at the end of the training loop\n",
        "    model = model_driven()\n",
        "    model.load_state_dict(torch.load(best_model, map_location=torch.device('cpu')))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        H_o, e_o = model(Di, H_in, e, xTx, xTy)\n",
        "\n",
        "        nmse = 0\n",
        "        for j in range (N_test):\n",
        "            nmse += NMSE(H_o[j], H_raw[j])\n",
        "\n",
        "        nmse = nmse / N_test\n",
        "        with Record(log):\n",
        "            print(format(nmse.item(), '.7f'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlrHA2aWimlC"
      },
      "outputs": [],
      "source": [
        "## Generate dataset for test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE_IiAiKimlC"
      },
      "outputs": [],
      "source": [
        "def LS(DataSet_x, DataSet_y):\n",
        "    start = timeit.default_timer()\n",
        "    for i in range (len(DataSet_x)):\n",
        "        H_hat = np.matmul(\n",
        "                    np.matmul(\n",
        "                        np.linalg.pinv(np.matmul(DataSet_x[i].transpose(), DataSet_x[i])),\n",
        "                        DataSet_x[i].transpose()),\n",
        "                        DataSet_y[i])\n",
        "    print(timeit.default_timer() - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PAf8LBUFimlC",
        "outputId": "ded0de52-d77d-4632-e2e7-2015b4f42c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-3e5f3c45b18a>:6: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  dP[i] = abs(IFsig[i])**2\n",
            "<ipython-input-16-3be048384f2d>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model, map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0010080\n",
            "0.0009923\n",
            "0.0009755\n",
            "0.0009905\n",
            "0.0009725\n",
            "0.0009809\n",
            "0.0009672\n",
            "0.0009686\n",
            "0.0009462\n",
            "0.0009799\n",
            "0.0009098\n",
            "0.0009757\n",
            "0.0010028\n",
            "0.0009557\n",
            "0.0009848\n",
            "0.0009742\n",
            "0.0009814\n",
            "0.0009644\n",
            "0.0009358\n",
            "0.0009700\n",
            "0.0009752\n",
            "0.0009516\n",
            "0.0010131\n",
            "0.0010046\n",
            "0.0010272\n",
            "0.0009760\n",
            "0.0009625\n",
            "0.0009880\n",
            "0.0009769\n",
            "0.0009757\n",
            "0.0010129\n",
            "0.0009704\n",
            "0.0009849\n",
            "0.0009896\n",
            "0.0010355\n",
            "0.0009889\n",
            "0.0010258\n",
            "0.0009813\n",
            "0.0010091\n",
            "0.0009863\n",
            "0.0009809\n",
            "0.0009537\n",
            "0.0009633\n",
            "0.0009714\n",
            "0.0010380\n",
            "0.0010246\n",
            "0.0009603\n",
            "0.0009965\n",
            "0.0009437\n",
            "0.0009479\n",
            "0.0009339\n",
            "0.0009668\n",
            "0.0010073\n",
            "0.0009456\n",
            "0.0009496\n",
            "0.0009909\n",
            "0.0009496\n",
            "0.0010190\n",
            "0.0009990\n",
            "0.0009743\n",
            "0.0009667\n",
            "0.0009793\n",
            "0.0009765\n",
            "0.0009381\n",
            "0.0009631\n",
            "0.0009950\n",
            "0.0010033\n",
            "0.0010187\n",
            "0.0010117\n",
            "0.0009649\n",
            "0.0009494\n",
            "0.0009711\n",
            "0.0009267\n",
            "0.0009936\n",
            "0.0010028\n",
            "0.0009488\n",
            "0.0009651\n",
            "0.0009971\n",
            "0.0010538\n",
            "0.0009818\n",
            "0.0009949\n",
            "0.0009811\n",
            "0.0010264\n",
            "0.0009446\n",
            "0.0009594\n",
            "0.0009990\n",
            "0.0010094\n",
            "0.0010001\n",
            "0.0009938\n",
            "0.0009671\n",
            "0.0009917\n",
            "0.0009908\n",
            "0.0009736\n",
            "0.0009727\n",
            "0.0009773\n",
            "0.0009616\n",
            "0.0009746\n",
            "0.0009765\n",
            "0.0009692\n",
            "0.0010590\n",
            "0.0009804\n",
            "0.0010349\n",
            "0.0009873\n",
            "0.0010011\n",
            "0.0009571\n",
            "0.0009279\n",
            "0.0009750\n",
            "0.0010007\n",
            "0.0009677\n",
            "0.0009881\n",
            "0.0010064\n",
            "0.0010624\n",
            "0.0009742\n",
            "0.0009818\n",
            "0.0009723\n",
            "0.0009988\n",
            "0.0009265\n",
            "0.0009772\n",
            "0.0009782\n",
            "0.0009573\n",
            "0.0009421\n",
            "0.0010033\n",
            "0.0009879\n",
            "0.0009472\n",
            "0.0009894\n",
            "0.0009534\n",
            "0.0009723\n",
            "0.0009692\n",
            "0.0009724\n",
            "0.0009315\n",
            "0.0009659\n",
            "0.0010144\n",
            "0.0010060\n",
            "0.0009740\n",
            "0.0009888\n",
            "0.0009917\n",
            "0.0009983\n",
            "0.0009414\n",
            "0.0009373\n",
            "0.0009901\n",
            "0.0009493\n",
            "0.0009148\n",
            "0.0009908\n",
            "0.0010298\n",
            "0.0009749\n",
            "0.0009713\n",
            "0.0009704\n",
            "0.0009939\n",
            "0.0009740\n",
            "0.0009537\n",
            "0.0009841\n",
            "0.0009961\n",
            "0.0009773\n",
            "0.0009650\n",
            "0.0009953\n",
            "0.0010853\n",
            "0.0009470\n",
            "0.0009807\n",
            "0.0009304\n",
            "0.0009816\n",
            "0.0009545\n",
            "0.0009987\n",
            "0.0009575\n",
            "0.0010007\n",
            "0.0009426\n",
            "0.0010678\n",
            "0.0009962\n",
            "0.0009890\n",
            "0.0009057\n",
            "0.0009715\n",
            "0.0009743\n",
            "0.0010182\n",
            "0.0009501\n",
            "0.0009654\n",
            "0.0009572\n",
            "0.0009617\n",
            "0.0010478\n",
            "0.0010089\n",
            "0.0010133\n",
            "0.0009664\n",
            "0.0009961\n",
            "0.0009759\n",
            "0.0010426\n",
            "0.0009088\n",
            "0.0009793\n",
            "0.0009945\n",
            "0.0009847\n",
            "0.0009895\n",
            "0.0009648\n",
            "0.0010022\n",
            "0.0009601\n",
            "0.0009437\n",
            "0.0009536\n",
            "0.0010043\n",
            "0.0009736\n",
            "0.0010067\n",
            "0.0009630\n",
            "0.0009493\n",
            "0.0010037\n",
            "0.0009758\n",
            "0.0009639\n",
            "0.0009469\n",
            "0.0009723\n",
            "0.0009747\n",
            "0.0009927\n",
            "0.0009509\n",
            "0.0010145\n",
            "0.0009800\n",
            "0.0009756\n",
            "0.0009717\n",
            "0.0009501\n",
            "0.0009938\n",
            "0.0009536\n",
            "0.0009576\n",
            "0.0009742\n",
            "0.0009972\n",
            "0.0009868\n",
            "0.0010067\n",
            "0.0009282\n",
            "0.0009847\n",
            "0.0009834\n",
            "0.0009957\n",
            "0.0009511\n",
            "0.0009757\n",
            "0.0009717\n",
            "0.0009595\n",
            "0.0009783\n",
            "0.0009742\n",
            "0.0009502\n",
            "0.0009183\n",
            "0.0009647\n",
            "0.0010302\n",
            "0.0009673\n",
            "0.0010061\n",
            "0.0010007\n",
            "0.0009888\n",
            "0.0009675\n",
            "0.0009726\n",
            "0.0009682\n",
            "0.0009955\n",
            "0.0009355\n",
            "0.0009936\n",
            "0.0010547\n",
            "0.0009848\n",
            "0.0009831\n",
            "0.0009268\n",
            "0.0009412\n",
            "0.0009937\n",
            "0.0009705\n",
            "0.0010777\n",
            "0.0009832\n",
            "0.0010046\n",
            "0.0009340\n",
            "0.0010460\n",
            "0.0010570\n",
            "0.0009685\n",
            "0.0009657\n",
            "0.0009367\n",
            "0.0010212\n",
            "0.0010321\n",
            "0.0010060\n",
            "0.0009428\n",
            "0.0009955\n",
            "0.0009447\n",
            "0.0010425\n",
            "0.0009754\n",
            "0.0010084\n",
            "0.0009622\n",
            "0.0009520\n",
            "0.0010206\n",
            "0.0009734\n",
            "0.0009389\n",
            "0.0009697\n",
            "0.0009847\n",
            "0.0009546\n",
            "0.0010150\n",
            "0.0010165\n",
            "0.0010265\n",
            "0.0009234\n",
            "0.0009273\n",
            "0.0009921\n",
            "0.0009919\n",
            "0.0009870\n",
            "0.0009396\n",
            "0.0009548\n",
            "0.0009701\n",
            "0.0010553\n",
            "0.0010314\n",
            "0.0010234\n",
            "0.0009858\n",
            "0.0009828\n",
            "0.0010016\n",
            "0.0009743\n",
            "0.0009594\n",
            "0.0009539\n",
            "0.0009285\n",
            "0.0009505\n",
            "0.0010342\n",
            "0.0009516\n",
            "0.0009999\n",
            "0.0009761\n",
            "0.0009906\n",
            "0.0009337\n",
            "0.0009432\n",
            "0.0009885\n",
            "0.0009309\n",
            "0.0009381\n",
            "0.0009645\n",
            "0.0010085\n",
            "0.0009728\n",
            "0.0009838\n",
            "0.0009695\n",
            "0.0010137\n",
            "0.0009683\n",
            "0.0009527\n",
            "0.0009678\n",
            "0.0010210\n",
            "0.0010052\n",
            "0.0009523\n",
            "0.0010255\n",
            "0.0009952\n",
            "0.0009800\n",
            "0.0009632\n",
            "0.0009526\n",
            "0.0010099\n",
            "0.0009379\n",
            "0.0009661\n",
            "0.0010045\n",
            "0.0009282\n",
            "0.0009847\n",
            "0.0010292\n",
            "0.0009879\n",
            "0.0009722\n",
            "0.0009465\n",
            "0.0009401\n",
            "0.0009780\n",
            "0.0009517\n",
            "0.0009949\n",
            "0.0009918\n",
            "0.0009906\n",
            "0.0009641\n",
            "0.0010138\n",
            "0.0009854\n",
            "0.0009929\n",
            "0.0009469\n",
            "0.0009438\n",
            "0.0009417\n",
            "0.0010041\n",
            "0.0009671\n",
            "0.0009814\n",
            "0.0009507\n",
            "0.0009235\n",
            "0.0010305\n",
            "0.0010102\n",
            "0.0009435\n",
            "0.0009833\n",
            "0.0009305\n",
            "0.0009637\n",
            "0.0009279\n",
            "0.0009980\n",
            "0.0009880\n",
            "0.0009889\n",
            "0.0009577\n",
            "0.0009906\n",
            "0.0010153\n",
            "0.0010275\n",
            "0.0009666\n",
            "0.0009869\n",
            "0.0009949\n",
            "0.0009542\n",
            "0.0009450\n",
            "0.0009264\n",
            "0.0009488\n",
            "0.0010173\n",
            "0.0010288\n",
            "0.0010375\n",
            "0.0010382\n",
            "0.0010163\n",
            "0.0010178\n",
            "0.0009976\n",
            "0.0009800\n",
            "0.0009543\n",
            "0.0009537\n",
            "0.0010018\n",
            "0.0009575\n",
            "0.0009712\n",
            "0.0010182\n",
            "0.0010054\n",
            "0.0009883\n",
            "0.0009320\n",
            "0.0009523\n",
            "0.0010254\n",
            "0.0009578\n",
            "0.0009622\n",
            "0.0009818\n",
            "0.0009489\n",
            "0.0010203\n",
            "0.0010043\n",
            "0.0009827\n",
            "0.0009882\n",
            "0.0010057\n",
            "0.0009344\n",
            "0.0009316\n",
            "0.0009165\n",
            "0.0009878\n",
            "0.0009691\n",
            "0.0009274\n",
            "0.0010121\n",
            "0.0010197\n",
            "0.0009501\n",
            "0.0009806\n",
            "0.0009501\n",
            "0.0009912\n",
            "0.0009851\n",
            "0.0009944\n",
            "0.0009224\n",
            "0.0009305\n",
            "0.0009961\n",
            "0.0009921\n",
            "0.0010012\n",
            "0.0010160\n",
            "0.0009653\n",
            "0.0010000\n",
            "0.0009960\n",
            "0.0009786\n",
            "0.0010123\n",
            "0.0009950\n",
            "0.0009571\n",
            "0.0009744\n",
            "0.0009660\n",
            "0.0009866\n",
            "0.0010024\n",
            "0.0009714\n",
            "0.0009831\n",
            "0.0010289\n",
            "0.0009420\n",
            "0.0009844\n",
            "0.0009773\n",
            "0.0009810\n",
            "0.0009530\n",
            "0.0010181\n",
            "0.0009485\n",
            "0.0009582\n",
            "0.0009854\n",
            "0.0009845\n",
            "0.0010228\n",
            "0.0009827\n",
            "0.0009222\n",
            "0.0009981\n",
            "0.0009616\n",
            "0.0009521\n",
            "0.0009796\n",
            "0.0009963\n",
            "0.0010218\n",
            "0.0009674\n",
            "0.0009501\n",
            "0.0009687\n",
            "0.0009787\n",
            "0.0009563\n",
            "0.0009782\n",
            "0.0009491\n",
            "0.0009419\n",
            "0.0010538\n",
            "0.0009680\n",
            "0.0009888\n",
            "0.0009717\n",
            "0.0009813\n",
            "0.0009744\n",
            "0.0009831\n",
            "0.0009593\n",
            "0.0009847\n",
            "0.0009498\n",
            "0.0010593\n",
            "0.0011143\n",
            "0.0010878\n",
            "0.0009791\n",
            "0.0009899\n",
            "0.0010148\n",
            "0.0009803\n",
            "0.0009583\n",
            "0.0010018\n",
            "0.0009894\n",
            "0.0009981\n",
            "0.0009838\n",
            "0.0009926\n",
            "0.0009716\n",
            "0.0009821\n",
            "0.0010209\n",
            "0.0009992\n",
            "0.0009299\n",
            "0.0009621\n",
            "0.0010105\n",
            "0.0009458\n",
            "0.0008979\n",
            "0.0009534\n",
            "0.0010406\n",
            "0.0009760\n",
            "0.0009842\n",
            "0.0009825\n",
            "0.0009309\n",
            "0.0009817\n",
            "0.0009730\n",
            "0.0009637\n",
            "0.0009712\n",
            "0.0009472\n",
            "0.0009762\n",
            "0.0010210\n",
            "0.0010099\n",
            "0.0010326\n",
            "0.0009963\n",
            "0.0010076\n",
            "0.0009850\n",
            "0.0009728\n",
            "0.0009036\n",
            "0.0009783\n",
            "0.0009753\n",
            "0.0010131\n",
            "0.0010459\n",
            "0.0010227\n",
            "0.0009684\n",
            "0.0009994\n",
            "0.0009847\n",
            "0.0009599\n",
            "0.0009972\n",
            "0.0009596\n",
            "0.0009637\n",
            "0.0009327\n",
            "0.0009317\n",
            "0.0010988\n",
            "0.0010494\n",
            "0.0009788\n",
            "0.0009563\n",
            "0.0009528\n",
            "0.0009600\n",
            "0.0009877\n",
            "0.0009870\n",
            "0.0008994\n",
            "0.0010153\n",
            "0.0009173\n",
            "0.0010269\n",
            "0.0009702\n",
            "0.0009756\n",
            "0.0010392\n",
            "0.0009415\n",
            "0.0009607\n",
            "0.0008924\n",
            "0.0009548\n",
            "0.0009033\n",
            "0.0009520\n",
            "0.0009541\n",
            "0.0009957\n",
            "0.0010177\n",
            "0.0010427\n",
            "0.0009082\n",
            "0.0010407\n",
            "0.0010147\n",
            "0.0009658\n",
            "0.0009881\n",
            "0.0009515\n",
            "0.0009416\n",
            "0.0009467\n",
            "0.0009321\n",
            "0.0009908\n",
            "0.0009725\n",
            "0.0009158\n",
            "0.0009215\n",
            "0.0010147\n",
            "0.0009875\n",
            "0.0010014\n",
            "0.0009986\n",
            "0.0009869\n",
            "0.0009904\n",
            "0.0010269\n",
            "0.0010262\n",
            "0.0009867\n",
            "0.0009719\n",
            "0.0009530\n",
            "0.0009648\n",
            "0.0009769\n",
            "0.0010144\n",
            "0.0009944\n",
            "0.0009475\n",
            "0.0009598\n",
            "0.0010354\n",
            "0.0009652\n",
            "0.0009633\n",
            "0.0009625\n",
            "0.0009453\n",
            "0.0009821\n",
            "0.0009599\n",
            "0.0010103\n",
            "0.0009606\n",
            "0.0009358\n",
            "0.0009292\n",
            "0.0009955\n",
            "0.0009780\n",
            "0.0010198\n",
            "0.0009713\n",
            "0.0009406\n",
            "0.0009591\n",
            "0.0009676\n",
            "0.0009394\n",
            "0.0009602\n",
            "0.0009675\n",
            "0.0009770\n",
            "0.0010260\n",
            "0.0009717\n",
            "0.0009521\n",
            "0.0009488\n",
            "0.0009673\n",
            "0.0010377\n",
            "0.0009557\n",
            "0.0009675\n",
            "0.0009755\n",
            "0.0009652\n",
            "0.0009662\n",
            "0.0010554\n",
            "0.0010152\n",
            "0.0009574\n",
            "0.0009634\n",
            "0.0009856\n",
            "0.0009703\n",
            "0.0010052\n",
            "0.0009658\n",
            "0.0009664\n",
            "0.0009658\n",
            "0.0009902\n",
            "0.0010436\n",
            "0.0009949\n",
            "0.0010048\n",
            "0.0009751\n",
            "0.0009501\n",
            "0.0009829\n",
            "0.0009850\n",
            "0.0009670\n",
            "0.0009353\n",
            "0.0009229\n",
            "0.0009867\n",
            "0.0010388\n",
            "0.0010008\n",
            "0.0010265\n",
            "0.0010070\n",
            "0.0009651\n",
            "0.0010201\n",
            "0.0009285\n",
            "0.0009437\n",
            "0.0010085\n",
            "0.0009594\n",
            "0.0009864\n",
            "0.0010226\n",
            "0.0009895\n",
            "0.0009888\n",
            "0.0009737\n",
            "0.0010610\n",
            "0.0009853\n",
            "0.0010016\n",
            "0.0010098\n",
            "0.0009856\n",
            "0.0009796\n",
            "0.0009556\n",
            "0.0009863\n",
            "0.0009621\n",
            "0.0009376\n",
            "0.0009988\n",
            "0.0009709\n",
            "0.0009785\n",
            "0.0009694\n",
            "0.0009909\n",
            "0.0009807\n",
            "0.0009549\n",
            "0.0009510\n",
            "0.0010946\n",
            "0.0010140\n",
            "0.0009864\n",
            "0.0010007\n",
            "0.0009960\n",
            "0.0009700\n",
            "0.0010200\n",
            "0.0009423\n",
            "0.0009885\n",
            "0.0009567\n",
            "0.0009335\n",
            "0.0010129\n",
            "0.0009775\n",
            "0.0009406\n",
            "0.0010109\n",
            "0.0009852\n",
            "0.0009797\n",
            "0.0009602\n",
            "0.0010210\n",
            "0.0009706\n",
            "0.0009306\n",
            "0.0009457\n",
            "0.0010829\n",
            "0.0009774\n",
            "0.0010033\n",
            "0.0010038\n",
            "0.0009387\n",
            "0.0009853\n",
            "0.0009712\n",
            "0.0009471\n",
            "0.0010235\n",
            "0.0010105\n",
            "0.0009197\n",
            "0.0010160\n",
            "0.0010386\n",
            "0.0010300\n",
            "0.0009634\n",
            "0.0009947\n",
            "0.0009974\n",
            "0.0010070\n",
            "0.0009186\n",
            "0.0009397\n",
            "0.0009469\n",
            "0.0009675\n",
            "0.0010023\n",
            "0.0010094\n",
            "0.0009397\n",
            "0.0009630\n",
            "0.0009728\n",
            "0.0009620\n",
            "0.0009421\n",
            "0.0009870\n",
            "0.0009404\n",
            "0.0009651\n",
            "0.0009698\n",
            "0.0010256\n",
            "0.0009982\n",
            "0.0010285\n",
            "0.0009872\n",
            "0.0010124\n",
            "0.0010045\n",
            "0.0009882\n",
            "0.0009915\n",
            "0.0009303\n",
            "0.0009179\n",
            "0.0009905\n",
            "0.0009997\n",
            "0.0009977\n",
            "0.0010197\n",
            "0.0009581\n",
            "0.0009949\n",
            "0.0009721\n",
            "0.0009548\n",
            "0.0009709\n",
            "0.0009348\n",
            "0.0009579\n",
            "0.0010062\n",
            "0.0009732\n",
            "0.0009947\n",
            "0.0009950\n",
            "0.0009376\n",
            "0.0009935\n",
            "0.0010188\n",
            "0.0009688\n",
            "0.0010024\n",
            "0.0009795\n",
            "0.0009982\n",
            "0.0009420\n",
            "0.0010238\n",
            "0.0009764\n",
            "0.0010177\n",
            "0.0009693\n",
            "0.0009784\n",
            "0.0009819\n",
            "0.0009810\n",
            "0.0009507\n",
            "0.0010338\n",
            "0.0009783\n",
            "0.0009571\n",
            "0.0010328\n",
            "0.0009910\n",
            "0.0009620\n",
            "0.0009539\n",
            "0.0009725\n",
            "0.0010245\n",
            "0.0009662\n",
            "0.0009973\n",
            "0.0009788\n",
            "0.0009426\n",
            "0.0009736\n",
            "0.0009878\n",
            "0.0010132\n",
            "0.0010168\n",
            "0.0009675\n",
            "0.0009654\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-03a89d972bcb>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#     print(snr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mDataSet_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataSet_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataSet_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataSet_HH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mH_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxTx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxTy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput_ISDNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataSet_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataSet_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataSet_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataSet_HH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# LS(DataSet_x, DataSet_y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-0730f61364af>\u001b[0m in \u001b[0;36mInput_ISDNN\u001b[0;34m(mode, DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_samp)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mH_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataSet_HH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mH_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataSet_H\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mDi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpinv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataSet_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataSet_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mxTy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataSet_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataSet_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mH_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxTy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "SNR_min_dB  = 0\n",
        "SNR_max_dB  = 20\n",
        "step_dB     = 2\n",
        "num_dB      = int((SNR_max_dB - SNR_min_dB) / step_dB) + 1\n",
        "\n",
        "SNR         = np.linspace(SNR_min_dB, SNR_max_dB, num=num_dB)\n",
        "log         = './model/log_test.txt'\n",
        "\n",
        "N_test = int(100)\n",
        "\n",
        "for i in range (100):\n",
        "    for snr in SNR:\n",
        "        # with Record(log):\n",
        "        #     print(snr)\n",
        "        DataSet_x, DataSet_y, DataSet_H, DataSet_HH = Gen_dataset('test', snr, 0, N_test)\n",
        "        H_true, H_raw, H_in, e, xTx, xTy, Di = Input_ISDNN('test', DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_test)\n",
        "\n",
        "        # LS(DataSet_x, DataSet_y)\n",
        "        test(H_raw, Di, H_in, e, xTx, xTy, N_test, log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6_n95BiimlC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}