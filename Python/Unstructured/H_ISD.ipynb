{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSmcwLZuptDU"
      },
      "source": [
        "# Proposed Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG5fZn_ypunP"
      },
      "source": [
        "# **Biblioheque**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--SvzvIspu-U"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from scipy.stats import rice\n",
        "import pickle\n",
        "# import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import sys\n",
        "import timeit\n",
        "import os\n",
        "\n",
        "torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
        "torch.set_default_dtype(torch.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBk01Vu0pvMm"
      },
      "source": [
        "# class to save results in file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06IymVCkpvYS"
      },
      "outputs": [],
      "source": [
        "class Record:\n",
        "    def __init__(self, TextName):\n",
        "        self.out_file = open(TextName, 'a')\n",
        "        self.old_stdout = sys.stdout\n",
        "        sys.stdout = self\n",
        "\n",
        "    def write(self, text):\n",
        "        self.old_stdout.write(text)\n",
        "        self.out_file.write(text)\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout = self.old_stdout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xPamGjNpv8E"
      },
      "source": [
        "# **slicer the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlQdA0xHpwF7"
      },
      "outputs": [],
      "source": [
        "def slicer(data):\n",
        "    dataI = data[slice(0, len(data), 2)]\n",
        "    dataQ = data[slice(1, len(data), 2)]\n",
        "    return(dataI, dataQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOoiIo_LpwQl"
      },
      "source": [
        "# **Modulation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIftC8fvpwir"
      },
      "outputs": [],
      "source": [
        "def mapper_16QAM(QAM16, data):\n",
        "    map0 = 2*data[slice(0, len(data), 2)] + data[slice(1, len(data), 2)]\n",
        "    map0 = list(map(int, map0))\n",
        "    dataMapped = []\n",
        "    for i in range(len(map0)):\n",
        "        dataMapped.append(QAM16[map0[i]])\n",
        "    return(dataMapped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYhPgAIcqtGT"
      },
      "outputs": [],
      "source": [
        "def calculate_bits(Modulation,NumSubcarriers,NumDataSymb):\n",
        "    if Modulation=='QPSK':\n",
        "        Nbpscs=2\n",
        "    elif Modulation=='16QAM':\n",
        "        Nbpscs=4\n",
        "    return NumDataSymb*NumSubcarriers*Nbpscs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgX6tiqFpwvb"
      },
      "source": [
        "# **generate noise**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LQwZvGaprKZ"
      },
      "outputs": [],
      "source": [
        "def AWGN(IFsig, SNR):\n",
        "    dP = np.zeros(len(IFsig))\n",
        "    P = 0\n",
        "\n",
        "    for i in range(len(IFsig)):\n",
        "        dP[i] = abs(IFsig[i])**2\n",
        "        P = P + dP[i]\n",
        "\n",
        "    P = P/len(IFsig)\n",
        "    gamma = 10**(SNR/10)\n",
        "    N0 = P/gamma\n",
        "    n = ((N0/2)**(0.5))*np.random.standard_normal(len(IFsig))\n",
        "    IF_n = np.zeros((len(IFsig),1))\n",
        "\n",
        "    for i in range(len(IFsig)):\n",
        "        IF_n[i,:] = IFsig[i] + n[i]\n",
        "\n",
        "    return(IF_n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDx6TEAyimk9"
      },
      "source": [
        "# Generate channel model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnoArgs7imk9"
      },
      "outputs": [],
      "source": [
        "def Generate_channel(Nr, Nt, type):\n",
        "    if (type == 'gauss'):\n",
        "        return (np.random.normal(size=(Nr,Nt))+1j*np.random.normal(size=(Nr,Nt)))/np.sqrt(2)\n",
        "    if (type == 'rayleigh'):\n",
        "        return (np.random.rayleigh(scale=(1/np.sqrt(2)), size=(Nr,Nt)) + 1j*np.random.rayleigh(scale=(1/np.sqrt(2)), size=(Nr,Nt)))/np.sqrt(2)\n",
        "    if (type == 'rician'):\n",
        "        b = 1/np.sqrt(2)\n",
        "        return (rice.rvs(b, size=(Nr,Nt)) + 1j*rice.rvs(b, size=(Nr,Nt)))/np.sqrt(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eciNtnFjq2yd"
      },
      "source": [
        "# **Generate Dataset**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AE4Q6jZCq3CY"
      },
      "outputs": [],
      "source": [
        "DataSet_x   = []  # x dataset after modulation\n",
        "DataSet_y   = []  # y dataset\n",
        "DataSet_HH  = []  # H dataset\n",
        "DataSet_b   = []  # binary dataset\n",
        "SNR_min_dB  = 0\n",
        "SNR_max_dB  = 20\n",
        "step_dB     = 5\n",
        "num_dB      = int((SNR_max_dB - SNR_min_dB) / step_dB) + 1\n",
        "\n",
        "SNR         = np.linspace(SNR_min_dB, SNR_max_dB, num=num_dB)\n",
        "\n",
        "\n",
        "Nt = 8             # Tx: 8\n",
        "Nr = 64            # Rx: 128\n",
        "N_samp = 4000\n",
        "\n",
        "\n",
        "def Gen_dataset(mode, snr, imperfect, N_samp):\n",
        "    DataSet_x   = []  # x dataset after modulation\n",
        "    DataSet_y   = []  # y dataset\n",
        "    DataSet_H   = []\n",
        "    DataSet_HH  = []\n",
        "\n",
        "    NumSubcarriers = 1\n",
        "    Modulation = '16QAM'\n",
        "    QAM16 = [-1, -0.333, 0.333, 1]\n",
        "    NumDataSymb = 1\n",
        "    N_type = 'gauss'\n",
        "\n",
        "    if mode == 'train':\n",
        "        for snr in SNR:\n",
        "            for runIdx in range(0, N_samp):      # ! 20000 x Nt: samples\n",
        "                H = Generate_channel(Nt, Nr, N_type)\n",
        "                HH = np.concatenate((np.concatenate((H.real, H.imag), axis=1),\n",
        "                                    np.concatenate((-H.imag, H.real), axis=1)), axis=0)\n",
        "                x = np.zeros((2*Nt, NumSubcarriers))\n",
        "                a = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "                DataRaw = np.zeros((Nt, a))\n",
        "                for t in range(Nt):\n",
        "                    #\"data symbol generate\"\n",
        "                    NumBits = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "                    bit = np.random.randint(1, 3, NumBits)-1\n",
        "                    DataRaw[t, :] = bit\n",
        "                    for j in range(4):\n",
        "                        DataSet_b.append(bit[j])\n",
        "                    I = np.zeros((1, a))\n",
        "                    I[0, :] = DataRaw[t, :]\n",
        "                    (dataI, dataQ) = slicer(I[0])\n",
        "\n",
        "                    # Mapper\n",
        "                    mapI = mapper_16QAM(QAM16, dataI)\n",
        "                    mapQ = mapper_16QAM(QAM16, dataQ)\n",
        "                    x[t] = mapI[0]\n",
        "                    x[t+Nt] = mapQ[0]\n",
        "\n",
        "                # transpose\n",
        "                x = x.transpose()\n",
        "\n",
        "                y_wo_noise = np.matmul(x, HH)\n",
        "\n",
        "                # noise\n",
        "                noise = AWGN(y_wo_noise.transpose(), snr)\n",
        "\n",
        "                y = y_wo_noise + noise.transpose()\n",
        "\n",
        "                DataSet_x.append(x)    # ! I, Q sample distance by Nt.\n",
        "                DataSet_y.append(y)                 # ! output sample\n",
        "\n",
        "                # Imperfect channel: 5%\n",
        "                # coef = (2*np.random.randint(0,2,size=HH.shape) - 1)\n",
        "                # HH = HH + coef * HH * 0.05\n",
        "                DataSet_HH.append(HH)\n",
        "                DataSet_H.append(H)               # ! Generated channel\n",
        "\n",
        "    else:\n",
        "        for runIdx in range(0, N_samp):      # ! 20000 x Nt: samples\n",
        "            H = Generate_channel(Nt, Nr, N_type)\n",
        "            HH = np.concatenate((np.concatenate((H.real, H.imag), axis=1),\n",
        "                                np.concatenate((-H.imag, H.real), axis=1)), axis=0)\n",
        "            x = np.zeros((2*Nt, NumSubcarriers))\n",
        "            a = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "            DataRaw = np.zeros((Nt, a))\n",
        "            for t in range(Nt):\n",
        "                #\"data symbol generate\"\n",
        "                NumBits = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "                bit = np.random.randint(1, 3, NumBits)-1\n",
        "                DataRaw[t, :] = bit\n",
        "                for j in range(4):\n",
        "                    DataSet_b.append(bit[j])\n",
        "                I = np.zeros((1, a))\n",
        "                I[0, :] = DataRaw[t, :]\n",
        "                (dataI, dataQ) = slicer(I[0])\n",
        "\n",
        "                # Mapper\n",
        "                mapI = mapper_16QAM(QAM16, dataI)\n",
        "                mapQ = mapper_16QAM(QAM16, dataQ)\n",
        "                x[t] = mapI[0]\n",
        "                x[t+Nt] = mapQ[0]\n",
        "\n",
        "            # transpose\n",
        "            x = x.transpose()\n",
        "\n",
        "            y_wo_noise = np.matmul(x, HH)\n",
        "\n",
        "            # noise\n",
        "            noise = AWGN(y_wo_noise.transpose(), snr)\n",
        "\n",
        "            y = y_wo_noise + noise.transpose()\n",
        "\n",
        "            DataSet_x.append(x)    # ! I, Q sample distance by Nt.\n",
        "            DataSet_y.append(y)                 # ! output sample\n",
        "\n",
        "            # Imperfect channel: 5%\n",
        "            DataSet_HH.append(HH)\n",
        "            DataSet_H.append(H)               # ! Generated channel\n",
        "\n",
        "\n",
        "    # Shuffle dataset\n",
        "    random.seed(1)\n",
        "    temp = list(zip(DataSet_x, DataSet_y, DataSet_H, DataSet_HH))\n",
        "    random.shuffle(temp)\n",
        "    DataSet_x, DataSet_y, DataSet_H, DataSet_HH = zip(*temp)\n",
        "\n",
        "    return DataSet_x, DataSet_y, DataSet_H, DataSet_HH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nWo2pTrimk-"
      },
      "outputs": [],
      "source": [
        "def reconstruct_channel (H):\n",
        "# H_raw = [R(H) I(H); -I(H) R(H)]\n",
        "# we have four version of H_est\n",
        "    H_est_1 = []\n",
        "    H_est_2 = []\n",
        "    H_est_3 = []\n",
        "    H_est_4 = []\n",
        "\n",
        "    H_est_Re_1 = H[0:Nt, 0:Nr]\n",
        "    H_est_Im_1 = H[0:Nt, Nr:2*Nr]\n",
        "    H_est_Im_2 = - H[Nt:2*Nt, 0:Nr]\n",
        "    H_est_Re_2 = H[Nt:2*Nt, Nr:2*Nr]\n",
        "\n",
        "    H_est_1 = H_est_Re_1 + 1j * H_est_Im_1\n",
        "    H_est_2 = H_est_Re_1 + 1j * H_est_Im_2\n",
        "    H_est_3 = H_est_Re_2 + 1j * H_est_Im_1\n",
        "    H_est_4 = H_est_Re_2 + 1j * H_est_Im_2\n",
        "\n",
        "    return H_est_1, H_est_2, H_est_3, H_est_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQUDE5Q1imk-"
      },
      "outputs": [],
      "source": [
        "# def NMSE(H_est, H_raw):\n",
        "#     H_est_1, H_est_2, H_est_3, H_est_4 = reconstruct_channel(H_est)\n",
        "#     H_est_vec_1 = torch.reshape(H_est_1, [Nt * Nr, 1])\n",
        "#     H_est_vec_2 = torch.reshape(H_est_2, [Nt * Nr, 1])\n",
        "#     H_est_vec_3 = torch.reshape(H_est_3, [Nt * Nr, 1])\n",
        "#     H_est_vec_4 = torch.reshape(H_est_4, [Nt * Nr, 1])\n",
        "\n",
        "#     H_raw_vec = torch.reshape(H_raw, [Nt * Nr, 1])\n",
        "\n",
        "#     mse_1       = (torch.norm(H_raw_vec - H_est_vec_1)**2) / len(H_raw_vec)\n",
        "#     mse_2       = (torch.norm(H_raw_vec - H_est_vec_2)**2) / len(H_raw_vec)\n",
        "#     mse_3       = (torch.norm(H_raw_vec - H_est_vec_3)**2) / len(H_raw_vec)\n",
        "#     mse_4       = (torch.norm(H_raw_vec - H_est_vec_4)**2) / len(H_raw_vec)\n",
        "\n",
        "#     sigEner   = torch.norm(H_raw_vec)**2\n",
        "\n",
        "#     nmse_1      = mse_1 / sigEner\n",
        "#     nmse_2      = mse_2 / sigEner\n",
        "#     nmse_3      = mse_3 / sigEner\n",
        "#     nmse_4      = mse_4 / sigEner\n",
        "\n",
        "#     # Best nmse\n",
        "#     nmse        = min([nmse_1, nmse_2, nmse_3, nmse_4])\n",
        "\n",
        "#     return torch.abs(nmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knU4OnL3imk_"
      },
      "outputs": [],
      "source": [
        "def NMSE(H_est, H_raw):\n",
        "    H_est_1, H_est_2, H_est_3, H_est_4 = reconstruct_channel(H_est)\n",
        "\n",
        "    # Lấy phần thực của các tensor nếu chúng là complex\n",
        "    H_est_vec_1 = torch.reshape(H_est_1, [Nt * Nr, 1]).abs()\n",
        "    H_est_vec_2 = torch.reshape(H_est_2, [Nt * Nr, 1]).abs()\n",
        "    H_est_vec_3 = torch.reshape(H_est_3, [Nt * Nr, 1]).abs()\n",
        "    H_est_vec_4 = torch.reshape(H_est_4, [Nt * Nr, 1]).abs()\n",
        "\n",
        "    H_raw_vec = torch.reshape(H_raw, [Nt * Nr, 1]).abs()\n",
        "\n",
        "    mse_1 = (torch.norm(H_raw_vec - H_est_vec_1)**2) / len(H_raw_vec)\n",
        "    mse_2 = (torch.norm(H_raw_vec - H_est_vec_2)**2) / len(H_raw_vec)\n",
        "    mse_3 = (torch.norm(H_raw_vec - H_est_vec_3)**2) / len(H_raw_vec)\n",
        "    mse_4 = (torch.norm(H_raw_vec - H_est_vec_4)**2) / len(H_raw_vec)\n",
        "\n",
        "    sigEner = torch.norm(H_raw_vec)**2\n",
        "\n",
        "    nmse_1 = mse_1 / sigEner\n",
        "    nmse_2 = mse_2 / sigEner\n",
        "    nmse_3 = mse_3 / sigEner\n",
        "    nmse_4 = mse_4 / sigEner\n",
        "\n",
        "    # Chọn NMSE tốt nhất\n",
        "    nmse = min([nmse_1, nmse_2, nmse_3, nmse_4])\n",
        "\n",
        "    return torch.abs(nmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyjIgxUurU0P"
      },
      "outputs": [],
      "source": [
        "def Input_ISDNN(mode, DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_samp):\n",
        "    H_in = []        # ! H_in    , np.diag(np.diag()) return a diag matrix instead of diag components.\n",
        "    H_true = []   # ! generated s\n",
        "    H_raw = []\n",
        "    e = []        # ! vector errors\n",
        "    xTx = []\n",
        "    xTy = []\n",
        "    Di = []\n",
        "    # steering = [] # ! Steering vector: ZoA and AoA\n",
        "\n",
        "    if mode == 'train':\n",
        "        n_sample = N_samp * len(SNR)\n",
        "    else:\n",
        "        n_sample = N_samp\n",
        "\n",
        "    for i in range (n_sample):\n",
        "        H_true.append(torch.tensor(DataSet_HH[i]))\n",
        "        H_raw.append(torch.tensor(DataSet_H[i]))\n",
        "        Di.append(torch.tensor(np.linalg.pinv(np.diag(np.diag(np.dot(DataSet_x[i].transpose(), DataSet_x[i]))))))\n",
        "        xTy.append(torch.tensor(np.dot(DataSet_x[i].transpose(), DataSet_y[i])))\n",
        "        H_in.append(torch.matmul(Di[i], xTy[i]))\n",
        "        e.append(torch.rand([2*Nt, 2*Nr]))\n",
        "        xTx.append(torch.tensor(np.dot(DataSet_x[i].transpose(), DataSet_x[i])))\n",
        "        # steering.append(torch.tensor(DataSet_Steering[i]))\n",
        "\n",
        "    H_true = torch.stack(H_true, dim=0)\n",
        "    H_raw = torch.stack(H_raw, dim=0)\n",
        "    H_in = torch.stack(H_in, dim=0)\n",
        "    e = torch.stack(e, dim=0)\n",
        "    xTx = torch.stack(xTx, dim=0)\n",
        "    xTy = torch.stack(xTy, dim=0)\n",
        "    Di = torch.stack(Di, dim=0)\n",
        "    # steering = torch.stack(steering, dim=0)\n",
        "\n",
        "    return H_true, H_raw, H_in, e, xTx, xTy, Di"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGhdBsghq3M9"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHceh5kuq3ZD"
      },
      "outputs": [],
      "source": [
        "class xv(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(xv, self).__init__()\n",
        "\n",
        "        self.alpha1 = torch.nn.parameter.Parameter(torch.rand(1))\n",
        "        self.alpha2 = torch.nn.parameter.Parameter(torch.tensor([0.5]))\n",
        "\n",
        "    def forward(self, Di, H, e, xTx, xTy):\n",
        "\n",
        "        xTxH = torch.bmm(xTx, H)\n",
        "\n",
        "        z    = H + torch.bmm(Di, torch.sub(xTy, xTxH)) + self.alpha1 * e\n",
        "\n",
        "        e    = torch.sub(xTy, xTxH)\n",
        "\n",
        "        H    = torch.add((1 - self.alpha2) * z, self.alpha2 * H)\n",
        "\n",
        "        return H, e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woRjD7lJssRq"
      },
      "outputs": [],
      "source": [
        "class model_driven(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(model_driven, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc2 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc3 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc4 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc5 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc6 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc7 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc8 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        # self.fc9 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        # self.fc10 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "\n",
        "        self.layer1=xv()\n",
        "        self.layer2=xv()\n",
        "        self.layer3=xv()\n",
        "        self.layer4=xv()\n",
        "        # self.layer5=xv()\n",
        "\n",
        "    def forward(self, Di, H_in, e, xTx, xTy):\n",
        "        e = self.fc1(e)\n",
        "        e = self.fc2(e)\n",
        "        H, e = self.layer1(Di, H_in, e, xTx, xTy)\n",
        "        H = torch.tanh(H)\n",
        "\n",
        "        e = self.fc3(e)\n",
        "        e = self.fc4(e)\n",
        "        H, e = self.layer2(Di, H, e, xTx, xTy)\n",
        "        H = torch.tanh(H)\n",
        "\n",
        "        e = self.fc5(e)\n",
        "        e = self.fc6(e)\n",
        "        H, e = self.layer3(Di, H, e, xTx, xTy)\n",
        "        H = torch.tanh(H)\n",
        "\n",
        "        e = self.fc7(e)\n",
        "        e = self.fc8(e)\n",
        "        H, e = self.layer4(Di, H, e, xTx, xTy)\n",
        "        # H = torch.tanh(H)\n",
        "\n",
        "        # e = self.fc9(e)\n",
        "        # e = self.fc10(e)\n",
        "        # H, e = self.layer5(Di, H, e, xTx, xTy)\n",
        "\n",
        "        return H, e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZjihTOXq3kG"
      },
      "source": [
        "# Define model, optimizer, and loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp9fRd3gq3tw"
      },
      "outputs": [],
      "source": [
        "def def_model():\n",
        "    model = model_driven()\n",
        "    loss = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    folder_model = './model/'\n",
        "\n",
        "    if not os.path.isdir(folder_model):\n",
        "        os.makedirs(folder_model)\n",
        "\n",
        "    file_model = folder_model + 'H'\n",
        "    # if os.path.isfile(file_model):\n",
        "    #     generator = torch.load(file_model)\n",
        "\n",
        "    record_file = 'H'\n",
        "    return model, loss, optimizer, record_file, file_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYWM7SzItKzS"
      },
      "source": [
        "# Main program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jv7lDwyxtFe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f00abafa-7efd-4e2e-f1bd-eb1c49bd1f90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dữ liệu đã được tải từ file tĩnh!\n",
            "Begin training...\n",
            "171.12210586375957\n",
            "169.1452664235152\n",
            "164.95787492583258\n",
            "172.2792463606125\n",
            "175.0703038807602\n",
            "174.08046134300122\n",
            "170.3904815190685\n",
            "173.8344158922376\n",
            "175.56228482273409\n",
            "166.06540017575537\n",
            "172.9352353302479\n",
            "171.55670202407993\n",
            "170.60322609819173\n",
            "169.1600745700964\n",
            "176.14904716361204\n",
            "173.1254239499164\n",
            "172.77947396676362\n",
            "169.35171610072874\n",
            "171.6783139521746\n",
            "172.91807321334727\n",
            "167.165763314133\n",
            "170.37997113510653\n",
            "166.37105419798596\n",
            "168.42234745356168\n",
            "170.9061348961076\n",
            "173.16115050042586\n",
            "174.8083467343944\n",
            "172.83461504882155\n",
            "170.75491853754426\n",
            "167.84177495170698\n",
            "173.28734194298733\n",
            "173.17026856302422\n",
            "169.7071472960083\n",
            "168.30002501814866\n",
            "169.65127186229927\n",
            "174.40446916222797\n",
            "168.60733437737596\n",
            "171.8115649833203\n",
            "176.88599156178566\n",
            "168.90262118842912\n",
            "169.40856585381204\n",
            "171.08868839518973\n",
            "172.81846616660414\n",
            "174.21700201600677\n",
            "170.90877205141211\n",
            "176.4517914511775\n",
            "172.24355834723394\n",
            "172.02940881710137\n",
            "170.15038117921722\n",
            "169.2248565048531\n",
            "172.74612735288443\n",
            "168.4178429490325\n",
            "170.29633469969232\n",
            "169.48727673538127\n",
            "165.6578402228426\n",
            "167.1704585809999\n",
            "172.35548704063186\n",
            "170.52009037579023\n",
            "170.28040802195602\n",
            "170.9220647250389\n",
            "171.82928023154528\n",
            "174.4005989387153\n",
            "170.98429301426708\n",
            "170.40648309301056\n",
            "165.33946683838454\n",
            "165.32226106603937\n",
            "171.274238368399\n",
            "172.86349847947608\n",
            "167.80923160678208\n",
            "167.22321773796895\n",
            "168.46920355772434\n",
            "172.088757599272\n",
            "173.71191290718974\n",
            "169.62231801453763\n",
            "171.55650474673385\n",
            "171.56589417536756\n",
            "167.91493126641896\n",
            "165.3683467376115\n",
            "173.74006792366504\n",
            "163.76780121273393\n",
            "171.21665579408065\n",
            "172.84210045305656\n",
            "171.12352236148763\n",
            "166.67350263449336\n",
            "174.44373002308325\n",
            "168.9241607114829\n",
            "172.11884373459603\n",
            "169.86492365195562\n",
            "173.74580917727508\n",
            "176.90584411122663\n",
            "173.21855837688972\n",
            "172.46040842932834\n",
            "167.05303742942738\n",
            "167.79891936585219\n",
            "175.14223485463663\n",
            "172.75967623926053\n",
            "171.65871547599295\n",
            "175.08020720062132\n",
            "171.00801711846404\n",
            "173.90499772889302\n",
            "172.19156943085045\n",
            "172.80669699445144\n",
            "166.95502486718044\n",
            "170.34953792723712\n",
            "174.4269349722894\n",
            "173.39883040552968\n",
            "171.43042387945732\n",
            "172.72504638557436\n",
            "178.26401003915655\n",
            "168.017742350037\n",
            "169.9511545558526\n",
            "170.64485820606444\n",
            "169.61943598420348\n",
            "173.01224435155413\n",
            "170.90957687647736\n",
            "174.90666454448095\n",
            "174.6953109644935\n",
            "172.0161205408368\n",
            "170.17047607433187\n",
            "171.57896949409297\n",
            "170.18900516555163\n",
            "170.8275052915779\n",
            "171.58892385327047\n",
            "176.97862310913618\n",
            "174.88170628291505\n",
            "171.02742556368932\n",
            "165.33525356674738\n",
            "168.4697103368245\n",
            "171.6169832840881\n",
            "174.561536026086\n",
            "171.88859775639693\n",
            "166.7147036045051\n",
            "176.5370577767452\n",
            "173.56552293070882\n",
            "171.3192116466714\n",
            "173.8687195256781\n",
            "169.7410065942395\n",
            "171.12776422932143\n",
            "175.71839690062822\n",
            "171.04709217443067\n",
            "169.17990867879834\n",
            "171.27374599860565\n",
            "167.26988374017503\n",
            "174.24943993451868\n",
            "173.4982418143978\n",
            "172.35109324372505\n",
            "170.17370675954848\n",
            "172.70559909463765\n",
            "168.29370812265722\n",
            "171.09487471291416\n",
            "173.47448074265105\n",
            "170.51345057129063\n",
            "164.30261628918544\n",
            "170.02000428438504\n",
            "167.924761672341\n",
            "164.9238662195997\n",
            "173.3473435236013\n",
            "172.4156475690263\n",
            "171.13667137014986\n",
            "174.37358172493902\n",
            "171.08726839815296\n",
            "169.77365540629233\n",
            "176.15949870655163\n",
            "170.27587757925076\n",
            "171.35698910614664\n",
            "169.74432179824404\n",
            "174.47404761909158\n",
            "172.61887012485866\n",
            "171.42733694308086\n",
            "170.9909860581614\n",
            "173.2917792185345\n",
            "170.3685167683737\n",
            "171.0885727449043\n",
            "164.48882847055967\n",
            "171.5463126483522\n",
            "172.48390855542894\n",
            "171.10093706642917\n",
            "166.9223210224802\n",
            "172.18528990773734\n",
            "174.02284284131227\n",
            "166.95162803852688\n",
            "171.28245254016906\n",
            "169.65675749794565\n",
            "175.39621697436965\n",
            "171.93559910366932\n",
            "171.15242546625538\n",
            "174.7159281216889\n",
            "168.82983963502105\n",
            "169.32800311945738\n",
            "170.01624498785247\n",
            "173.88799492560366\n",
            "171.85950761719388\n",
            "171.65306317687282\n",
            "173.94278283223665\n",
            "172.5231797614898\n",
            "174.59144935105996\n",
            "169.14068241012797\n",
            "172.23711865039525\n",
            "171.21028240440373\n",
            "168.81629886713839\n",
            "172.99028610986403\n",
            "171.16008577721595\n",
            "165.27445486818962\n",
            "173.37008109164245\n",
            "171.3756798920567\n",
            "176.46779726772556\n",
            "172.24236439857984\n",
            "168.97522400907522\n",
            "172.6059645131634\n",
            "172.8754519825616\n",
            "166.4276291590833\n",
            "168.67332731822296\n",
            "170.73247216268058\n",
            "173.8846522154185\n",
            "169.50510785169354\n",
            "172.3352496353413\n",
            "170.9945712718639\n",
            "170.91896045949227\n",
            "175.55353685465604\n",
            "173.00147051668156\n",
            "171.46857034908444\n",
            "167.35172942570878\n",
            "165.71511571249061\n",
            "167.70483606919134\n",
            "169.97633672047138\n",
            "172.10685821246793\n",
            "171.29170342773722\n",
            "171.80894905172644\n",
            "171.89707700741815\n",
            "170.7426750652345\n",
            "172.9528398932575\n",
            "170.79965298545224\n",
            "166.54824554407782\n",
            "169.73925569788753\n",
            "167.06549968334602\n",
            "171.24674046972927\n",
            "173.97678849406452\n",
            "177.03525176403807\n",
            "174.0653712489983\n",
            "172.53437286769994\n",
            "169.4268869983271\n",
            "170.22176789298385\n",
            "171.41707613361737\n",
            "170.03297618085796\n",
            "172.66724322762758\n",
            "173.4520261043042\n",
            "171.08063853870217\n",
            "166.96325320377372\n",
            "168.08783760865757\n",
            "172.79363306997297\n",
            "168.10146133553388\n",
            "169.44625110335295\n",
            "173.01479338391295\n",
            "175.17683844944202\n",
            "169.6018396429919\n",
            "168.48331693275293\n",
            "171.58513453338534\n",
            "169.93403098288073\n",
            "173.15690950747458\n",
            "165.41696628938934\n",
            "172.65945954584348\n",
            "172.2699143075041\n",
            "170.85194787285468\n",
            "170.7617302077424\n",
            "172.4251439164808\n",
            "169.59869737956586\n",
            "167.2377891465096\n",
            "168.3950634398566\n",
            "166.4918833635593\n",
            "164.49156893426795\n",
            "175.37776581595182\n",
            "170.82129969166002\n",
            "171.58718402227856\n",
            "172.61862538743608\n",
            "175.72640550192074\n",
            "169.72977886956988\n",
            "170.35535998657113\n",
            "171.89447830539513\n",
            "168.37059776965998\n",
            "168.60744114635278\n",
            "169.13050427608627\n",
            "170.1619307109072\n",
            "171.18453254627357\n",
            "175.20738125998656\n",
            "170.741879359573\n",
            "173.27443918188675\n",
            "172.05404534607274\n",
            "169.4076898307052\n",
            "167.92090706300345\n",
            "169.71771911145603\n",
            "171.04981128241292\n",
            "166.48479676765362\n",
            "166.89757428743792\n",
            "172.66843159451497\n",
            "171.04080414673354\n",
            "170.32747766837866\n",
            "168.24191783373837\n",
            "168.03063052036464\n",
            "174.33387764235505\n",
            "171.61396254766828\n",
            "171.8692496671738\n",
            "171.1678709427513\n",
            "176.37315371563705\n",
            "170.06778252168888\n",
            "168.0791003976179\n",
            "167.90360486118365\n",
            "170.80535855936105\n",
            "172.53895337710097\n",
            "170.2411170000355\n",
            "169.2317596429102\n",
            "168.08043317570693\n",
            "171.43870161690614\n",
            "171.28128467553506\n",
            "178.44901532056213\n",
            "167.83658099996336\n",
            "173.42230830261224\n",
            "174.59091299897352\n",
            "172.95029447786527\n",
            "173.3247635171542\n",
            "167.84237442978647\n",
            "168.7670212610098\n",
            "169.69977758961156\n",
            "169.889856677705\n",
            "169.64505323357045\n",
            "168.86155343574205\n",
            "172.93865256424928\n",
            "172.45907423746803\n",
            "168.09685221226678\n",
            "173.06372229816662\n",
            "173.81370779906084\n",
            "165.80245054274803\n",
            "170.1147108007195\n",
            "169.5350397697907\n",
            "160.88322618361104\n",
            "171.42607563234458\n",
            "174.23434296560265\n",
            "174.00563500449326\n",
            "172.0276959957086\n",
            "170.74392448427545\n",
            "168.73629048664228\n",
            "171.15986702008829\n",
            "167.83718466800102\n",
            "173.10385475024196\n",
            "167.96473169507345\n",
            "168.04024205737554\n",
            "172.3808076532266\n",
            "168.13864879030595\n",
            "170.68415449108568\n",
            "169.6391631144535\n",
            "170.9157254972205\n",
            "175.55194358638622\n",
            "167.3529196017392\n",
            "171.69362299384093\n",
            "171.7226248762338\n",
            "166.6427087099758\n",
            "172.2926714849038\n",
            "172.59142095476238\n",
            "168.1327705881583\n",
            "173.38188245009417\n",
            "170.83312992659629\n",
            "174.9012889224702\n",
            "166.9690868428013\n",
            "174.57234572331828\n",
            "166.6589475351115\n",
            "174.70076508746018\n",
            "176.278332674532\n",
            "167.9157149085081\n",
            "175.88791201514852\n",
            "164.56196454513196\n",
            "172.3949751172503\n",
            "169.52714984586672\n",
            "166.9147388089609\n",
            "171.19926665537193\n",
            "172.3923699471134\n",
            "174.98392155458524\n",
            "175.57271775023096\n",
            "175.07747484891283\n",
            "170.1088945283421\n",
            "170.84954627649515\n",
            "170.13320492193466\n",
            "172.5832251077722\n",
            "174.82731184379207\n",
            "168.48824433885414\n",
            "174.04915282579861\n",
            "168.63815354594345\n",
            "164.95474539708417\n",
            "168.49828475089396\n",
            "172.66830041531932\n",
            "166.02176696854804\n",
            "171.33208603679762\n",
            "172.57310299323552\n",
            "170.53838442605246\n",
            "174.63412542688718\n",
            "171.22569919213544\n",
            "172.4746735560083\n",
            "171.1637765244025\n",
            "172.3499026748464\n",
            "173.18292677351573\n",
            "172.14482145880584\n",
            "172.45284904560083\n",
            "170.5783775335893\n",
            "171.27457204405076\n",
            "173.94688682996792\n",
            "171.0371356248113\n",
            "169.88123883578567\n",
            "168.025086174083\n",
            "168.56115811722566\n",
            "172.34677123821996\n",
            "171.45827647150918\n",
            "169.42924107517257\n",
            "172.90063175106792\n",
            "171.45880040988894\n",
            "170.05279847343468\n",
            "174.28431391982866\n",
            "170.08665478443118\n",
            "170.85215575012037\n",
            "171.00444578418455\n",
            "172.3637009234534\n",
            "178.40823847448772\n",
            "168.67371364400117\n",
            "174.66803041643783\n",
            "174.11275674693022\n",
            "171.3988637646001\n",
            "175.1501662792807\n",
            "171.4096558812536\n",
            "169.3095117698097\n",
            "172.49513225406704\n",
            "173.73797512126606\n",
            "173.60932872899235\n",
            "171.9402006223113\n",
            "173.14737456995644\n",
            "172.71620199070145\n",
            "172.8638921819666\n",
            "173.02508903623323\n",
            "173.73208548020995\n",
            "171.07443689245278\n",
            "171.49275393418657\n",
            "173.79583314420367\n",
            "167.79557543078718\n",
            "174.28662035648048\n",
            "168.0868887766253\n",
            "176.01273073758136\n",
            "175.50737407852418\n",
            "170.89259715761307\n",
            "172.62293503666288\n",
            "169.49966227632052\n",
            "168.60843650379562\n",
            "170.66508281943734\n",
            "171.2371304896424\n",
            "172.2489045881692\n",
            "171.28458021867317\n",
            "170.25023832292914\n",
            "173.81013473135914\n",
            "170.71690475701695\n",
            "172.34126844697474\n",
            "170.78084326520548\n",
            "168.95963855087962\n",
            "168.49530379178202\n",
            "168.04866165136687\n",
            "174.47442244806385\n",
            "170.4730869979329\n",
            "172.77377358231624\n",
            "167.50226376837736\n",
            "171.3301888394008\n",
            "173.16042302772928\n",
            "169.90298594437476\n",
            "170.85709419559237\n",
            "171.1514760104996\n",
            "174.30424755664623\n",
            "171.22487752812867\n",
            "171.18321501547626\n",
            "165.48420782102426\n",
            "170.09855968372196\n",
            "173.51126065379955\n",
            "173.22492023735742\n",
            "173.1226235036232\n",
            "169.71984013179946\n",
            "171.3598751371386\n",
            "168.8045880002608\n",
            "173.35093171058566\n",
            "174.2208709946695\n",
            "176.09433086362995\n",
            "169.45801061328882\n",
            "174.20330663903658\n",
            "171.73521006924003\n",
            "172.53038091236982\n",
            "175.34278932279764\n",
            "173.62484664800505\n",
            "170.5614267674614\n",
            "172.32269547064578\n",
            "169.1074041905296\n",
            "169.7481260594626\n",
            "168.1468044455487\n",
            "171.32647203559029\n",
            "166.1401409707706\n",
            "174.82544519501397\n",
            "170.32168897053717\n",
            "169.16878492256393\n",
            "171.06337159308552\n",
            "173.85259256261494\n",
            "170.18620983684377\n",
            "167.9063423803703\n",
            "171.64925680996984\n",
            "173.92712870476097\n",
            "169.41708078308244\n",
            "172.48204774702245\n",
            "169.6580153488997\n",
            "172.73945111188144\n",
            "171.0373624646603\n",
            "173.13516777270186\n",
            "171.581509125438\n",
            "172.31791013196295\n",
            "1 0.2430115358052343 37.39300665928924\n",
            "100 0.0008713629419684805 0.5547487173747865\n",
            "200 0.0008645935816545667 0.5533397462053481\n",
            "300 0.0008791204475932285 0.5556123653106487\n",
            "400 0.00087727563580094 0.5638077701020436\n",
            "500 0.0008697670687385021 0.5595654883894065\n",
            "1678.5251036710001\n"
          ]
        }
      ],
      "source": [
        "epoch         = 0\n",
        "expected_epoch = 20000\n",
        "num_samp      = N_samp * len(SNR)\n",
        "best_nmse     = 1e9\n",
        "best_trainloss = 1e9\n",
        "early_stop    = 0\n",
        "best_model    = ''\n",
        "batch_size    = int(num_samp / 512)\n",
        "\n",
        "# Kiểm tra nếu file tĩnh tồn tại\n",
        "if os.path.exists('dataset_ISDNN.pkl'):\n",
        "    with open('dataset_ISDNN.pkl', 'rb') as f:\n",
        "        DataSet_x, DataSet_y, DataSet_H, DataSet_HH, H_true, H_raw, H_in, e, xTx, xTy, Di = pickle.load(f)\n",
        "    print(\"Dữ liệu đã được tải từ file tĩnh!\")\n",
        "else:\n",
        "    # Sinh dữ liệu nếu file tĩnh không tồn tại\n",
        "    DataSet_x, DataSet_y, DataSet_H, DataSet_HH = Gen_dataset('train', 0, 0, N_samp)\n",
        "    H_true, H_raw, H_in, e, xTx, xTy, Di = Input_ISDNN('train', DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_samp)\n",
        "\n",
        "    # Lưu dữ liệu để lần sau không phải sinh lại\n",
        "    with open('dataset_ISDNN.pkl', 'wb') as f:\n",
        "        pickle.dump((DataSet_x, DataSet_y, DataSet_H, DataSet_HH, H_true, H_raw, H_in, e, xTx, xTy, Di), f)\n",
        "    print(\"Dữ liệu đã được sinh và lưu lại!\")\n",
        "\n",
        "print(\"Begin training...\")\n",
        "starttime = timeit.default_timer()\n",
        "\n",
        "while(True):\n",
        "        epoch = epoch + 1\n",
        "\n",
        "        init_loss = 1e9\n",
        "        while( epoch == 1 and init_loss > 200):\n",
        "\n",
        "                model, loss, optimizer, record_file, file_model = def_model()\n",
        "                for bs in range (int(num_samp / batch_size)):\n",
        "                    H_1, e_1 = model(Di[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 H_in[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 e[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTx[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTy[0 + batch_size * bs:batch_size * (bs+1), :, :])   # predict output from the model\n",
        "                    init_loss = loss(H_1, H_true[0 + batch_size * bs:batch_size * (bs+1), :, :]).item()\n",
        "                    print(init_loss)\n",
        "\n",
        "        optimizer.zero_grad()   # zero the parameter gradients\n",
        "        train_loss = 0\n",
        "        H_f = torch.empty([num_samp, 2*Nt, 2*Nr])\n",
        "        for bs in range (int(num_samp / batch_size)):\n",
        "                H_o, e_o = model(Di[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 H_in[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 e[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTx[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTy[0 + batch_size * bs:batch_size * (bs+1), :, :])   # predict output from the model\n",
        "                H_f[0 + batch_size * bs:batch_size * (bs+1), :, :] = H_o\n",
        "                train_loss = loss(H_o,\n",
        "                                  H_true[0 + batch_size * bs:batch_size * (bs+1), :, :])   # calculate loss for the predicted output\n",
        "                train_loss.backward()   # backpropagate the loss\n",
        "                optimizer.step()        # adjust parameters based on the calculated gradients\n",
        "\n",
        "        if (epoch % 100 == 0 or epoch == 1):\n",
        "                nmse = 0\n",
        "                for j in range (num_samp):\n",
        "                        nmse += NMSE(H_f[j], H_raw[j])\n",
        "                nmse = nmse / num_samp\n",
        "\n",
        "                if (nmse <= best_nmse and train_loss <= best_trainloss):\n",
        "                        torch.save(model.state_dict(), file_model + '_' + str(epoch) + '.pth')\n",
        "                        best_model = file_model + '_' + str(epoch) + '.pth'\n",
        "                        best_nmse = nmse\n",
        "                        early_stop = 0\n",
        "                else:\n",
        "                        early_stop += 1\n",
        "\n",
        "                if (nmse > best_nmse and early_stop == 3 and train_loss > best_trainloss):\n",
        "                        with Record(record_file + '_log.txt'):\n",
        "                                print(epoch, nmse.item(), train_loss.item())\n",
        "                                print(str(timeit.default_timer()-starttime))\n",
        "                        break\n",
        "\n",
        "                with Record(record_file + '_log.txt'):\n",
        "                        print(epoch, nmse.item(), train_loss.item())\n",
        "\n",
        "        if epoch  == expected_epoch:\n",
        "                torch.save(model.state_dict(), file_model + '_' + str(epoch) + '.pth')\n",
        "                best_model = file_model + '_' + str(epoch) + '.pth'\n",
        "                with Record(record_file + '_log.txt'):\n",
        "                        print(\"epoch:\\n\", epoch)\n",
        "                        print(\"Latest NMSE:\\n\", nmse.item())\n",
        "                        print(\"Latest Loss:\\n\", train_loss.item())\n",
        "                        print(str(timeit.default_timer()-starttime))\n",
        "\n",
        "                break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOBK-_l-tRMO"
      },
      "source": [
        "# Test function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRRFGAX4tg_6"
      },
      "source": [
        "# Function to test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfvpexfwthNq"
      },
      "outputs": [],
      "source": [
        "def test(H_raw, Di, H_in, e, xTx, xTy, N_test, log):\n",
        "    # Load the model that we saved at the end of the training loop\n",
        "    model = model_driven()\n",
        "    model.load_state_dict(torch.load(best_model, map_location=torch.device('cpu')))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        H_o, e_o = model(Di, H_in, e, xTx, xTy)\n",
        "\n",
        "        nmse = 0\n",
        "        for j in range (N_test):\n",
        "            nmse += NMSE(H_o[j], H_raw[j])\n",
        "\n",
        "        nmse = nmse / N_test\n",
        "        with Record(log):\n",
        "            print(format(nmse.item(), '.7f'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlrHA2aWimlC"
      },
      "outputs": [],
      "source": [
        "## Generate dataset for test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE_IiAiKimlC"
      },
      "outputs": [],
      "source": [
        "def LS(DataSet_x, DataSet_y):\n",
        "    start = timeit.default_timer()\n",
        "    for i in range (len(DataSet_x)):\n",
        "        H_hat = np.matmul(\n",
        "                    np.matmul(\n",
        "                        np.linalg.pinv(np.matmul(DataSet_x[i].transpose(), DataSet_x[i])),\n",
        "                        DataSet_x[i].transpose()),\n",
        "                        DataSet_y[i])\n",
        "    print(timeit.default_timer() - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "PAf8LBUFimlC"
      },
      "outputs": [],
      "source": [
        "SNR_min_dB  = 0\n",
        "SNR_max_dB  = 20\n",
        "step_dB     = 5\n",
        "num_dB      = int((SNR_max_dB - SNR_min_dB) / step_dB) + 1\n",
        "\n",
        "SNR         = np.linspace(SNR_min_dB, SNR_max_dB, num=num_dB)\n",
        "log         = './model/log_test.txt'\n",
        "\n",
        "N_test = int(num_samp * 30/100)\n",
        "\n",
        "for i in range (100):\n",
        "    for snr in SNR:\n",
        "        # with Record(log):\n",
        "        #     print(snr)\n",
        "        DataSet_x, DataSet_y, DataSet_H, DataSet_HH = Gen_dataset('test', snr, 0, N_test)\n",
        "        H_true, H_raw, H_in, e, xTx, xTy, Di = Input_ISDNN('test', DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_test)\n",
        "\n",
        "        # LS(DataSet_x, DataSet_y)\n",
        "        test(H_raw, Di, H_in, e, xTx, xTy, N_test, log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6_n95BiimlC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}