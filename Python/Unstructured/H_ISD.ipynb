{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSmcwLZuptDU"
      },
      "source": [
        "# Proposed Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG5fZn_ypunP"
      },
      "source": [
        "# **Biblioheque**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "--SvzvIspu-U"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from scipy.stats import rice\n",
        "import pickle\n",
        "# import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import sys\n",
        "import timeit\n",
        "import os\n",
        "\n",
        "torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
        "torch.set_default_dtype(torch.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBk01Vu0pvMm"
      },
      "source": [
        "# class to save results in file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "06IymVCkpvYS"
      },
      "outputs": [],
      "source": [
        "class Record:\n",
        "    def __init__(self, TextName):\n",
        "        self.out_file = open(TextName, 'a')\n",
        "        self.old_stdout = sys.stdout\n",
        "        sys.stdout = self\n",
        "\n",
        "    def write(self, text):\n",
        "        self.old_stdout.write(text)\n",
        "        self.out_file.write(text)\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout = self.old_stdout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xPamGjNpv8E"
      },
      "source": [
        "# **slicer the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dlQdA0xHpwF7"
      },
      "outputs": [],
      "source": [
        "def slicer(data):\n",
        "    dataI = data[slice(0, len(data), 2)]\n",
        "    dataQ = data[slice(1, len(data), 2)]\n",
        "    return(dataI, dataQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOoiIo_LpwQl"
      },
      "source": [
        "# **Modulation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xIftC8fvpwir"
      },
      "outputs": [],
      "source": [
        "def mapper_16QAM(QAM16, data):\n",
        "    map0 = 2*data[slice(0, len(data), 2)] + data[slice(1, len(data), 2)]\n",
        "    map0 = list(map(int, map0))\n",
        "    dataMapped = []\n",
        "    for i in range(len(map0)):\n",
        "        dataMapped.append(QAM16[map0[i]])\n",
        "    return(dataMapped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "AYhPgAIcqtGT"
      },
      "outputs": [],
      "source": [
        "def calculate_bits(Modulation,NumSubcarriers,NumDataSymb):\n",
        "    if Modulation=='QPSK':\n",
        "        Nbpscs=2\n",
        "    elif Modulation=='16QAM':\n",
        "        Nbpscs=4\n",
        "    return NumDataSymb*NumSubcarriers*Nbpscs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgX6tiqFpwvb"
      },
      "source": [
        "# **generate noise**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2LQwZvGaprKZ"
      },
      "outputs": [],
      "source": [
        "def AWGN(IFsig, SNR):\n",
        "    dP = np.zeros(len(IFsig))\n",
        "    P = 0\n",
        "\n",
        "    for i in range(len(IFsig)):\n",
        "        dP[i] = abs(IFsig[i])**2\n",
        "        P = P + dP[i]\n",
        "\n",
        "    P = P/len(IFsig)\n",
        "    gamma = 10**(SNR/10)\n",
        "    N0 = P/gamma\n",
        "    n = ((N0/2)**(0.5))*np.random.standard_normal(len(IFsig))\n",
        "    IF_n = np.zeros((len(IFsig),1))\n",
        "\n",
        "    for i in range(len(IFsig)):\n",
        "        IF_n[i,:] = IFsig[i] + n[i]\n",
        "\n",
        "    return(IF_n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDx6TEAyimk9"
      },
      "source": [
        "# Generate channel model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "DnoArgs7imk9"
      },
      "outputs": [],
      "source": [
        "def Generate_channel(Nr, Nt, type):\n",
        "    if (type == 'gauss'):\n",
        "        return (np.random.normal(size=(Nr,Nt))+1j*np.random.normal(size=(Nr,Nt)))/np.sqrt(2)\n",
        "    if (type == 'rayleigh'):\n",
        "        return (np.random.rayleigh(scale=(1/np.sqrt(2)), size=(Nr,Nt)) + 1j*np.random.rayleigh(scale=(1/np.sqrt(2)), size=(Nr,Nt)))/np.sqrt(2)\n",
        "    if (type == 'rician'):\n",
        "        b = 1/np.sqrt(2)\n",
        "        return (rice.rvs(b, size=(Nr,Nt)) + 1j*rice.rvs(b, size=(Nr,Nt)))/np.sqrt(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eciNtnFjq2yd"
      },
      "source": [
        "# **Generate Dataset**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "AE4Q6jZCq3CY"
      },
      "outputs": [],
      "source": [
        "DataSet_x   = []  # x dataset after modulation\n",
        "DataSet_y   = []  # y dataset\n",
        "DataSet_HH  = []  # H dataset\n",
        "DataSet_b   = []  # binary dataset\n",
        "SNR_min_dB  = 0\n",
        "SNR_max_dB  = 20\n",
        "step_dB     = 5\n",
        "num_dB      = int((SNR_max_dB - SNR_min_dB) / step_dB) + 1\n",
        "\n",
        "SNR         = np.linspace(SNR_min_dB, SNR_max_dB, num=num_dB)\n",
        "\n",
        "\n",
        "Nt = 8             # Tx: 8\n",
        "Nr = 64            # Rx: 128\n",
        "N_samp = 4000\n",
        "\n",
        "\n",
        "def Gen_dataset(mode, snr, imperfect, N_samp):\n",
        "    DataSet_x   = []  # x dataset after modulation\n",
        "    DataSet_y   = []  # y dataset\n",
        "    DataSet_H   = []\n",
        "    DataSet_HH  = []\n",
        "\n",
        "    NumSubcarriers = 1\n",
        "    Modulation = '16QAM'\n",
        "    QAM16 = [-1, -0.333, 0.333, 1]\n",
        "    NumDataSymb = 1\n",
        "    N_type = 'gauss'\n",
        "\n",
        "    if mode == 'train':\n",
        "        for snr in SNR:\n",
        "            for runIdx in range(0, N_samp):      # ! 20000 x Nt: samples\n",
        "                H = Generate_channel(Nt, Nr, N_type)\n",
        "                HH = np.concatenate((np.concatenate((H.real, H.imag), axis=1),\n",
        "                                    np.concatenate((-H.imag, H.real), axis=1)), axis=0)\n",
        "                x = np.zeros((2*Nt, NumSubcarriers))\n",
        "                a = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "                DataRaw = np.zeros((Nt, a))\n",
        "                for t in range(Nt):\n",
        "                    #\"data symbol generate\"\n",
        "                    NumBits = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "                    bit = np.random.randint(1, 3, NumBits)-1\n",
        "                    DataRaw[t, :] = bit\n",
        "                    for j in range(4):\n",
        "                        DataSet_b.append(bit[j])\n",
        "                    I = np.zeros((1, a))\n",
        "                    I[0, :] = DataRaw[t, :]\n",
        "                    (dataI, dataQ) = slicer(I[0])\n",
        "\n",
        "                    # Mapper\n",
        "                    mapI = mapper_16QAM(QAM16, dataI)\n",
        "                    mapQ = mapper_16QAM(QAM16, dataQ)\n",
        "                    x[t] = mapI[0]\n",
        "                    x[t+Nt] = mapQ[0]\n",
        "\n",
        "                # transpose\n",
        "                x = x.transpose()\n",
        "\n",
        "                y_wo_noise = np.matmul(x, HH)\n",
        "\n",
        "                # noise\n",
        "                noise = AWGN(y_wo_noise.transpose(), snr)\n",
        "\n",
        "                y = y_wo_noise + noise.transpose()\n",
        "\n",
        "                DataSet_x.append(x)    # ! I, Q sample distance by Nt.\n",
        "                DataSet_y.append(y)                 # ! output sample\n",
        "\n",
        "                # Imperfect channel: 5%\n",
        "                # coef = (2*np.random.randint(0,2,size=HH.shape) - 1)\n",
        "                # HH = HH + coef * HH * 0.05\n",
        "                DataSet_HH.append(HH)\n",
        "                DataSet_H.append(H)               # ! Generated channel\n",
        "\n",
        "    else:\n",
        "        for runIdx in range(0, N_samp):      # ! 20000 x Nt: samples\n",
        "            H = Generate_channel(Nt, Nr, N_type)\n",
        "            HH = np.concatenate((np.concatenate((H.real, H.imag), axis=1),\n",
        "                                np.concatenate((-H.imag, H.real), axis=1)), axis=0)\n",
        "            x = np.zeros((2*Nt, NumSubcarriers))\n",
        "            a = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "            DataRaw = np.zeros((Nt, a))\n",
        "            for t in range(Nt):\n",
        "                #\"data symbol generate\"\n",
        "                NumBits = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
        "                bit = np.random.randint(1, 3, NumBits)-1\n",
        "                DataRaw[t, :] = bit\n",
        "                for j in range(4):\n",
        "                    DataSet_b.append(bit[j])\n",
        "                I = np.zeros((1, a))\n",
        "                I[0, :] = DataRaw[t, :]\n",
        "                (dataI, dataQ) = slicer(I[0])\n",
        "\n",
        "                # Mapper\n",
        "                mapI = mapper_16QAM(QAM16, dataI)\n",
        "                mapQ = mapper_16QAM(QAM16, dataQ)\n",
        "                x[t] = mapI[0]\n",
        "                x[t+Nt] = mapQ[0]\n",
        "\n",
        "            # transpose\n",
        "            x = x.transpose()\n",
        "\n",
        "            y_wo_noise = np.matmul(x, HH)\n",
        "\n",
        "            # noise\n",
        "            noise = AWGN(y_wo_noise.transpose(), snr)\n",
        "\n",
        "            y = y_wo_noise + noise.transpose()\n",
        "\n",
        "            DataSet_x.append(x)    # ! I, Q sample distance by Nt.\n",
        "            DataSet_y.append(y)                 # ! output sample\n",
        "\n",
        "            # Imperfect channel: 5%\n",
        "            DataSet_HH.append(HH)\n",
        "            DataSet_H.append(H)               # ! Generated channel\n",
        "\n",
        "\n",
        "    # Shuffle dataset\n",
        "    random.seed(1)\n",
        "    temp = list(zip(DataSet_x, DataSet_y, DataSet_H, DataSet_HH))\n",
        "    random.shuffle(temp)\n",
        "    DataSet_x, DataSet_y, DataSet_H, DataSet_HH = zip(*temp)\n",
        "\n",
        "    return DataSet_x, DataSet_y, DataSet_H, DataSet_HH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1nWo2pTrimk-"
      },
      "outputs": [],
      "source": [
        "def reconstruct_channel (H):\n",
        "# H_raw = [R(H) I(H); -I(H) R(H)]\n",
        "# we have four version of H_est\n",
        "    H_est_1 = []\n",
        "    H_est_2 = []\n",
        "    H_est_3 = []\n",
        "    H_est_4 = []\n",
        "\n",
        "    H_est_Re_1 = H[0:Nt, 0:Nr]\n",
        "    H_est_Im_1 = H[0:Nt, Nr:2*Nr]\n",
        "    H_est_Im_2 = - H[Nt:2*Nt, 0:Nr]\n",
        "    H_est_Re_2 = H[Nt:2*Nt, Nr:2*Nr]\n",
        "\n",
        "    H_est_1 = H_est_Re_1 + 1j * H_est_Im_1\n",
        "    H_est_2 = H_est_Re_1 + 1j * H_est_Im_2\n",
        "    H_est_3 = H_est_Re_2 + 1j * H_est_Im_1\n",
        "    H_est_4 = H_est_Re_2 + 1j * H_est_Im_2\n",
        "\n",
        "    return H_est_1, H_est_2, H_est_3, H_est_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mQUDE5Q1imk-"
      },
      "outputs": [],
      "source": [
        "# def NMSE(H_est, H_raw):\n",
        "#     H_est_1, H_est_2, H_est_3, H_est_4 = reconstruct_channel(H_est)\n",
        "#     H_est_vec_1 = torch.reshape(H_est_1, [Nt * Nr, 1])\n",
        "#     H_est_vec_2 = torch.reshape(H_est_2, [Nt * Nr, 1])\n",
        "#     H_est_vec_3 = torch.reshape(H_est_3, [Nt * Nr, 1])\n",
        "#     H_est_vec_4 = torch.reshape(H_est_4, [Nt * Nr, 1])\n",
        "\n",
        "#     H_raw_vec = torch.reshape(H_raw, [Nt * Nr, 1])\n",
        "\n",
        "#     mse_1       = (torch.norm(H_raw_vec - H_est_vec_1)**2) / len(H_raw_vec)\n",
        "#     mse_2       = (torch.norm(H_raw_vec - H_est_vec_2)**2) / len(H_raw_vec)\n",
        "#     mse_3       = (torch.norm(H_raw_vec - H_est_vec_3)**2) / len(H_raw_vec)\n",
        "#     mse_4       = (torch.norm(H_raw_vec - H_est_vec_4)**2) / len(H_raw_vec)\n",
        "\n",
        "#     sigEner   = torch.norm(H_raw_vec)**2\n",
        "\n",
        "#     nmse_1      = mse_1 / sigEner\n",
        "#     nmse_2      = mse_2 / sigEner\n",
        "#     nmse_3      = mse_3 / sigEner\n",
        "#     nmse_4      = mse_4 / sigEner\n",
        "\n",
        "#     # Best nmse\n",
        "#     nmse        = min([nmse_1, nmse_2, nmse_3, nmse_4])\n",
        "\n",
        "#     return torch.abs(nmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "knU4OnL3imk_"
      },
      "outputs": [],
      "source": [
        "def NMSE(H_est, H_raw):\n",
        "    H_est_1, H_est_2, H_est_3, H_est_4 = reconstruct_channel(H_est)\n",
        "\n",
        "    # Lấy phần thực của các tensor nếu chúng là complex\n",
        "    H_est_vec_1 = torch.reshape(H_est_1, [Nt * Nr, 1]).abs()\n",
        "    H_est_vec_2 = torch.reshape(H_est_2, [Nt * Nr, 1]).abs()\n",
        "    H_est_vec_3 = torch.reshape(H_est_3, [Nt * Nr, 1]).abs()\n",
        "    H_est_vec_4 = torch.reshape(H_est_4, [Nt * Nr, 1]).abs()\n",
        "\n",
        "    H_raw_vec = torch.reshape(H_raw, [Nt * Nr, 1]).abs()\n",
        "\n",
        "    mse_1 = (torch.norm(H_raw_vec - H_est_vec_1)**2) / len(H_raw_vec)\n",
        "    mse_2 = (torch.norm(H_raw_vec - H_est_vec_2)**2) / len(H_raw_vec)\n",
        "    mse_3 = (torch.norm(H_raw_vec - H_est_vec_3)**2) / len(H_raw_vec)\n",
        "    mse_4 = (torch.norm(H_raw_vec - H_est_vec_4)**2) / len(H_raw_vec)\n",
        "\n",
        "    sigEner = torch.norm(H_raw_vec)**2\n",
        "\n",
        "    nmse_1 = mse_1 / sigEner\n",
        "    nmse_2 = mse_2 / sigEner\n",
        "    nmse_3 = mse_3 / sigEner\n",
        "    nmse_4 = mse_4 / sigEner\n",
        "\n",
        "    # Chọn NMSE tốt nhất\n",
        "    nmse = min([nmse_1, nmse_2, nmse_3, nmse_4])\n",
        "\n",
        "    return torch.abs(nmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "pyjIgxUurU0P"
      },
      "outputs": [],
      "source": [
        "def Input_ISDNN(mode, DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_samp):\n",
        "    H_in = []        # ! H_in    , np.diag(np.diag()) return a diag matrix instead of diag components.\n",
        "    H_true = []   # ! generated s\n",
        "    H_raw = []\n",
        "    e = []        # ! vector errors\n",
        "    xTx = []\n",
        "    xTy = []\n",
        "    Di = []\n",
        "    # steering = [] # ! Steering vector: ZoA and AoA\n",
        "\n",
        "    if mode == 'train':\n",
        "        n_sample = N_samp * len(SNR)\n",
        "    else:\n",
        "        n_sample = N_samp\n",
        "\n",
        "    for i in range (n_sample):\n",
        "        H_true.append(torch.tensor(DataSet_HH[i]))\n",
        "        H_raw.append(torch.tensor(DataSet_H[i]))\n",
        "        Di.append(torch.tensor(np.linalg.pinv(np.diag(np.diag(np.dot(DataSet_x[i].transpose(), DataSet_x[i]))))))\n",
        "        xTy.append(torch.tensor(np.dot(DataSet_x[i].transpose(), DataSet_y[i])))\n",
        "        H_in.append(torch.matmul(Di[i], xTy[i]))\n",
        "        e.append(torch.rand([2*Nt, 2*Nr]))\n",
        "        xTx.append(torch.tensor(np.dot(DataSet_x[i].transpose(), DataSet_x[i])))\n",
        "        # steering.append(torch.tensor(DataSet_Steering[i]))\n",
        "\n",
        "    H_true = torch.stack(H_true, dim=0)\n",
        "    H_raw = torch.stack(H_raw, dim=0)\n",
        "    H_in = torch.stack(H_in, dim=0)\n",
        "    e = torch.stack(e, dim=0)\n",
        "    xTx = torch.stack(xTx, dim=0)\n",
        "    xTy = torch.stack(xTy, dim=0)\n",
        "    Di = torch.stack(Di, dim=0)\n",
        "    # steering = torch.stack(steering, dim=0)\n",
        "\n",
        "    return H_true, H_raw, H_in, e, xTx, xTy, Di"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGhdBsghq3M9"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "GHceh5kuq3ZD"
      },
      "outputs": [],
      "source": [
        "class xv(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(xv, self).__init__()\n",
        "\n",
        "        self.alpha1 = torch.nn.parameter.Parameter(torch.rand(1))\n",
        "        self.alpha2 = torch.nn.parameter.Parameter(torch.tensor([0.5]))\n",
        "\n",
        "    def forward(self, Di, H, e, xTx, xTy):\n",
        "\n",
        "        xTxH = torch.bmm(xTx, H)\n",
        "\n",
        "        z    = H + torch.bmm(Di, torch.sub(xTy, xTxH)) + self.alpha1 * e\n",
        "\n",
        "        e    = torch.sub(xTy, xTxH)\n",
        "\n",
        "        H    = torch.add((1 - self.alpha2) * z, self.alpha2 * H)\n",
        "\n",
        "        return H, e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "woRjD7lJssRq"
      },
      "outputs": [],
      "source": [
        "class model_driven(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(model_driven, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc2 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc3 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc4 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc5 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc6 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc7 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc8 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc9 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "        self.fc10 = torch.nn.Linear(2*Nr, 2*Nr)\n",
        "\n",
        "        self.layer1=xv()\n",
        "        self.layer2=xv()\n",
        "        self.layer3=xv()\n",
        "        self.layer4=xv()\n",
        "        self.layer5=xv()\n",
        "\n",
        "    def forward(self, Di, H_in, e, xTx, xTy):\n",
        "        e = self.fc1(e)\n",
        "        e = self.fc2(e)\n",
        "        H, e = self.layer1(Di, H_in, e, xTx, xTy)\n",
        "        H = torch.tanh(H)\n",
        "\n",
        "        e = self.fc3(e)\n",
        "        e = self.fc4(e)\n",
        "        H, e = self.layer2(Di, H, e, xTx, xTy)\n",
        "        H = torch.tanh(H)\n",
        "\n",
        "        e = self.fc5(e)\n",
        "        e = self.fc6(e)\n",
        "        H, e = self.layer3(Di, H, e, xTx, xTy)\n",
        "        H = torch.tanh(H)\n",
        "\n",
        "        e = self.fc7(e)\n",
        "        e = self.fc8(e)\n",
        "        H, e = self.layer4(Di, H, e, xTx, xTy)\n",
        "        H = torch.tanh(H)\n",
        "\n",
        "        e = self.fc9(e)\n",
        "        e = self.fc10(e)\n",
        "        H, e = self.layer5(Di, H, e, xTx, xTy)\n",
        "\n",
        "        return H, e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZjihTOXq3kG"
      },
      "source": [
        "# Define model, optimizer, and loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Vp9fRd3gq3tw"
      },
      "outputs": [],
      "source": [
        "def def_model():\n",
        "    model = model_driven()\n",
        "    loss = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    folder_model = './model/'\n",
        "\n",
        "    if not os.path.isdir(folder_model):\n",
        "        os.makedirs(folder_model)\n",
        "\n",
        "    file_model = folder_model + 'H'\n",
        "    # if os.path.isfile(file_model):\n",
        "    #     generator = torch.load(file_model)\n",
        "\n",
        "    record_file = 'H'\n",
        "    return model, loss, optimizer, record_file, file_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYWM7SzItKzS"
      },
      "source": [
        "# Main program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "jv7lDwyxtFe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a53ad0e3-22f7-44d8-a2cb-937e52761649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-3e5f3c45b18a>:6: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  dP[i] = abs(IFsig[i])**2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dữ liệu đã được sinh và lưu lại!\n",
            "Begin training...\n",
            "139.8337374126988\n",
            "140.59506523611162\n",
            "137.0628502704071\n",
            "141.53454211508898\n",
            "139.14180273654677\n",
            "138.19464918875252\n",
            "141.07153793733067\n",
            "140.25056745281233\n",
            "140.45362942532014\n",
            "135.49683912730478\n",
            "134.5478044934898\n",
            "138.603089242691\n",
            "142.1676489510276\n",
            "143.1110047671928\n",
            "140.28272851311306\n",
            "142.4729761392435\n",
            "139.22241261853756\n",
            "143.0680491821313\n",
            "141.17700982291578\n",
            "140.4402583997653\n",
            "138.76685599606353\n",
            "138.72612329319313\n",
            "142.4089393545527\n",
            "138.6990130108087\n",
            "136.9203227957303\n",
            "139.70636053385311\n",
            "139.11709122389613\n",
            "136.03176812934137\n",
            "144.33826297853054\n",
            "144.56701042808663\n",
            "137.985618209737\n",
            "141.7341194780759\n",
            "146.00305797716794\n",
            "135.4768575327231\n",
            "140.33748170890408\n",
            "140.94555088329287\n",
            "140.96210715674377\n",
            "139.6434175253015\n",
            "141.22359275797405\n",
            "139.5551272294579\n",
            "139.75597570189365\n",
            "140.34058835387114\n",
            "138.063774508113\n",
            "135.68660328296974\n",
            "138.5781221431168\n",
            "138.66253896285295\n",
            "135.69614130428874\n",
            "144.449742794047\n",
            "139.34992916704826\n",
            "140.54406984894743\n",
            "138.06221137408747\n",
            "141.0510752188546\n",
            "143.4184027632831\n",
            "142.25501912532891\n",
            "142.69046491542295\n",
            "141.55243835283034\n",
            "143.91888820142648\n",
            "139.82832032251517\n",
            "140.82487692464537\n",
            "138.63740799072144\n",
            "137.3982583274249\n",
            "141.91252013853625\n",
            "136.90801034886607\n",
            "142.04191652065782\n",
            "143.52136114922453\n",
            "140.91043455107456\n",
            "137.05174260759057\n",
            "140.86104442256868\n",
            "143.69194215459643\n",
            "139.26231314792915\n",
            "144.96010277915389\n",
            "142.34627025923575\n",
            "140.34893038435422\n",
            "135.6229569398612\n",
            "142.79935937863846\n",
            "137.4432602355585\n",
            "142.9687262108453\n",
            "141.0124012804211\n",
            "140.12158512214035\n",
            "146.13631495418835\n",
            "138.2668755548322\n",
            "141.99883868762947\n",
            "138.59066109630842\n",
            "141.77042472373873\n",
            "140.4603343910848\n",
            "144.02869817107444\n",
            "141.05465232172116\n",
            "142.80701809289167\n",
            "142.13437496859075\n",
            "140.66838548038078\n",
            "143.02555148403476\n",
            "141.3739612615257\n",
            "139.8216305401191\n",
            "142.11652818432532\n",
            "143.58911997859042\n",
            "141.40002646976276\n",
            "141.25787371903454\n",
            "134.08432478124453\n",
            "137.22430466887073\n",
            "140.2550602451485\n",
            "137.95853581869008\n",
            "137.51904902219627\n",
            "144.07460691683474\n",
            "134.36332212738984\n",
            "134.796700279916\n",
            "139.22785653985585\n",
            "143.97958706905348\n",
            "143.55731306446648\n",
            "143.4619676157506\n",
            "140.43004184932025\n",
            "136.28886453989102\n",
            "142.51772550742282\n",
            "140.12082012406006\n",
            "142.77456015728126\n",
            "140.82745360267745\n",
            "143.5567855225844\n",
            "142.6557413452132\n",
            "138.8359572584907\n",
            "143.41379237057947\n",
            "140.35674469001398\n",
            "139.9173007268603\n",
            "139.05670166092128\n",
            "142.79928250625167\n",
            "138.10492723967727\n",
            "140.42502437480536\n",
            "147.21702359477186\n",
            "142.3941979451264\n",
            "139.18141009205718\n",
            "138.58570931841297\n",
            "138.0940471738107\n",
            "142.39061998778112\n",
            "139.5172721761686\n",
            "141.14509696690894\n",
            "136.35072230214183\n",
            "141.5046421805716\n",
            "138.8996429945236\n",
            "141.66274624489847\n",
            "139.15251118093124\n",
            "138.32992707092737\n",
            "145.86573954474403\n",
            "142.10910945316925\n",
            "142.70636769619915\n",
            "145.08803538044597\n",
            "142.80024907282112\n",
            "142.8886160470887\n",
            "141.3423501830247\n",
            "140.32708996229164\n",
            "144.55238775412298\n",
            "140.6273630881982\n",
            "140.6195981393567\n",
            "141.38299436528652\n",
            "142.51846611384258\n",
            "139.9275536723555\n",
            "139.81355228473222\n",
            "142.1198352847595\n",
            "141.26472962335438\n",
            "139.0535011850663\n",
            "142.80348631260125\n",
            "141.5787528371991\n",
            "140.42091228810347\n",
            "138.12090376883177\n",
            "143.38397180305387\n",
            "140.74173842204402\n",
            "138.1432753243872\n",
            "142.31881936919032\n",
            "139.54131863141842\n",
            "146.74253389450726\n",
            "137.6531721532217\n",
            "140.87644206950353\n",
            "141.11866220184288\n",
            "136.88905014359318\n",
            "142.48401675850363\n",
            "142.35459072892138\n",
            "139.08609859137786\n",
            "142.3813658504149\n",
            "145.35265740797522\n",
            "138.14068963233854\n",
            "142.09837322548373\n",
            "141.50720794106832\n",
            "141.80927532012768\n",
            "143.1041069090735\n",
            "139.02535702489132\n",
            "141.20092611061912\n",
            "140.90835151727578\n",
            "139.6354947583749\n",
            "144.7567214616576\n",
            "138.5662418736515\n",
            "143.68467997345763\n",
            "145.34078964451186\n",
            "141.6505608316493\n",
            "140.89285997501648\n",
            "140.64052186447066\n",
            "142.36045609998257\n",
            "141.37103843288983\n",
            "133.87279699543072\n",
            "135.92016144090303\n",
            "141.4650348909329\n",
            "139.30980798929488\n",
            "138.87207924176997\n",
            "144.09039337645999\n",
            "136.49610749841608\n",
            "137.47169901454106\n",
            "141.56318943178871\n",
            "139.47467086918775\n",
            "143.4479918016446\n",
            "142.60217961450772\n",
            "142.42449396102796\n",
            "147.16913784033363\n",
            "140.04737451201234\n",
            "144.50593638085468\n",
            "142.60882250258575\n",
            "139.7966274863274\n",
            "143.98002071841563\n",
            "138.22206061231753\n",
            "139.95100588003908\n",
            "141.62060452749233\n",
            "142.59491007549798\n",
            "139.01891097390464\n",
            "139.6559041575877\n",
            "140.22033311664234\n",
            "143.97559429931687\n",
            "145.21839073675358\n",
            "142.50317697801765\n",
            "143.5555708414418\n",
            "143.4493588754244\n",
            "138.61004343058178\n",
            "142.62259792971844\n",
            "137.77695544302543\n",
            "141.05311543025354\n",
            "138.84795212705592\n",
            "141.05401181412574\n",
            "138.09371934336704\n",
            "138.13236175847422\n",
            "145.12404699559875\n",
            "141.05082157286796\n",
            "139.81769060036558\n",
            "142.41544408291776\n",
            "143.76415482856962\n",
            "145.68593693631112\n",
            "137.3878919084295\n",
            "142.97397509182792\n",
            "141.55515975851196\n",
            "143.34205693925398\n",
            "142.34582350959792\n",
            "138.96315266825655\n",
            "140.96605682566533\n",
            "138.48108654378586\n",
            "141.85455856593072\n",
            "138.06638259554674\n",
            "147.52383726494634\n",
            "138.33657604920222\n",
            "137.21725024728389\n",
            "136.80888396094403\n",
            "142.37196461961727\n",
            "145.26851320973094\n",
            "138.86770763243115\n",
            "142.24234354334118\n",
            "138.87375143690122\n",
            "139.9113655280846\n",
            "138.13999813136934\n",
            "140.54966778561814\n",
            "140.15840029693703\n",
            "142.78930250473763\n",
            "137.61344782785275\n",
            "144.40282838480692\n",
            "139.4616757520113\n",
            "140.15273652652013\n",
            "139.4953301044785\n",
            "141.28805355781193\n",
            "143.51961357018732\n",
            "144.71420011904112\n",
            "138.32953590116125\n",
            "139.0259789799004\n",
            "143.2518706726241\n",
            "138.43233682994784\n",
            "144.1043912569557\n",
            "144.17092075884176\n",
            "140.899165788768\n",
            "144.90741235710155\n",
            "139.68416151773775\n",
            "143.10773607292293\n",
            "141.56397733861394\n",
            "142.30087740459174\n",
            "141.1640230294894\n",
            "135.04654013014255\n",
            "146.36015202766285\n",
            "142.6164782578346\n",
            "141.34180069228196\n",
            "137.5798899182991\n",
            "143.80010525116145\n",
            "143.1043451549518\n",
            "140.78057400444152\n",
            "142.3691675549023\n",
            "133.1593646361957\n",
            "136.80283834937256\n",
            "140.11825018410968\n",
            "143.59889337783662\n",
            "143.875162216004\n",
            "139.94959905679076\n",
            "138.27284989741804\n",
            "138.85881192594468\n",
            "141.10333261983433\n",
            "136.2247044022362\n",
            "136.76237640442898\n",
            "142.20884983410636\n",
            "137.60179955331924\n",
            "140.65043611128633\n",
            "142.06218754576963\n",
            "137.8960533538674\n",
            "137.83112380919664\n",
            "141.23516566898294\n",
            "137.0891148182297\n",
            "139.43706324999204\n",
            "142.52589896041223\n",
            "143.53851089400015\n",
            "137.06173969196016\n",
            "142.85053939910986\n",
            "139.45786997963964\n",
            "139.07993613525355\n",
            "137.55494626828\n",
            "138.46576842000286\n",
            "140.6100370504856\n",
            "139.39306804583526\n",
            "133.74297269410144\n",
            "143.26425564296125\n",
            "139.85888428926538\n",
            "139.2418630792883\n",
            "135.52226458864084\n",
            "141.61428916009422\n",
            "140.25947813234325\n",
            "140.78473003143833\n",
            "138.9501864604881\n",
            "143.81923681199146\n",
            "141.45746140538643\n",
            "142.43637726270316\n",
            "138.2264557377729\n",
            "141.77557181969158\n",
            "140.30356887842134\n",
            "138.63821435372813\n",
            "140.35444892968397\n",
            "140.60225874043368\n",
            "140.3624978305972\n",
            "138.86851596311757\n",
            "137.37297405186914\n",
            "143.86274603308777\n",
            "144.6988657617831\n",
            "143.7610332059727\n",
            "134.6801120138081\n",
            "141.66162156628044\n",
            "142.96592332131496\n",
            "145.12431826107425\n",
            "141.6310175818867\n",
            "144.78434476370302\n",
            "139.79917560528048\n",
            "137.63278462404799\n",
            "140.83621033053507\n",
            "138.73045065548655\n",
            "143.0600277689596\n",
            "140.58479873919165\n",
            "142.2217964140012\n",
            "138.7969438572198\n",
            "139.0389980223311\n",
            "143.0181126236864\n",
            "141.13754766974944\n",
            "141.84783786318167\n",
            "139.26184125108787\n",
            "141.97245471883403\n",
            "143.1404486587087\n",
            "139.97580445945835\n",
            "145.03486308092906\n",
            "137.5278409892949\n",
            "134.8426326605396\n",
            "142.44463424277805\n",
            "140.70822033652806\n",
            "141.72949412430717\n",
            "135.95274982765253\n",
            "139.3851829731679\n",
            "139.7757343845663\n",
            "143.22377437980168\n",
            "136.04513247046864\n",
            "141.95371850424468\n",
            "142.70681279345015\n",
            "138.80419878200266\n",
            "140.33538174013677\n",
            "141.490849872839\n",
            "139.5089756641752\n",
            "136.69874547799367\n",
            "145.76383594754483\n",
            "143.4748932402385\n",
            "140.15911483620675\n",
            "141.7427664582017\n",
            "140.95221907821218\n",
            "141.54582105565592\n",
            "138.82788451255942\n",
            "147.00253396867203\n",
            "136.6290251269358\n",
            "139.2808247491609\n",
            "138.44801881360692\n",
            "139.10904914310845\n",
            "141.42509377310003\n",
            "138.7463543957147\n",
            "141.69133196321573\n",
            "137.90329934271463\n",
            "139.0313113908982\n",
            "138.36913813701912\n",
            "141.51819969299856\n",
            "144.60372970571126\n",
            "138.28302951442458\n",
            "139.63530576138106\n",
            "146.36567017769673\n",
            "139.4040936017946\n",
            "142.32799202431448\n",
            "140.29079944467628\n",
            "141.41620774978853\n",
            "138.6239785182005\n",
            "137.84338548928176\n",
            "139.59072932129112\n",
            "143.03973870770844\n",
            "140.948229183097\n",
            "135.25127863178582\n",
            "141.82061878296338\n",
            "142.5146294830056\n",
            "143.04054660032673\n",
            "139.77522119032275\n",
            "141.16560943816285\n",
            "138.85977270131195\n",
            "137.5873312555395\n",
            "136.16628699597234\n",
            "140.1578780294157\n",
            "140.3565978294972\n",
            "144.6342670193613\n",
            "141.1131551756737\n",
            "133.99755581547308\n",
            "142.23706832174818\n",
            "145.9714437971716\n",
            "139.51449922066143\n",
            "138.5412898315633\n",
            "138.9803093486318\n",
            "143.17701774230244\n",
            "144.59357076767506\n",
            "142.34391527549687\n",
            "138.40978969920653\n",
            "143.76552215028656\n",
            "139.34458313063274\n",
            "140.4337827772693\n",
            "142.3181599551775\n",
            "140.79009950409068\n",
            "143.17131674513567\n",
            "139.8343292127292\n",
            "141.27862636951448\n",
            "142.5384991701485\n",
            "142.38208200393817\n",
            "142.41441364049965\n",
            "143.7388897280911\n",
            "139.10193993311262\n",
            "140.11824455250502\n",
            "141.995876615651\n",
            "135.2559071227006\n",
            "145.56557048193122\n",
            "135.99061442282476\n",
            "139.75916114293284\n",
            "141.55719597891243\n",
            "135.71490303609008\n",
            "140.87389733990386\n",
            "140.61216773469985\n",
            "139.90347952681253\n",
            "142.7876067022587\n",
            "143.60129216409425\n",
            "140.92607589732415\n",
            "138.327704513609\n",
            "143.15451628529064\n",
            "140.6607883677826\n",
            "139.8527107130357\n",
            "143.22929028406887\n",
            "136.2927696921522\n",
            "138.6305664484363\n",
            "138.27201299891757\n",
            "136.10977903758533\n",
            "141.58481257513398\n",
            "139.59120578425578\n",
            "140.43184994084993\n",
            "139.7120497092255\n",
            "140.66738412245928\n",
            "138.65301061961216\n",
            "138.5456588079737\n",
            "143.3449495589686\n",
            "140.37613732154142\n",
            "136.1118915412412\n",
            "136.65490998056103\n",
            "138.74164522100082\n",
            "141.08359397572906\n",
            "147.1678656916997\n",
            "141.09251867489462\n",
            "139.77831829184038\n",
            "138.7320437372458\n",
            "140.6378344262241\n",
            "142.8212283416467\n",
            "142.42871287381143\n",
            "147.5108305825705\n",
            "142.742192397883\n",
            "140.85287165081363\n",
            "137.78031592731716\n",
            "142.67034373805456\n",
            "143.40630147768547\n",
            "144.43328293885037\n",
            "139.60451554163322\n",
            "141.09493627994246\n",
            "139.9152254222834\n",
            "142.26560242595895\n",
            "138.88755146728826\n",
            "146.53781166860608\n",
            "140.28733382591813\n",
            "1 0.03619684067588674 23.778142675591596\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-9b629dc8d3ad>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m                                   H_true[0 + batch_size * bs:batch_size * (bs+1), :, :])   # calculate loss for the predicted output\n\u001b[1;32m     56\u001b[0m                 \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# backpropagate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# adjust parameters based on the calculated gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    221\u001b[0m             )\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    593\u001b[0m                 \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_state_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m             ]\n\u001b[0;32m--> 595\u001b[0;31m             bias_correction2 = [\n\u001b[0m\u001b[1;32m    596\u001b[0m                 \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_state_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    594\u001b[0m             ]\n\u001b[1;32m    595\u001b[0m             bias_correction2 = [\n\u001b[0;32m--> 596\u001b[0;31m                 \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_state_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m             ]\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "epoch         = 0\n",
        "expected_epoch = 20000\n",
        "num_samp      = N_samp * len(SNR)\n",
        "best_nmse     = 1e9\n",
        "best_trainloss = 1e9\n",
        "early_stop    = 0\n",
        "best_model    = ''\n",
        "batch_size    = int(num_samp / 512)\n",
        "\n",
        "# Kiểm tra nếu file tĩnh tồn tại\n",
        "if os.path.exists('dataset_ISDNN.pkl'):\n",
        "    with open('dataset_ISDNN.pkl', 'rb') as f:\n",
        "        DataSet_x, DataSet_y, DataSet_H, DataSet_HH, H_true, H_raw, H_in, e, xTx, xTy, Di = pickle.load(f)\n",
        "    print(\"Dữ liệu đã được tải từ file tĩnh!\")\n",
        "else:\n",
        "    # Sinh dữ liệu nếu file tĩnh không tồn tại\n",
        "    DataSet_x, DataSet_y, DataSet_H, DataSet_HH = Gen_dataset('train', 0, 0, N_samp)\n",
        "    H_true, H_raw, H_in, e, xTx, xTy, Di = Input_ISDNN('train', DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_samp)\n",
        "\n",
        "    # Lưu dữ liệu để lần sau không phải sinh lại\n",
        "    with open('dataset_ISDNN.pkl', 'wb') as f:\n",
        "        pickle.dump((DataSet_x, DataSet_y, DataSet_H, DataSet_HH, H_true, H_raw, H_in, e, xTx, xTy, Di), f)\n",
        "    print(\"Dữ liệu đã được sinh và lưu lại!\")\n",
        "\n",
        "print(\"Begin training...\")\n",
        "starttime = timeit.default_timer()\n",
        "\n",
        "while(True):\n",
        "        epoch = epoch + 1\n",
        "\n",
        "        init_loss = 1e9\n",
        "        while( epoch == 1 and init_loss > 200):\n",
        "\n",
        "                model, loss, optimizer, record_file, file_model = def_model()\n",
        "                for bs in range (int(num_samp / batch_size)):\n",
        "                    H_1, e_1 = model(Di[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 H_in[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 e[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTx[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTy[0 + batch_size * bs:batch_size * (bs+1), :, :])   # predict output from the model\n",
        "                    init_loss = loss(H_1, H_true[0 + batch_size * bs:batch_size * (bs+1), :, :]).item()\n",
        "                    print(init_loss)\n",
        "\n",
        "        optimizer.zero_grad()   # zero the parameter gradients\n",
        "        train_loss = 0\n",
        "        H_f = torch.empty([num_samp, 2*Nt, 2*Nr])\n",
        "        for bs in range (int(num_samp / batch_size)):\n",
        "                H_o, e_o = model(Di[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 H_in[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 e[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTx[0 + batch_size * bs:batch_size * (bs+1), :, :],\n",
        "                                 xTy[0 + batch_size * bs:batch_size * (bs+1), :, :])   # predict output from the model\n",
        "                H_f[0 + batch_size * bs:batch_size * (bs+1), :, :] = H_o\n",
        "                train_loss = loss(H_o,\n",
        "                                  H_true[0 + batch_size * bs:batch_size * (bs+1), :, :])   # calculate loss for the predicted output\n",
        "                train_loss.backward()   # backpropagate the loss\n",
        "                optimizer.step()        # adjust parameters based on the calculated gradients\n",
        "\n",
        "        if (epoch % 100 == 0 or epoch == 1):\n",
        "                nmse = 0\n",
        "                for j in range (num_samp):\n",
        "                        nmse += NMSE(H_f[j], H_raw[j])\n",
        "                nmse = nmse / num_samp\n",
        "\n",
        "                if (nmse <= best_nmse and train_loss <= best_trainloss):\n",
        "                        torch.save(model.state_dict(), file_model + '_' + str(epoch) + '.pth')\n",
        "                        best_model = file_model + '_' + str(epoch) + '.pth'\n",
        "                        best_nmse = nmse\n",
        "                        early_stop = 0\n",
        "                else:\n",
        "                        early_stop += 1\n",
        "\n",
        "                if (nmse > best_nmse and early_stop == 3 and train_loss > best_trainloss):\n",
        "                        with Record(record_file + '_log.txt'):\n",
        "                                print(epoch, nmse.item(), train_loss.item())\n",
        "                                print(str(timeit.default_timer()-starttime))\n",
        "                        break\n",
        "\n",
        "                with Record(record_file + '_log.txt'):\n",
        "                        print(epoch, nmse.item(), train_loss.item())\n",
        "\n",
        "        if epoch  == expected_epoch:\n",
        "                torch.save(model.state_dict(), file_model + '_' + str(epoch) + '.pth')\n",
        "                best_model = file_model + '_' + str(epoch) + '.pth'\n",
        "                with Record(record_file + '_log.txt'):\n",
        "                        print(\"epoch:\\n\", epoch)\n",
        "                        print(\"Latest NMSE:\\n\", nmse.item())\n",
        "                        print(\"Latest Loss:\\n\", train_loss.item())\n",
        "                        print(str(timeit.default_timer()-starttime))\n",
        "\n",
        "                break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOBK-_l-tRMO"
      },
      "source": [
        "# Test function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRRFGAX4tg_6"
      },
      "source": [
        "# Function to test the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = './model/H_500.pth'"
      ],
      "metadata": {
        "id": "hbzTeVvCcKJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfvpexfwthNq"
      },
      "outputs": [],
      "source": [
        "def test(H_raw, Di, H_in, e, xTx, xTy, N_test, log):\n",
        "    # Load the model that we saved at the end of the training loop\n",
        "    model = model_driven()\n",
        "    model.load_state_dict(torch.load(best_model, map_location=torch.device('cpu')))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        H_o, e_o = model(Di, H_in, e, xTx, xTy)\n",
        "\n",
        "        nmse = 0\n",
        "        for j in range (N_test):\n",
        "            nmse += NMSE(H_o[j], H_raw[j])\n",
        "\n",
        "        nmse = nmse / N_test\n",
        "        with Record(log):\n",
        "            print(format(nmse.item(), '.7f'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlrHA2aWimlC"
      },
      "outputs": [],
      "source": [
        "## Generate dataset for test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE_IiAiKimlC"
      },
      "outputs": [],
      "source": [
        "def LS(DataSet_x, DataSet_y):\n",
        "    start = timeit.default_timer()\n",
        "    for i in range (len(DataSet_x)):\n",
        "        H_hat = np.matmul(\n",
        "                    np.matmul(\n",
        "                        np.linalg.pinv(np.matmul(DataSet_x[i].transpose(), DataSet_x[i])),\n",
        "                        DataSet_x[i].transpose()),\n",
        "                        DataSet_y[i])\n",
        "    print(timeit.default_timer() - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "PAf8LBUFimlC"
      },
      "outputs": [],
      "source": [
        "SNR_min_dB  = 0\n",
        "SNR_max_dB  = 20\n",
        "step_dB     = 5\n",
        "num_dB      = int((SNR_max_dB - SNR_min_dB) / step_dB) + 1\n",
        "\n",
        "SNR         = np.linspace(SNR_min_dB, SNR_max_dB, num=num_dB)\n",
        "log         = './model/log_test.txt'\n",
        "\n",
        "N_test = int(num_samp * 30/100)\n",
        "\n",
        "for i in range (100):\n",
        "    for snr in SNR:\n",
        "        # with Record(log):\n",
        "        #     print(snr)\n",
        "        DataSet_x, DataSet_y, DataSet_H, DataSet_HH = Gen_dataset('test', snr, 0, N_test)\n",
        "        H_true, H_raw, H_in, e, xTx, xTy, Di = Input_ISDNN('test', DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_test)\n",
        "\n",
        "        # LS(DataSet_x, DataSet_y)\n",
        "        test(H_raw, Di, H_in, e, xTx, xTy, N_test, log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6_n95BiimlC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}